includes:
  - headers/device.yaml
# dataset: "cifar100"
save_path: ./
data_root: /root/autodl-tmp/cifar100

use_pretrain_init_state: False
state_path: ./state_dict_model_200iniEpochs.pth

init_cls_num: 20
inc_cls_num: 20
total_cls_num: 100
task_num: 5
init_epoch: 200
val_per_epoch: 10
epoch: 170 # stands for boosting_epochs
init_lr: 0.1
init_weight_decay: 5e-4

# model_name: "foster"
# convnet_type: "resnet32"
warmup: 0

device_ids: 0
n_gpu: 1

deterministic: True

backbone:
  name: resnet32
  kwargs: ~

classifier:
  name: FOSTER
  kwargs:
    num_class: 100
    feat_dim: 64
    beta1: [0.97, 0.96, 0.95, 0.94] #0.95
    beta2: 0.97
    is_teacher_wa: false
    is_student_wa: false
    lambda_okd: 1
    wa_value: 1
    oofc: "ft"
    convnet_type: "resnet32"
    T: 2
    fixed_memory: False
    memory_per_class: 20

    lr: 0.1
    compression_epochs: 130
    samples_new_class: 500 # number of train samples per class


batch_size: 128
num_workers: 8


buffer:
  name: LinearBuffer
  kwargs:
    buffer_size: 2000
    batch_size: 32       # has no effect on LinearBuffer
    strategy: foster    # random, equal_random, reservoir, herding

optimizer:
  name: SGD
  kwargs:
    lr: 0.1
    momentum: 0.9
    weight_decay: 5e-4

lr_scheduler:
  name: CosineAnnealingLR
  kwargs:
    T_max: 170
