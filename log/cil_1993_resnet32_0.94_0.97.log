2023-12-09 12:46:15,373 [trainer.py] => config: ./configs/cifar/b50inc10.json
2023-12-09 12:46:15,373 [trainer.py] => prefix: cil
2023-12-09 12:46:15,373 [trainer.py] => dataset: cifar100
2023-12-09 12:46:15,373 [trainer.py] => memory_size: 2000
2023-12-09 12:46:15,373 [trainer.py] => memory_per_class: 20
2023-12-09 12:46:15,373 [trainer.py] => fixed_memory: True
2023-12-09 12:46:15,373 [trainer.py] => shuffle: True
2023-12-09 12:46:15,373 [trainer.py] => init_cls: 50
2023-12-09 12:46:15,373 [trainer.py] => increment: 10
2023-12-09 12:46:15,373 [trainer.py] => model_name: foster
2023-12-09 12:46:15,373 [trainer.py] => convnet_type: resnet32
2023-12-09 12:46:15,373 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-09 12:46:15,373 [trainer.py] => seed: 1993
2023-12-09 12:46:15,374 [trainer.py] => beta1: 0.94
2023-12-09 12:46:15,374 [trainer.py] => beta2: 0.97
2023-12-09 12:46:15,374 [trainer.py] => oofc: ft
2023-12-09 12:46:15,374 [trainer.py] => is_teacher_wa: False
2023-12-09 12:46:15,374 [trainer.py] => is_student_wa: False
2023-12-09 12:46:15,374 [trainer.py] => lambda_okd: 1
2023-12-09 12:46:15,374 [trainer.py] => wa_value: 1
2023-12-09 12:46:15,374 [trainer.py] => init_epochs: 200
2023-12-09 12:46:15,374 [trainer.py] => init_lr: 0.1
2023-12-09 12:46:15,374 [trainer.py] => init_weight_decay: 0.0005
2023-12-09 12:46:15,374 [trainer.py] => boosting_epochs: 170
2023-12-09 12:46:15,374 [trainer.py] => compression_epochs: 130
2023-12-09 12:46:15,374 [trainer.py] => lr: 0.1
2023-12-09 12:46:15,374 [trainer.py] => batch_size: 128
2023-12-09 12:46:15,374 [trainer.py] => weight_decay: 0.0005
2023-12-09 12:46:15,374 [trainer.py] => num_workers: 8
2023-12-09 12:46:15,374 [trainer.py] => T: 2
2023-12-09 12:46:17,202 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-09 12:46:17,583 [trainer.py] => All params: 0
2023-12-09 12:46:17,583 [trainer.py] => Trainable params: 0
2023-12-09 12:46:17,600 [foster.py] => Learning on 0-50
2023-12-09 12:46:17,600 [foster.py] => All params: 470654
2023-12-09 12:46:17,601 [foster.py] => Trainable params: 470654
2023-12-09 12:46:33,327 [foster.py] => Task 0, Epoch 1/200 => Loss 3.918, Train_accy 3.69, Test_accy 7.54
2023-12-09 12:46:41,234 [foster.py] => Task 0, Epoch 2/200 => Loss 3.592, Train_accy 8.35
2023-12-09 12:46:49,062 [foster.py] => Task 0, Epoch 3/200 => Loss 3.408, Train_accy 12.11
2023-12-09 12:46:57,161 [foster.py] => Task 0, Epoch 4/200 => Loss 3.268, Train_accy 15.18
2023-12-09 12:47:04,969 [foster.py] => Task 0, Epoch 5/200 => Loss 3.140, Train_accy 17.57
2023-12-09 12:47:14,763 [foster.py] => Task 0, Epoch 6/200 => Loss 3.007, Train_accy 20.46, Test_accy 25.60
2023-12-09 12:47:22,527 [foster.py] => Task 0, Epoch 7/200 => Loss 2.871, Train_accy 23.11
2023-12-09 12:47:30,646 [foster.py] => Task 0, Epoch 8/200 => Loss 2.740, Train_accy 25.99
2023-12-09 12:47:38,681 [foster.py] => Task 0, Epoch 9/200 => Loss 2.647, Train_accy 28.20
2023-12-09 12:47:46,468 [foster.py] => Task 0, Epoch 10/200 => Loss 2.548, Train_accy 30.37
2023-12-09 12:47:56,201 [foster.py] => Task 0, Epoch 11/200 => Loss 2.485, Train_accy 32.19, Test_accy 41.72
2023-12-09 12:48:04,272 [foster.py] => Task 0, Epoch 12/200 => Loss 2.418, Train_accy 33.78
2023-12-09 12:48:12,053 [foster.py] => Task 0, Epoch 13/200 => Loss 2.357, Train_accy 35.50
2023-12-09 12:48:20,130 [foster.py] => Task 0, Epoch 14/200 => Loss 2.294, Train_accy 37.32
2023-12-09 12:48:28,059 [foster.py] => Task 0, Epoch 15/200 => Loss 2.244, Train_accy 38.15
2023-12-09 12:48:37,545 [foster.py] => Task 0, Epoch 16/200 => Loss 2.188, Train_accy 39.75, Test_accy 49.06
2023-12-09 12:48:45,292 [foster.py] => Task 0, Epoch 17/200 => Loss 2.158, Train_accy 40.08
2023-12-09 12:48:53,535 [foster.py] => Task 0, Epoch 18/200 => Loss 2.117, Train_accy 41.24
2023-12-09 12:49:01,256 [foster.py] => Task 0, Epoch 19/200 => Loss 2.095, Train_accy 41.95
2023-12-09 12:49:09,091 [foster.py] => Task 0, Epoch 20/200 => Loss 2.051, Train_accy 42.43
2023-12-09 12:49:18,595 [foster.py] => Task 0, Epoch 21/200 => Loss 2.040, Train_accy 43.04, Test_accy 42.36
2023-12-09 12:49:26,151 [foster.py] => Task 0, Epoch 22/200 => Loss 2.034, Train_accy 43.28
2023-12-09 12:49:34,130 [foster.py] => Task 0, Epoch 23/200 => Loss 1.992, Train_accy 44.36
2023-12-09 12:49:41,841 [foster.py] => Task 0, Epoch 24/200 => Loss 1.973, Train_accy 45.00
2023-12-09 12:49:50,036 [foster.py] => Task 0, Epoch 25/200 => Loss 1.962, Train_accy 44.99
2023-12-09 12:49:59,667 [foster.py] => Task 0, Epoch 26/200 => Loss 1.956, Train_accy 45.42, Test_accy 50.12
2023-12-09 12:50:07,545 [foster.py] => Task 0, Epoch 27/200 => Loss 1.923, Train_accy 46.22
2023-12-09 12:50:15,263 [foster.py] => Task 0, Epoch 28/200 => Loss 1.924, Train_accy 46.29
2023-12-09 12:50:23,057 [foster.py] => Task 0, Epoch 29/200 => Loss 1.893, Train_accy 47.04
2023-12-09 12:50:31,149 [foster.py] => Task 0, Epoch 30/200 => Loss 1.898, Train_accy 46.86
2023-12-09 12:50:40,766 [foster.py] => Task 0, Epoch 31/200 => Loss 1.891, Train_accy 46.87, Test_accy 52.18
2023-12-09 12:50:48,655 [foster.py] => Task 0, Epoch 32/200 => Loss 1.871, Train_accy 47.59
2023-12-09 12:50:56,666 [foster.py] => Task 0, Epoch 33/200 => Loss 1.862, Train_accy 47.60
2023-12-09 12:51:04,468 [foster.py] => Task 0, Epoch 34/200 => Loss 1.865, Train_accy 47.49
2023-12-09 12:51:12,459 [foster.py] => Task 0, Epoch 35/200 => Loss 1.836, Train_accy 47.97
2023-12-09 12:51:22,288 [foster.py] => Task 0, Epoch 36/200 => Loss 1.833, Train_accy 48.58, Test_accy 49.46
2023-12-09 12:51:30,044 [foster.py] => Task 0, Epoch 37/200 => Loss 1.823, Train_accy 48.68
2023-12-09 12:51:37,876 [foster.py] => Task 0, Epoch 38/200 => Loss 1.838, Train_accy 48.16
2023-12-09 12:51:45,643 [foster.py] => Task 0, Epoch 39/200 => Loss 1.798, Train_accy 49.29
2023-12-09 12:51:53,567 [foster.py] => Task 0, Epoch 40/200 => Loss 1.791, Train_accy 49.36
2023-12-09 12:52:03,152 [foster.py] => Task 0, Epoch 41/200 => Loss 1.794, Train_accy 49.23, Test_accy 51.90
2023-12-09 12:52:11,020 [foster.py] => Task 0, Epoch 42/200 => Loss 1.769, Train_accy 50.08
2023-12-09 12:52:18,889 [foster.py] => Task 0, Epoch 43/200 => Loss 1.797, Train_accy 49.07
2023-12-09 12:52:26,752 [foster.py] => Task 0, Epoch 44/200 => Loss 1.775, Train_accy 49.56
2023-12-09 12:52:34,727 [foster.py] => Task 0, Epoch 45/200 => Loss 1.767, Train_accy 50.02
2023-12-09 12:52:44,088 [foster.py] => Task 0, Epoch 46/200 => Loss 1.760, Train_accy 50.44, Test_accy 49.18
2023-12-09 12:52:52,164 [foster.py] => Task 0, Epoch 47/200 => Loss 1.749, Train_accy 50.70
2023-12-09 12:52:59,944 [foster.py] => Task 0, Epoch 48/200 => Loss 1.745, Train_accy 50.84
2023-12-09 12:53:06,935 [foster.py] => Task 0, Epoch 49/200 => Loss 1.753, Train_accy 50.30
2023-12-09 12:53:14,418 [foster.py] => Task 0, Epoch 50/200 => Loss 1.735, Train_accy 50.66
2023-12-09 12:53:24,041 [foster.py] => Task 0, Epoch 51/200 => Loss 1.740, Train_accy 50.25, Test_accy 55.78
2023-12-09 12:53:32,326 [foster.py] => Task 0, Epoch 52/200 => Loss 1.721, Train_accy 51.44
2023-12-09 12:53:40,160 [foster.py] => Task 0, Epoch 53/200 => Loss 1.724, Train_accy 51.23
2023-12-09 12:53:48,309 [foster.py] => Task 0, Epoch 54/200 => Loss 1.724, Train_accy 51.24
2023-12-09 12:53:56,196 [foster.py] => Task 0, Epoch 55/200 => Loss 1.722, Train_accy 51.40
2023-12-09 12:54:05,867 [foster.py] => Task 0, Epoch 56/200 => Loss 1.707, Train_accy 51.63, Test_accy 47.52
2023-12-09 12:54:12,928 [foster.py] => Task 0, Epoch 57/200 => Loss 1.702, Train_accy 51.69
2023-12-09 12:54:21,188 [foster.py] => Task 0, Epoch 58/200 => Loss 1.696, Train_accy 52.06
2023-12-09 12:54:29,055 [foster.py] => Task 0, Epoch 59/200 => Loss 1.705, Train_accy 51.52
2023-12-09 12:54:37,043 [foster.py] => Task 0, Epoch 60/200 => Loss 1.688, Train_accy 52.08
2023-12-09 12:54:46,624 [foster.py] => Task 0, Epoch 61/200 => Loss 1.679, Train_accy 52.68, Test_accy 57.88
2023-12-09 12:54:54,579 [foster.py] => Task 0, Epoch 62/200 => Loss 1.705, Train_accy 51.52
2023-12-09 12:55:02,462 [foster.py] => Task 0, Epoch 63/200 => Loss 1.663, Train_accy 52.60
2023-12-09 12:55:10,459 [foster.py] => Task 0, Epoch 64/200 => Loss 1.675, Train_accy 52.56
2023-12-09 12:55:18,418 [foster.py] => Task 0, Epoch 65/200 => Loss 1.674, Train_accy 52.40
2023-12-09 12:55:28,169 [foster.py] => Task 0, Epoch 66/200 => Loss 1.666, Train_accy 52.88, Test_accy 60.62
2023-12-09 12:55:35,987 [foster.py] => Task 0, Epoch 67/200 => Loss 1.639, Train_accy 53.47
2023-12-09 12:55:44,022 [foster.py] => Task 0, Epoch 68/200 => Loss 1.651, Train_accy 53.09
2023-12-09 12:55:51,735 [foster.py] => Task 0, Epoch 69/200 => Loss 1.635, Train_accy 53.36
2023-12-09 12:55:59,727 [foster.py] => Task 0, Epoch 70/200 => Loss 1.642, Train_accy 53.12
2023-12-09 12:56:09,008 [foster.py] => Task 0, Epoch 71/200 => Loss 1.627, Train_accy 53.93, Test_accy 58.34
2023-12-09 12:56:16,846 [foster.py] => Task 0, Epoch 72/200 => Loss 1.640, Train_accy 53.50
2023-12-09 12:56:24,725 [foster.py] => Task 0, Epoch 73/200 => Loss 1.630, Train_accy 53.29
2023-12-09 12:56:32,677 [foster.py] => Task 0, Epoch 74/200 => Loss 1.609, Train_accy 54.34
2023-12-09 12:56:40,535 [foster.py] => Task 0, Epoch 75/200 => Loss 1.588, Train_accy 54.71
2023-12-09 12:56:50,005 [foster.py] => Task 0, Epoch 76/200 => Loss 1.603, Train_accy 54.31, Test_accy 60.50
2023-12-09 12:56:58,278 [foster.py] => Task 0, Epoch 77/200 => Loss 1.610, Train_accy 54.44
2023-12-09 12:57:06,254 [foster.py] => Task 0, Epoch 78/200 => Loss 1.602, Train_accy 54.42
2023-12-09 12:57:14,047 [foster.py] => Task 0, Epoch 79/200 => Loss 1.583, Train_accy 54.88
2023-12-09 12:57:21,807 [foster.py] => Task 0, Epoch 80/200 => Loss 1.592, Train_accy 54.59
2023-12-09 12:57:31,764 [foster.py] => Task 0, Epoch 81/200 => Loss 1.572, Train_accy 54.87, Test_accy 64.16
2023-12-09 12:57:39,443 [foster.py] => Task 0, Epoch 82/200 => Loss 1.580, Train_accy 54.67
2023-12-09 12:57:47,431 [foster.py] => Task 0, Epoch 83/200 => Loss 1.554, Train_accy 55.56
2023-12-09 12:57:55,408 [foster.py] => Task 0, Epoch 84/200 => Loss 1.570, Train_accy 55.05
2023-12-09 12:58:03,297 [foster.py] => Task 0, Epoch 85/200 => Loss 1.570, Train_accy 55.39
2023-12-09 12:58:12,643 [foster.py] => Task 0, Epoch 86/200 => Loss 1.566, Train_accy 55.56, Test_accy 62.26
2023-12-09 12:58:20,485 [foster.py] => Task 0, Epoch 87/200 => Loss 1.535, Train_accy 56.24
2023-12-09 12:58:28,336 [foster.py] => Task 0, Epoch 88/200 => Loss 1.554, Train_accy 55.65
2023-12-09 12:58:35,785 [foster.py] => Task 0, Epoch 89/200 => Loss 1.531, Train_accy 56.56
2023-12-09 12:58:43,667 [foster.py] => Task 0, Epoch 90/200 => Loss 1.527, Train_accy 56.66
2023-12-09 12:58:53,409 [foster.py] => Task 0, Epoch 91/200 => Loss 1.541, Train_accy 56.02, Test_accy 62.80
2023-12-09 12:59:01,459 [foster.py] => Task 0, Epoch 92/200 => Loss 1.524, Train_accy 56.23
2023-12-09 12:59:09,350 [foster.py] => Task 0, Epoch 93/200 => Loss 1.517, Train_accy 56.45
2023-12-09 12:59:17,250 [foster.py] => Task 0, Epoch 94/200 => Loss 1.512, Train_accy 56.56
2023-12-09 12:59:25,006 [foster.py] => Task 0, Epoch 95/200 => Loss 1.504, Train_accy 57.17
2023-12-09 12:59:34,645 [foster.py] => Task 0, Epoch 96/200 => Loss 1.490, Train_accy 57.16, Test_accy 63.68
2023-12-09 12:59:42,667 [foster.py] => Task 0, Epoch 97/200 => Loss 1.500, Train_accy 57.27
2023-12-09 12:59:50,292 [foster.py] => Task 0, Epoch 98/200 => Loss 1.491, Train_accy 57.00
2023-12-09 12:59:58,088 [foster.py] => Task 0, Epoch 99/200 => Loss 1.490, Train_accy 57.29
2023-12-09 13:00:06,065 [foster.py] => Task 0, Epoch 100/200 => Loss 1.483, Train_accy 57.44
2023-12-09 13:00:15,710 [foster.py] => Task 0, Epoch 101/200 => Loss 1.472, Train_accy 57.71, Test_accy 66.78
2023-12-09 13:00:23,759 [foster.py] => Task 0, Epoch 102/200 => Loss 1.472, Train_accy 57.84
2023-12-09 13:00:32,269 [foster.py] => Task 0, Epoch 103/200 => Loss 1.463, Train_accy 57.97
2023-12-09 13:00:40,630 [foster.py] => Task 0, Epoch 104/200 => Loss 1.462, Train_accy 58.26
2023-12-09 13:00:48,642 [foster.py] => Task 0, Epoch 105/200 => Loss 1.448, Train_accy 58.68
2023-12-09 13:00:59,230 [foster.py] => Task 0, Epoch 106/200 => Loss 1.436, Train_accy 58.38, Test_accy 65.10
2023-12-09 13:01:07,659 [foster.py] => Task 0, Epoch 107/200 => Loss 1.432, Train_accy 58.92
2023-12-09 13:01:16,058 [foster.py] => Task 0, Epoch 108/200 => Loss 1.422, Train_accy 59.20
2023-12-09 13:01:24,703 [foster.py] => Task 0, Epoch 109/200 => Loss 1.417, Train_accy 59.60
2023-12-09 13:01:33,357 [foster.py] => Task 0, Epoch 110/200 => Loss 1.419, Train_accy 59.48
2023-12-09 13:01:43,292 [foster.py] => Task 0, Epoch 111/200 => Loss 1.407, Train_accy 59.28, Test_accy 65.44
2023-12-09 13:01:51,534 [foster.py] => Task 0, Epoch 112/200 => Loss 1.402, Train_accy 59.24
2023-12-09 13:02:00,305 [foster.py] => Task 0, Epoch 113/200 => Loss 1.401, Train_accy 59.85
2023-12-09 13:02:08,741 [foster.py] => Task 0, Epoch 114/200 => Loss 1.382, Train_accy 60.12
2023-12-09 13:02:17,417 [foster.py] => Task 0, Epoch 115/200 => Loss 1.387, Train_accy 60.01
2023-12-09 13:02:28,138 [foster.py] => Task 0, Epoch 116/200 => Loss 1.367, Train_accy 60.59, Test_accy 65.86
2023-12-09 13:02:36,788 [foster.py] => Task 0, Epoch 117/200 => Loss 1.365, Train_accy 60.70
2023-12-09 13:02:44,976 [foster.py] => Task 0, Epoch 118/200 => Loss 1.357, Train_accy 60.76
2023-12-09 13:02:53,443 [foster.py] => Task 0, Epoch 119/200 => Loss 1.350, Train_accy 60.92
2023-12-09 13:03:01,741 [foster.py] => Task 0, Epoch 120/200 => Loss 1.359, Train_accy 60.54
2023-12-09 13:03:12,191 [foster.py] => Task 0, Epoch 121/200 => Loss 1.336, Train_accy 61.76, Test_accy 69.70
2023-12-09 13:03:20,542 [foster.py] => Task 0, Epoch 122/200 => Loss 1.321, Train_accy 61.77
2023-12-09 13:03:29,288 [foster.py] => Task 0, Epoch 123/200 => Loss 1.322, Train_accy 62.06
2023-12-09 13:03:37,619 [foster.py] => Task 0, Epoch 124/200 => Loss 1.315, Train_accy 62.21
2023-12-09 13:03:45,760 [foster.py] => Task 0, Epoch 125/200 => Loss 1.302, Train_accy 62.10
2023-12-09 13:03:56,444 [foster.py] => Task 0, Epoch 126/200 => Loss 1.297, Train_accy 62.48, Test_accy 69.36
2023-12-09 13:04:05,052 [foster.py] => Task 0, Epoch 127/200 => Loss 1.296, Train_accy 62.38
2023-12-09 13:04:13,600 [foster.py] => Task 0, Epoch 128/200 => Loss 1.287, Train_accy 62.65
2023-12-09 13:04:22,268 [foster.py] => Task 0, Epoch 129/200 => Loss 1.278, Train_accy 62.98
2023-12-09 13:04:30,319 [foster.py] => Task 0, Epoch 130/200 => Loss 1.275, Train_accy 63.07
2023-12-09 13:04:40,739 [foster.py] => Task 0, Epoch 131/200 => Loss 1.262, Train_accy 63.50, Test_accy 66.42
2023-12-09 13:04:48,955 [foster.py] => Task 0, Epoch 132/200 => Loss 1.268, Train_accy 63.60
2023-12-09 13:04:57,552 [foster.py] => Task 0, Epoch 133/200 => Loss 1.268, Train_accy 63.50
2023-12-09 13:05:06,195 [foster.py] => Task 0, Epoch 134/200 => Loss 1.258, Train_accy 63.26
2023-12-09 13:05:14,878 [foster.py] => Task 0, Epoch 135/200 => Loss 1.233, Train_accy 64.25
2023-12-09 13:05:24,400 [foster.py] => Task 0, Epoch 136/200 => Loss 1.219, Train_accy 64.55, Test_accy 71.10
2023-12-09 13:05:33,002 [foster.py] => Task 0, Epoch 137/200 => Loss 1.207, Train_accy 64.88
2023-12-09 13:05:40,963 [foster.py] => Task 0, Epoch 138/200 => Loss 1.208, Train_accy 65.09
2023-12-09 13:05:48,975 [foster.py] => Task 0, Epoch 139/200 => Loss 1.204, Train_accy 65.16
2023-12-09 13:05:57,613 [foster.py] => Task 0, Epoch 140/200 => Loss 1.185, Train_accy 65.31
2023-12-09 13:06:08,256 [foster.py] => Task 0, Epoch 141/200 => Loss 1.182, Train_accy 65.82, Test_accy 71.60
2023-12-09 13:06:16,709 [foster.py] => Task 0, Epoch 142/200 => Loss 1.178, Train_accy 65.54
2023-12-09 13:06:25,437 [foster.py] => Task 0, Epoch 143/200 => Loss 1.179, Train_accy 65.44
2023-12-09 13:06:34,055 [foster.py] => Task 0, Epoch 144/200 => Loss 1.155, Train_accy 66.22
2023-12-09 13:06:42,451 [foster.py] => Task 0, Epoch 145/200 => Loss 1.148, Train_accy 66.46
2023-12-09 13:06:52,810 [foster.py] => Task 0, Epoch 146/200 => Loss 1.143, Train_accy 66.48, Test_accy 70.94
2023-12-09 13:07:01,501 [foster.py] => Task 0, Epoch 147/200 => Loss 1.137, Train_accy 66.94
2023-12-09 13:07:10,038 [foster.py] => Task 0, Epoch 148/200 => Loss 1.110, Train_accy 67.84
2023-12-09 13:07:18,728 [foster.py] => Task 0, Epoch 149/200 => Loss 1.118, Train_accy 67.48
2023-12-09 13:07:27,368 [foster.py] => Task 0, Epoch 150/200 => Loss 1.118, Train_accy 67.23
2023-12-09 13:07:37,799 [foster.py] => Task 0, Epoch 151/200 => Loss 1.117, Train_accy 66.92, Test_accy 74.50
2023-12-09 13:07:45,730 [foster.py] => Task 0, Epoch 152/200 => Loss 1.088, Train_accy 68.21
2023-12-09 13:07:54,226 [foster.py] => Task 0, Epoch 153/200 => Loss 1.068, Train_accy 68.77
2023-12-09 13:08:02,710 [foster.py] => Task 0, Epoch 154/200 => Loss 1.072, Train_accy 68.84
2023-12-09 13:08:11,386 [foster.py] => Task 0, Epoch 155/200 => Loss 1.063, Train_accy 68.79
2023-12-09 13:08:21,522 [foster.py] => Task 0, Epoch 156/200 => Loss 1.063, Train_accy 69.06, Test_accy 76.00
2023-12-09 13:08:30,353 [foster.py] => Task 0, Epoch 157/200 => Loss 1.038, Train_accy 69.58
2023-12-09 13:08:39,075 [foster.py] => Task 0, Epoch 158/200 => Loss 1.047, Train_accy 69.56
2023-12-09 13:08:47,116 [foster.py] => Task 0, Epoch 159/200 => Loss 1.029, Train_accy 69.97
2023-12-09 13:08:55,497 [foster.py] => Task 0, Epoch 160/200 => Loss 1.030, Train_accy 69.74
2023-12-09 13:09:06,240 [foster.py] => Task 0, Epoch 161/200 => Loss 1.011, Train_accy 70.14, Test_accy 77.02
2023-12-09 13:09:14,687 [foster.py] => Task 0, Epoch 162/200 => Loss 1.005, Train_accy 70.62
2023-12-09 13:09:23,421 [foster.py] => Task 0, Epoch 163/200 => Loss 0.997, Train_accy 70.86
2023-12-09 13:09:32,139 [foster.py] => Task 0, Epoch 164/200 => Loss 0.979, Train_accy 71.62
2023-12-09 13:09:40,288 [foster.py] => Task 0, Epoch 165/200 => Loss 0.970, Train_accy 71.33
2023-12-09 13:09:50,304 [foster.py] => Task 0, Epoch 166/200 => Loss 0.954, Train_accy 72.05, Test_accy 76.84
2023-12-09 13:09:58,945 [foster.py] => Task 0, Epoch 167/200 => Loss 0.957, Train_accy 72.02
2023-12-09 13:10:07,488 [foster.py] => Task 0, Epoch 168/200 => Loss 0.914, Train_accy 73.07
2023-12-09 13:10:16,227 [foster.py] => Task 0, Epoch 169/200 => Loss 0.938, Train_accy 72.59
2023-12-09 13:10:24,791 [foster.py] => Task 0, Epoch 170/200 => Loss 0.921, Train_accy 72.78
2023-12-09 13:10:35,010 [foster.py] => Task 0, Epoch 171/200 => Loss 0.918, Train_accy 73.14, Test_accy 78.02
2023-12-09 13:10:43,298 [foster.py] => Task 0, Epoch 172/200 => Loss 0.901, Train_accy 73.58
2023-12-09 13:10:51,601 [foster.py] => Task 0, Epoch 173/200 => Loss 0.898, Train_accy 73.82
2023-12-09 13:11:00,251 [foster.py] => Task 0, Epoch 174/200 => Loss 0.892, Train_accy 73.98
2023-12-09 13:11:08,946 [foster.py] => Task 0, Epoch 175/200 => Loss 0.870, Train_accy 74.63
2023-12-09 13:11:19,241 [foster.py] => Task 0, Epoch 176/200 => Loss 0.862, Train_accy 74.84, Test_accy 79.20
2023-12-09 13:11:27,876 [foster.py] => Task 0, Epoch 177/200 => Loss 0.854, Train_accy 74.66
2023-12-09 13:11:36,373 [foster.py] => Task 0, Epoch 178/200 => Loss 0.845, Train_accy 75.29
2023-12-09 13:11:44,336 [foster.py] => Task 0, Epoch 179/200 => Loss 0.841, Train_accy 75.34
2023-12-09 13:11:52,754 [foster.py] => Task 0, Epoch 180/200 => Loss 0.830, Train_accy 75.31
2023-12-09 13:12:02,478 [foster.py] => Task 0, Epoch 181/200 => Loss 0.822, Train_accy 75.98, Test_accy 79.44
2023-12-09 13:12:10,757 [foster.py] => Task 0, Epoch 182/200 => Loss 0.824, Train_accy 75.85
2023-12-09 13:12:18,637 [foster.py] => Task 0, Epoch 183/200 => Loss 0.809, Train_accy 76.11
2023-12-09 13:12:26,803 [foster.py] => Task 0, Epoch 184/200 => Loss 0.810, Train_accy 76.04
2023-12-09 13:12:34,512 [foster.py] => Task 0, Epoch 185/200 => Loss 0.803, Train_accy 76.28
2023-12-09 13:12:43,533 [foster.py] => Task 0, Epoch 186/200 => Loss 0.805, Train_accy 76.17, Test_accy 80.22
2023-12-09 13:12:51,615 [foster.py] => Task 0, Epoch 187/200 => Loss 0.804, Train_accy 76.18
2023-12-09 13:12:59,494 [foster.py] => Task 0, Epoch 188/200 => Loss 0.804, Train_accy 76.44
2023-12-09 13:13:07,543 [foster.py] => Task 0, Epoch 189/200 => Loss 0.793, Train_accy 76.55
2023-12-09 13:13:15,362 [foster.py] => Task 0, Epoch 190/200 => Loss 0.784, Train_accy 76.72
2023-12-09 13:13:25,134 [foster.py] => Task 0, Epoch 191/200 => Loss 0.793, Train_accy 76.70, Test_accy 80.80
2023-12-09 13:13:33,079 [foster.py] => Task 0, Epoch 192/200 => Loss 0.786, Train_accy 77.10
2023-12-09 13:13:40,886 [foster.py] => Task 0, Epoch 193/200 => Loss 0.775, Train_accy 77.26
2023-12-09 13:13:48,186 [foster.py] => Task 0, Epoch 194/200 => Loss 0.772, Train_accy 77.27
2023-12-09 13:13:56,315 [foster.py] => Task 0, Epoch 195/200 => Loss 0.769, Train_accy 77.66
2023-12-09 13:14:05,916 [foster.py] => Task 0, Epoch 196/200 => Loss 0.773, Train_accy 77.58, Test_accy 80.54
2023-12-09 13:14:13,721 [foster.py] => Task 0, Epoch 197/200 => Loss 0.761, Train_accy 77.58
2023-12-09 13:14:21,552 [foster.py] => Task 0, Epoch 198/200 => Loss 0.758, Train_accy 77.95
2023-12-09 13:14:29,518 [foster.py] => Task 0, Epoch 199/200 => Loss 0.763, Train_accy 77.58
2023-12-09 13:14:37,471 [foster.py] => Task 0, Epoch 200/200 => Loss 0.771, Train_accy 77.72
2023-12-09 13:14:37,474 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 13:15:58,361 [foster.py] => Exemplar size: 1000
2023-12-09 13:15:58,362 [trainer.py] => CNN: {'total': 80.38, '00-09': 84.7, '10-19': 75.1, '20-29': 83.4, '30-39': 76.3, '40-49': 82.4, 'old': 0, 'new': 80.38}
2023-12-09 13:15:58,362 [trainer.py] => NME: {'total': 78.66, '00-09': 82.4, '10-19': 72.6, '20-29': 80.9, '30-39': 75.5, '40-49': 81.9, 'old': 0, 'new': 78.66}
2023-12-09 13:15:58,362 [trainer.py] => CNN top1 curve: [80.38]
2023-12-09 13:15:58,362 [trainer.py] => CNN top5 curve: [96.48]
2023-12-09 13:15:58,362 [trainer.py] => NME top1 curve: [78.66]
2023-12-09 13:15:58,362 [trainer.py] => NME top5 curve: [96.2]

2023-12-09 13:15:58,362 [trainer.py] => All params: 470654
2023-12-09 13:15:58,363 [trainer.py] => Trainable params: 470654
2023-12-09 13:15:58,385 [foster.py] => Learning on 50-60
2023-12-09 13:15:58,386 [foster.py] => All params: 943198
2023-12-09 13:15:58,387 [foster.py] => Trainable params: 475794
2023-12-09 13:15:58,490 [foster.py] => per cls weights : [1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764 1.05080764
 1.05080764 1.05080764 0.74596179 0.74596179 0.74596179 0.74596179
 0.74596179 0.74596179 0.74596179 0.74596179 0.74596179 0.74596179]
2023-12-09 13:16:01,821 [foster.py] => Task 1, Epoch 1/170 => Loss 7.928, Loss_clf 1.930, Loss_fe 3.195, Loss_kd 2.336, Train_accy 44.03
2023-12-09 13:16:07,288 [foster.py] => Task 1, Epoch 2/170 => Loss 7.044, Loss_clf 1.342, Loss_fe 2.922, Loss_kd 2.317, Train_accy 49.88, Test_accy 67.98
2023-12-09 13:16:12,791 [foster.py] => Task 1, Epoch 3/170 => Loss 6.841, Loss_clf 1.301, Loss_fe 2.754, Loss_kd 2.321, Train_accy 49.75, Test_accy 72.02
2023-12-09 13:16:18,281 [foster.py] => Task 1, Epoch 4/170 => Loss 6.677, Loss_clf 1.220, Loss_fe 2.682, Loss_kd 2.312, Train_accy 52.02, Test_accy 71.38
2023-12-09 13:16:23,786 [foster.py] => Task 1, Epoch 5/170 => Loss 6.628, Loss_clf 1.228, Loss_fe 2.643, Loss_kd 2.299, Train_accy 52.33, Test_accy 72.27
2023-12-09 13:16:27,101 [foster.py] => Task 1, Epoch 6/170 => Loss 6.545, Loss_clf 1.176, Loss_fe 2.593, Loss_kd 2.314, Train_accy 51.52
2023-12-09 13:16:32,305 [foster.py] => Task 1, Epoch 7/170 => Loss 6.535, Loss_clf 1.208, Loss_fe 2.548, Loss_kd 2.316, Train_accy 51.97, Test_accy 71.18
2023-12-09 13:16:37,864 [foster.py] => Task 1, Epoch 8/170 => Loss 6.445, Loss_clf 1.158, Loss_fe 2.518, Loss_kd 2.307, Train_accy 53.93, Test_accy 72.57
2023-12-09 13:16:43,255 [foster.py] => Task 1, Epoch 9/170 => Loss 6.475, Loss_clf 1.188, Loss_fe 2.512, Loss_kd 2.313, Train_accy 53.18, Test_accy 70.30
2023-12-09 13:16:48,620 [foster.py] => Task 1, Epoch 10/170 => Loss 6.512, Loss_clf 1.269, Loss_fe 2.475, Loss_kd 2.306, Train_accy 52.20, Test_accy 70.72
2023-12-09 13:16:52,039 [foster.py] => Task 1, Epoch 11/170 => Loss 6.467, Loss_clf 1.234, Loss_fe 2.455, Loss_kd 2.315, Train_accy 53.50
2023-12-09 13:16:57,623 [foster.py] => Task 1, Epoch 12/170 => Loss 6.412, Loss_clf 1.216, Loss_fe 2.425, Loss_kd 2.309, Train_accy 52.82, Test_accy 71.97
2023-12-09 13:17:03,057 [foster.py] => Task 1, Epoch 13/170 => Loss 6.278, Loss_clf 1.122, Loss_fe 2.393, Loss_kd 2.303, Train_accy 55.27, Test_accy 71.77
2023-12-09 13:17:08,607 [foster.py] => Task 1, Epoch 14/170 => Loss 6.321, Loss_clf 1.165, Loss_fe 2.381, Loss_kd 2.313, Train_accy 53.72, Test_accy 71.80
2023-12-09 13:17:14,219 [foster.py] => Task 1, Epoch 15/170 => Loss 6.235, Loss_clf 1.134, Loss_fe 2.334, Loss_kd 2.306, Train_accy 54.07, Test_accy 72.33
2023-12-09 13:17:17,758 [foster.py] => Task 1, Epoch 16/170 => Loss 6.264, Loss_clf 1.143, Loss_fe 2.350, Loss_kd 2.309, Train_accy 53.98
2023-12-09 13:17:23,221 [foster.py] => Task 1, Epoch 17/170 => Loss 6.253, Loss_clf 1.149, Loss_fe 2.331, Loss_kd 2.312, Train_accy 53.83, Test_accy 72.35
2023-12-09 13:17:28,477 [foster.py] => Task 1, Epoch 18/170 => Loss 6.243, Loss_clf 1.165, Loss_fe 2.311, Loss_kd 2.306, Train_accy 55.33, Test_accy 71.98
2023-12-09 13:17:34,074 [foster.py] => Task 1, Epoch 19/170 => Loss 6.212, Loss_clf 1.137, Loss_fe 2.301, Loss_kd 2.311, Train_accy 54.78, Test_accy 72.38
2023-12-09 13:17:39,452 [foster.py] => Task 1, Epoch 20/170 => Loss 6.225, Loss_clf 1.173, Loss_fe 2.274, Loss_kd 2.315, Train_accy 54.62, Test_accy 72.00
2023-12-09 13:17:42,992 [foster.py] => Task 1, Epoch 21/170 => Loss 6.173, Loss_clf 1.126, Loss_fe 2.275, Loss_kd 2.310, Train_accy 54.18
2023-12-09 13:17:48,498 [foster.py] => Task 1, Epoch 22/170 => Loss 6.213, Loss_clf 1.176, Loss_fe 2.257, Loss_kd 2.317, Train_accy 53.62, Test_accy 72.08
2023-12-09 13:17:54,163 [foster.py] => Task 1, Epoch 23/170 => Loss 6.071, Loss_clf 1.090, Loss_fe 2.219, Loss_kd 2.302, Train_accy 55.88, Test_accy 72.63
2023-12-09 13:17:59,613 [foster.py] => Task 1, Epoch 24/170 => Loss 6.094, Loss_clf 1.123, Loss_fe 2.205, Loss_kd 2.305, Train_accy 54.87, Test_accy 72.15
2023-12-09 13:18:05,198 [foster.py] => Task 1, Epoch 25/170 => Loss 6.097, Loss_clf 1.145, Loss_fe 2.178, Loss_kd 2.311, Train_accy 54.77, Test_accy 72.02
2023-12-09 13:18:08,631 [foster.py] => Task 1, Epoch 26/170 => Loss 6.107, Loss_clf 1.154, Loss_fe 2.179, Loss_kd 2.313, Train_accy 54.38
2023-12-09 13:18:14,063 [foster.py] => Task 1, Epoch 27/170 => Loss 6.039, Loss_clf 1.121, Loss_fe 2.141, Loss_kd 2.314, Train_accy 55.05, Test_accy 73.05
2023-12-09 13:18:19,608 [foster.py] => Task 1, Epoch 28/170 => Loss 5.994, Loss_clf 1.084, Loss_fe 2.135, Loss_kd 2.313, Train_accy 55.45, Test_accy 70.05
2023-12-09 13:18:25,144 [foster.py] => Task 1, Epoch 29/170 => Loss 6.044, Loss_clf 1.126, Loss_fe 2.144, Loss_kd 2.312, Train_accy 55.87, Test_accy 72.67
2023-12-09 13:18:30,731 [foster.py] => Task 1, Epoch 30/170 => Loss 5.995, Loss_clf 1.141, Loss_fe 2.088, Loss_kd 2.305, Train_accy 55.42, Test_accy 72.35
2023-12-09 13:18:34,019 [foster.py] => Task 1, Epoch 31/170 => Loss 5.976, Loss_clf 1.110, Loss_fe 2.096, Loss_kd 2.308, Train_accy 56.10
2023-12-09 13:18:39,439 [foster.py] => Task 1, Epoch 32/170 => Loss 6.057, Loss_clf 1.182, Loss_fe 2.098, Loss_kd 2.315, Train_accy 55.40, Test_accy 72.57
2023-12-09 13:18:45,331 [foster.py] => Task 1, Epoch 33/170 => Loss 5.980, Loss_clf 1.136, Loss_fe 2.071, Loss_kd 2.311, Train_accy 55.58, Test_accy 72.63
2023-12-09 13:18:50,732 [foster.py] => Task 1, Epoch 34/170 => Loss 5.966, Loss_clf 1.150, Loss_fe 2.057, Loss_kd 2.300, Train_accy 56.23, Test_accy 72.05
2023-12-09 13:18:56,201 [foster.py] => Task 1, Epoch 35/170 => Loss 5.942, Loss_clf 1.127, Loss_fe 2.042, Loss_kd 2.310, Train_accy 55.77, Test_accy 72.60
2023-12-09 13:18:59,539 [foster.py] => Task 1, Epoch 36/170 => Loss 5.917, Loss_clf 1.113, Loss_fe 2.025, Loss_kd 2.316, Train_accy 56.63
2023-12-09 13:19:05,091 [foster.py] => Task 1, Epoch 37/170 => Loss 5.977, Loss_clf 1.167, Loss_fe 2.037, Loss_kd 2.311, Train_accy 56.00, Test_accy 72.23
2023-12-09 13:19:10,527 [foster.py] => Task 1, Epoch 38/170 => Loss 5.872, Loss_clf 1.099, Loss_fe 2.000, Loss_kd 2.311, Train_accy 56.23, Test_accy 72.77
2023-12-09 13:19:15,971 [foster.py] => Task 1, Epoch 39/170 => Loss 5.879, Loss_clf 1.116, Loss_fe 1.992, Loss_kd 2.309, Train_accy 56.17, Test_accy 72.88
2023-12-09 13:19:21,741 [foster.py] => Task 1, Epoch 40/170 => Loss 5.876, Loss_clf 1.116, Loss_fe 1.987, Loss_kd 2.311, Train_accy 56.55, Test_accy 72.82
2023-12-09 13:19:25,098 [foster.py] => Task 1, Epoch 41/170 => Loss 5.841, Loss_clf 1.083, Loss_fe 1.984, Loss_kd 2.311, Train_accy 56.62
2023-12-09 13:19:30,646 [foster.py] => Task 1, Epoch 42/170 => Loss 5.816, Loss_clf 1.056, Loss_fe 1.980, Loss_kd 2.317, Train_accy 56.87, Test_accy 70.42
2023-12-09 13:19:36,153 [foster.py] => Task 1, Epoch 43/170 => Loss 5.782, Loss_clf 1.071, Loss_fe 1.946, Loss_kd 2.304, Train_accy 57.55, Test_accy 73.27
2023-12-09 13:19:41,669 [foster.py] => Task 1, Epoch 44/170 => Loss 5.797, Loss_clf 1.076, Loss_fe 1.954, Loss_kd 2.306, Train_accy 56.77, Test_accy 73.02
2023-12-09 13:19:47,267 [foster.py] => Task 1, Epoch 45/170 => Loss 5.753, Loss_clf 1.046, Loss_fe 1.926, Loss_kd 2.317, Train_accy 58.65, Test_accy 72.40
2023-12-09 13:19:50,524 [foster.py] => Task 1, Epoch 46/170 => Loss 5.697, Loss_clf 1.016, Loss_fe 1.901, Loss_kd 2.317, Train_accy 57.77
2023-12-09 13:19:56,044 [foster.py] => Task 1, Epoch 47/170 => Loss 5.692, Loss_clf 1.016, Loss_fe 1.907, Loss_kd 2.308, Train_accy 59.10, Test_accy 73.08
2023-12-09 13:20:01,476 [foster.py] => Task 1, Epoch 48/170 => Loss 5.693, Loss_clf 1.028, Loss_fe 1.896, Loss_kd 2.308, Train_accy 58.25, Test_accy 69.78
2023-12-09 13:20:06,910 [foster.py] => Task 1, Epoch 49/170 => Loss 5.703, Loss_clf 1.050, Loss_fe 1.877, Loss_kd 2.314, Train_accy 58.68, Test_accy 73.05
2023-12-09 13:20:12,392 [foster.py] => Task 1, Epoch 50/170 => Loss 5.717, Loss_clf 1.073, Loss_fe 1.867, Loss_kd 2.314, Train_accy 58.73, Test_accy 71.93
2023-12-09 13:20:15,790 [foster.py] => Task 1, Epoch 51/170 => Loss 5.671, Loss_clf 1.027, Loss_fe 1.873, Loss_kd 2.310, Train_accy 58.87
2023-12-09 13:20:21,428 [foster.py] => Task 1, Epoch 52/170 => Loss 5.649, Loss_clf 1.028, Loss_fe 1.853, Loss_kd 2.307, Train_accy 58.92, Test_accy 73.05
2023-12-09 13:20:26,876 [foster.py] => Task 1, Epoch 53/170 => Loss 5.710, Loss_clf 1.071, Loss_fe 1.859, Loss_kd 2.317, Train_accy 58.90, Test_accy 73.07
2023-12-09 13:20:32,610 [foster.py] => Task 1, Epoch 54/170 => Loss 5.609, Loss_clf 1.013, Loss_fe 1.822, Loss_kd 2.311, Train_accy 59.00, Test_accy 72.58
2023-12-09 13:20:38,058 [foster.py] => Task 1, Epoch 55/170 => Loss 5.666, Loss_clf 1.043, Loss_fe 1.851, Loss_kd 2.311, Train_accy 59.17, Test_accy 73.97
2023-12-09 13:20:41,381 [foster.py] => Task 1, Epoch 56/170 => Loss 5.591, Loss_clf 1.007, Loss_fe 1.813, Loss_kd 2.308, Train_accy 59.57
2023-12-09 13:20:47,344 [foster.py] => Task 1, Epoch 57/170 => Loss 5.626, Loss_clf 1.040, Loss_fe 1.815, Loss_kd 2.309, Train_accy 58.55, Test_accy 73.60
2023-12-09 13:20:53,031 [foster.py] => Task 1, Epoch 58/170 => Loss 5.550, Loss_clf 0.987, Loss_fe 1.790, Loss_kd 2.311, Train_accy 60.57, Test_accy 72.23
2023-12-09 13:20:58,500 [foster.py] => Task 1, Epoch 59/170 => Loss 5.548, Loss_clf 1.018, Loss_fe 1.753, Loss_kd 2.314, Train_accy 59.63, Test_accy 73.77
2023-12-09 13:21:04,155 [foster.py] => Task 1, Epoch 60/170 => Loss 5.564, Loss_clf 1.025, Loss_fe 1.766, Loss_kd 2.311, Train_accy 60.45, Test_accy 73.90
2023-12-09 13:21:07,615 [foster.py] => Task 1, Epoch 61/170 => Loss 5.525, Loss_clf 0.979, Loss_fe 1.766, Loss_kd 2.317, Train_accy 60.25
2023-12-09 13:21:13,191 [foster.py] => Task 1, Epoch 62/170 => Loss 5.490, Loss_clf 0.961, Loss_fe 1.742, Loss_kd 2.322, Train_accy 60.92, Test_accy 73.63
2023-12-09 13:21:18,750 [foster.py] => Task 1, Epoch 63/170 => Loss 5.469, Loss_clf 0.963, Loss_fe 1.730, Loss_kd 2.313, Train_accy 60.97, Test_accy 71.77
2023-12-09 13:21:24,909 [foster.py] => Task 1, Epoch 64/170 => Loss 5.537, Loss_clf 1.032, Loss_fe 1.729, Loss_kd 2.314, Train_accy 60.07, Test_accy 73.60
2023-12-09 13:21:30,658 [foster.py] => Task 1, Epoch 65/170 => Loss 5.401, Loss_clf 0.945, Loss_fe 1.691, Loss_kd 2.304, Train_accy 61.93, Test_accy 73.18
2023-12-09 13:21:34,130 [foster.py] => Task 1, Epoch 66/170 => Loss 5.432, Loss_clf 0.959, Loss_fe 1.706, Loss_kd 2.306, Train_accy 61.03
2023-12-09 13:21:40,167 [foster.py] => Task 1, Epoch 67/170 => Loss 5.459, Loss_clf 0.999, Loss_fe 1.690, Loss_kd 2.308, Train_accy 60.57, Test_accy 73.15
2023-12-09 13:21:46,087 [foster.py] => Task 1, Epoch 68/170 => Loss 5.431, Loss_clf 0.970, Loss_fe 1.686, Loss_kd 2.312, Train_accy 61.00, Test_accy 73.55
2023-12-09 13:21:52,126 [foster.py] => Task 1, Epoch 69/170 => Loss 5.385, Loss_clf 0.948, Loss_fe 1.670, Loss_kd 2.307, Train_accy 62.08, Test_accy 73.60
2023-12-09 13:21:58,369 [foster.py] => Task 1, Epoch 70/170 => Loss 5.344, Loss_clf 0.925, Loss_fe 1.651, Loss_kd 2.306, Train_accy 62.50, Test_accy 72.98
2023-12-09 13:22:02,136 [foster.py] => Task 1, Epoch 71/170 => Loss 5.344, Loss_clf 0.923, Loss_fe 1.655, Loss_kd 2.305, Train_accy 63.02
2023-12-09 13:22:08,138 [foster.py] => Task 1, Epoch 72/170 => Loss 5.336, Loss_clf 0.922, Loss_fe 1.657, Loss_kd 2.298, Train_accy 61.72, Test_accy 73.57
2023-12-09 13:22:14,516 [foster.py] => Task 1, Epoch 73/170 => Loss 5.315, Loss_clf 0.929, Loss_fe 1.627, Loss_kd 2.299, Train_accy 62.22, Test_accy 72.87
2023-12-09 13:22:20,145 [foster.py] => Task 1, Epoch 74/170 => Loss 5.346, Loss_clf 0.944, Loss_fe 1.626, Loss_kd 2.313, Train_accy 62.55, Test_accy 73.63
2023-12-09 13:22:25,674 [foster.py] => Task 1, Epoch 75/170 => Loss 5.288, Loss_clf 0.916, Loss_fe 1.598, Loss_kd 2.311, Train_accy 63.17, Test_accy 71.90
2023-12-09 13:22:29,074 [foster.py] => Task 1, Epoch 76/170 => Loss 5.328, Loss_clf 0.946, Loss_fe 1.613, Loss_kd 2.307, Train_accy 61.63
2023-12-09 13:22:34,478 [foster.py] => Task 1, Epoch 77/170 => Loss 5.265, Loss_clf 0.899, Loss_fe 1.598, Loss_kd 2.307, Train_accy 63.12, Test_accy 73.92
2023-12-09 13:22:40,051 [foster.py] => Task 1, Epoch 78/170 => Loss 5.259, Loss_clf 0.912, Loss_fe 1.575, Loss_kd 2.310, Train_accy 62.93, Test_accy 72.65
2023-12-09 13:22:45,555 [foster.py] => Task 1, Epoch 79/170 => Loss 5.271, Loss_clf 0.913, Loss_fe 1.592, Loss_kd 2.305, Train_accy 62.72, Test_accy 74.75
2023-12-09 13:22:51,116 [foster.py] => Task 1, Epoch 80/170 => Loss 5.227, Loss_clf 0.890, Loss_fe 1.572, Loss_kd 2.303, Train_accy 63.92, Test_accy 73.70
2023-12-09 13:22:54,613 [foster.py] => Task 1, Epoch 81/170 => Loss 5.210, Loss_clf 0.884, Loss_fe 1.567, Loss_kd 2.299, Train_accy 63.37
2023-12-09 13:23:00,235 [foster.py] => Task 1, Epoch 82/170 => Loss 5.190, Loss_clf 0.880, Loss_fe 1.537, Loss_kd 2.311, Train_accy 64.20, Test_accy 73.87
2023-12-09 13:23:05,741 [foster.py] => Task 1, Epoch 83/170 => Loss 5.195, Loss_clf 0.885, Loss_fe 1.540, Loss_kd 2.309, Train_accy 63.28, Test_accy 74.27
2023-12-09 13:23:11,225 [foster.py] => Task 1, Epoch 84/170 => Loss 5.220, Loss_clf 0.886, Loss_fe 1.554, Loss_kd 2.317, Train_accy 63.90, Test_accy 74.38
2023-12-09 13:23:16,739 [foster.py] => Task 1, Epoch 85/170 => Loss 5.215, Loss_clf 0.906, Loss_fe 1.526, Loss_kd 2.320, Train_accy 63.85, Test_accy 74.48
2023-12-09 13:23:20,102 [foster.py] => Task 1, Epoch 86/170 => Loss 5.155, Loss_clf 0.894, Loss_fe 1.507, Loss_kd 2.295, Train_accy 64.30
2023-12-09 13:23:25,625 [foster.py] => Task 1, Epoch 87/170 => Loss 5.120, Loss_clf 0.854, Loss_fe 1.491, Loss_kd 2.313, Train_accy 64.98, Test_accy 73.57
2023-12-09 13:23:31,181 [foster.py] => Task 1, Epoch 88/170 => Loss 5.159, Loss_clf 0.874, Loss_fe 1.516, Loss_kd 2.308, Train_accy 63.58, Test_accy 74.78
2023-12-09 13:23:36,640 [foster.py] => Task 1, Epoch 89/170 => Loss 5.132, Loss_clf 0.854, Loss_fe 1.508, Loss_kd 2.309, Train_accy 65.00, Test_accy 74.40
2023-12-09 13:23:42,206 [foster.py] => Task 1, Epoch 90/170 => Loss 5.091, Loss_clf 0.852, Loss_fe 1.474, Loss_kd 2.304, Train_accy 64.73, Test_accy 74.32
2023-12-09 13:23:45,637 [foster.py] => Task 1, Epoch 91/170 => Loss 5.103, Loss_clf 0.865, Loss_fe 1.470, Loss_kd 2.307, Train_accy 64.48
2023-12-09 13:23:51,223 [foster.py] => Task 1, Epoch 92/170 => Loss 5.078, Loss_clf 0.851, Loss_fe 1.456, Loss_kd 2.309, Train_accy 65.70, Test_accy 75.67
2023-12-09 13:23:56,552 [foster.py] => Task 1, Epoch 93/170 => Loss 4.996, Loss_clf 0.820, Loss_fe 1.424, Loss_kd 2.293, Train_accy 66.47, Test_accy 74.22
2023-12-09 13:24:02,019 [foster.py] => Task 1, Epoch 94/170 => Loss 5.028, Loss_clf 0.839, Loss_fe 1.428, Loss_kd 2.301, Train_accy 66.10, Test_accy 74.32
2023-12-09 13:24:07,380 [foster.py] => Task 1, Epoch 95/170 => Loss 5.070, Loss_clf 0.852, Loss_fe 1.446, Loss_kd 2.310, Train_accy 65.75, Test_accy 74.82
2023-12-09 13:24:10,920 [foster.py] => Task 1, Epoch 96/170 => Loss 4.979, Loss_clf 0.808, Loss_fe 1.409, Loss_kd 2.302, Train_accy 66.83
2023-12-09 13:24:16,376 [foster.py] => Task 1, Epoch 97/170 => Loss 5.020, Loss_clf 0.833, Loss_fe 1.420, Loss_kd 2.305, Train_accy 65.40, Test_accy 75.28
2023-12-09 13:24:21,806 [foster.py] => Task 1, Epoch 98/170 => Loss 4.965, Loss_clf 0.791, Loss_fe 1.397, Loss_kd 2.314, Train_accy 67.40, Test_accy 74.63
2023-12-09 13:24:27,256 [foster.py] => Task 1, Epoch 99/170 => Loss 5.036, Loss_clf 0.848, Loss_fe 1.412, Loss_kd 2.313, Train_accy 65.82, Test_accy 75.00
2023-12-09 13:24:32,686 [foster.py] => Task 1, Epoch 100/170 => Loss 4.974, Loss_clf 0.816, Loss_fe 1.390, Loss_kd 2.307, Train_accy 67.03, Test_accy 75.27
2023-12-09 13:24:35,943 [foster.py] => Task 1, Epoch 101/170 => Loss 4.972, Loss_clf 0.817, Loss_fe 1.387, Loss_kd 2.306, Train_accy 66.68
2023-12-09 13:24:41,359 [foster.py] => Task 1, Epoch 102/170 => Loss 4.959, Loss_clf 0.802, Loss_fe 1.386, Loss_kd 2.310, Train_accy 66.93, Test_accy 74.85
2023-12-09 13:24:46,788 [foster.py] => Task 1, Epoch 103/170 => Loss 4.938, Loss_clf 0.801, Loss_fe 1.367, Loss_kd 2.309, Train_accy 67.10, Test_accy 75.45
2023-12-09 13:24:52,247 [foster.py] => Task 1, Epoch 104/170 => Loss 4.914, Loss_clf 0.796, Loss_fe 1.355, Loss_kd 2.303, Train_accy 67.20, Test_accy 74.70
2023-12-09 13:24:57,749 [foster.py] => Task 1, Epoch 105/170 => Loss 4.869, Loss_clf 0.774, Loss_fe 1.325, Loss_kd 2.309, Train_accy 68.55, Test_accy 75.65
2023-12-09 13:25:01,078 [foster.py] => Task 1, Epoch 106/170 => Loss 4.874, Loss_clf 0.768, Loss_fe 1.334, Loss_kd 2.310, Train_accy 67.88
2023-12-09 13:25:06,647 [foster.py] => Task 1, Epoch 107/170 => Loss 4.869, Loss_clf 0.780, Loss_fe 1.313, Loss_kd 2.314, Train_accy 68.00, Test_accy 74.58
2023-12-09 13:25:12,083 [foster.py] => Task 1, Epoch 108/170 => Loss 4.846, Loss_clf 0.768, Loss_fe 1.308, Loss_kd 2.308, Train_accy 68.37, Test_accy 74.98
2023-12-09 13:25:17,554 [foster.py] => Task 1, Epoch 109/170 => Loss 4.868, Loss_clf 0.773, Loss_fe 1.322, Loss_kd 2.310, Train_accy 68.32, Test_accy 74.63
2023-12-09 13:25:23,083 [foster.py] => Task 1, Epoch 110/170 => Loss 4.838, Loss_clf 0.763, Loss_fe 1.309, Loss_kd 2.305, Train_accy 68.63, Test_accy 74.68
2023-12-09 13:25:26,383 [foster.py] => Task 1, Epoch 111/170 => Loss 4.770, Loss_clf 0.742, Loss_fe 1.271, Loss_kd 2.297, Train_accy 69.42
2023-12-09 13:25:31,879 [foster.py] => Task 1, Epoch 112/170 => Loss 4.797, Loss_clf 0.755, Loss_fe 1.278, Loss_kd 2.304, Train_accy 69.03, Test_accy 75.35
2023-12-09 13:25:37,412 [foster.py] => Task 1, Epoch 113/170 => Loss 4.767, Loss_clf 0.753, Loss_fe 1.251, Loss_kd 2.303, Train_accy 69.12, Test_accy 75.68
2023-12-09 13:25:42,950 [foster.py] => Task 1, Epoch 114/170 => Loss 4.816, Loss_clf 0.768, Loss_fe 1.284, Loss_kd 2.303, Train_accy 68.43, Test_accy 75.10
2023-12-09 13:25:48,467 [foster.py] => Task 1, Epoch 115/170 => Loss 4.765, Loss_clf 0.738, Loss_fe 1.259, Loss_kd 2.307, Train_accy 69.03, Test_accy 75.22
2023-12-09 13:25:51,838 [foster.py] => Task 1, Epoch 116/170 => Loss 4.707, Loss_clf 0.727, Loss_fe 1.224, Loss_kd 2.297, Train_accy 69.20
2023-12-09 13:25:57,362 [foster.py] => Task 1, Epoch 117/170 => Loss 4.723, Loss_clf 0.721, Loss_fe 1.229, Loss_kd 2.310, Train_accy 70.13, Test_accy 75.25
2023-12-09 13:26:02,661 [foster.py] => Task 1, Epoch 118/170 => Loss 4.711, Loss_clf 0.719, Loss_fe 1.222, Loss_kd 2.308, Train_accy 69.65, Test_accy 75.92
2023-12-09 13:26:08,028 [foster.py] => Task 1, Epoch 119/170 => Loss 4.615, Loss_clf 0.681, Loss_fe 1.176, Loss_kd 2.298, Train_accy 71.47, Test_accy 75.48
2023-12-09 13:26:13,543 [foster.py] => Task 1, Epoch 120/170 => Loss 4.677, Loss_clf 0.704, Loss_fe 1.201, Loss_kd 2.310, Train_accy 70.40, Test_accy 76.00
2023-12-09 13:26:16,868 [foster.py] => Task 1, Epoch 121/170 => Loss 4.633, Loss_clf 0.683, Loss_fe 1.177, Loss_kd 2.311, Train_accy 71.77
2023-12-09 13:26:22,262 [foster.py] => Task 1, Epoch 122/170 => Loss 4.693, Loss_clf 0.719, Loss_fe 1.199, Loss_kd 2.312, Train_accy 71.62, Test_accy 75.32
2023-12-09 13:26:27,708 [foster.py] => Task 1, Epoch 123/170 => Loss 4.592, Loss_clf 0.674, Loss_fe 1.164, Loss_kd 2.295, Train_accy 71.58, Test_accy 75.83
2023-12-09 13:26:33,186 [foster.py] => Task 1, Epoch 124/170 => Loss 4.617, Loss_clf 0.677, Loss_fe 1.166, Loss_kd 2.311, Train_accy 71.83, Test_accy 75.45
2023-12-09 13:26:38,587 [foster.py] => Task 1, Epoch 125/170 => Loss 4.658, Loss_clf 0.699, Loss_fe 1.180, Loss_kd 2.315, Train_accy 71.08, Test_accy 75.70
2023-12-09 13:26:42,002 [foster.py] => Task 1, Epoch 126/170 => Loss 4.582, Loss_clf 0.673, Loss_fe 1.138, Loss_kd 2.309, Train_accy 72.13
2023-12-09 13:26:47,574 [foster.py] => Task 1, Epoch 127/170 => Loss 4.576, Loss_clf 0.670, Loss_fe 1.142, Loss_kd 2.304, Train_accy 72.10, Test_accy 75.78
2023-12-09 13:26:53,160 [foster.py] => Task 1, Epoch 128/170 => Loss 4.556, Loss_clf 0.665, Loss_fe 1.128, Loss_kd 2.303, Train_accy 72.02, Test_accy 76.68
2023-12-09 13:26:58,772 [foster.py] => Task 1, Epoch 129/170 => Loss 4.541, Loss_clf 0.662, Loss_fe 1.125, Loss_kd 2.295, Train_accy 72.37, Test_accy 76.03
2023-12-09 13:27:04,267 [foster.py] => Task 1, Epoch 130/170 => Loss 4.522, Loss_clf 0.647, Loss_fe 1.106, Loss_kd 2.307, Train_accy 72.82, Test_accy 75.83
2023-12-09 13:27:07,677 [foster.py] => Task 1, Epoch 131/170 => Loss 4.551, Loss_clf 0.669, Loss_fe 1.122, Loss_kd 2.300, Train_accy 72.72
2023-12-09 13:27:13,187 [foster.py] => Task 1, Epoch 132/170 => Loss 4.520, Loss_clf 0.648, Loss_fe 1.104, Loss_kd 2.306, Train_accy 73.55, Test_accy 75.93
2023-12-09 13:27:18,680 [foster.py] => Task 1, Epoch 133/170 => Loss 4.519, Loss_clf 0.649, Loss_fe 1.092, Loss_kd 2.315, Train_accy 72.82, Test_accy 76.27
2023-12-09 13:27:24,123 [foster.py] => Task 1, Epoch 134/170 => Loss 4.531, Loss_clf 0.643, Loss_fe 1.101, Loss_kd 2.323, Train_accy 73.38, Test_accy 76.27
2023-12-09 13:27:29,572 [foster.py] => Task 1, Epoch 135/170 => Loss 4.427, Loss_clf 0.624, Loss_fe 1.045, Loss_kd 2.299, Train_accy 73.47, Test_accy 75.88
2023-12-09 13:27:32,909 [foster.py] => Task 1, Epoch 136/170 => Loss 4.445, Loss_clf 0.625, Loss_fe 1.047, Loss_kd 2.311, Train_accy 73.48
2023-12-09 13:27:38,253 [foster.py] => Task 1, Epoch 137/170 => Loss 4.458, Loss_clf 0.631, Loss_fe 1.055, Loss_kd 2.310, Train_accy 73.45, Test_accy 76.37
2023-12-09 13:27:43,823 [foster.py] => Task 1, Epoch 138/170 => Loss 4.411, Loss_clf 0.611, Loss_fe 1.042, Loss_kd 2.299, Train_accy 74.25, Test_accy 76.30
2023-12-09 13:27:49,299 [foster.py] => Task 1, Epoch 139/170 => Loss 4.439, Loss_clf 0.616, Loss_fe 1.046, Loss_kd 2.315, Train_accy 74.23, Test_accy 76.25
2023-12-09 13:27:54,792 [foster.py] => Task 1, Epoch 140/170 => Loss 4.395, Loss_clf 0.600, Loss_fe 1.032, Loss_kd 2.302, Train_accy 75.37, Test_accy 76.42
2023-12-09 13:27:58,086 [foster.py] => Task 1, Epoch 141/170 => Loss 4.426, Loss_clf 0.621, Loss_fe 1.036, Loss_kd 2.308, Train_accy 73.90
2023-12-09 13:28:03,808 [foster.py] => Task 1, Epoch 142/170 => Loss 4.384, Loss_clf 0.602, Loss_fe 1.018, Loss_kd 2.304, Train_accy 75.40, Test_accy 76.15
2023-12-09 13:28:09,196 [foster.py] => Task 1, Epoch 143/170 => Loss 4.385, Loss_clf 0.601, Loss_fe 1.013, Loss_kd 2.309, Train_accy 74.47, Test_accy 76.02
2023-12-09 13:28:14,571 [foster.py] => Task 1, Epoch 144/170 => Loss 4.391, Loss_clf 0.612, Loss_fe 1.009, Loss_kd 2.309, Train_accy 74.17, Test_accy 76.17
2023-12-09 13:28:20,070 [foster.py] => Task 1, Epoch 145/170 => Loss 4.315, Loss_clf 0.576, Loss_fe 0.980, Loss_kd 2.299, Train_accy 75.18, Test_accy 76.42
2023-12-09 13:28:23,457 [foster.py] => Task 1, Epoch 146/170 => Loss 4.312, Loss_clf 0.578, Loss_fe 0.969, Loss_kd 2.304, Train_accy 75.27
2023-12-09 13:28:28,951 [foster.py] => Task 1, Epoch 147/170 => Loss 4.342, Loss_clf 0.594, Loss_fe 0.990, Loss_kd 2.298, Train_accy 75.28, Test_accy 76.08
2023-12-09 13:28:34,429 [foster.py] => Task 1, Epoch 148/170 => Loss 4.307, Loss_clf 0.575, Loss_fe 0.972, Loss_kd 2.300, Train_accy 75.67, Test_accy 76.95
2023-12-09 13:28:40,155 [foster.py] => Task 1, Epoch 149/170 => Loss 4.305, Loss_clf 0.578, Loss_fe 0.965, Loss_kd 2.301, Train_accy 76.27, Test_accy 76.15
2023-12-09 13:28:45,549 [foster.py] => Task 1, Epoch 150/170 => Loss 4.316, Loss_clf 0.588, Loss_fe 0.967, Loss_kd 2.302, Train_accy 75.60, Test_accy 76.48
2023-12-09 13:28:48,844 [foster.py] => Task 1, Epoch 151/170 => Loss 4.292, Loss_clf 0.574, Loss_fe 0.949, Loss_kd 2.308, Train_accy 76.25
2023-12-09 13:28:54,552 [foster.py] => Task 1, Epoch 152/170 => Loss 4.269, Loss_clf 0.560, Loss_fe 0.942, Loss_kd 2.306, Train_accy 76.40, Test_accy 76.70
2023-12-09 13:28:59,966 [foster.py] => Task 1, Epoch 153/170 => Loss 4.240, Loss_clf 0.552, Loss_fe 0.934, Loss_kd 2.295, Train_accy 76.78, Test_accy 76.47
2023-12-09 13:29:05,430 [foster.py] => Task 1, Epoch 154/170 => Loss 4.285, Loss_clf 0.574, Loss_fe 0.945, Loss_kd 2.305, Train_accy 76.63, Test_accy 76.38
2023-12-09 13:29:10,813 [foster.py] => Task 1, Epoch 155/170 => Loss 4.267, Loss_clf 0.559, Loss_fe 0.939, Loss_kd 2.307, Train_accy 75.78, Test_accy 76.55
2023-12-09 13:29:14,133 [foster.py] => Task 1, Epoch 156/170 => Loss 4.240, Loss_clf 0.546, Loss_fe 0.925, Loss_kd 2.307, Train_accy 76.65
2023-12-09 13:29:19,567 [foster.py] => Task 1, Epoch 157/170 => Loss 4.246, Loss_clf 0.557, Loss_fe 0.926, Loss_kd 2.303, Train_accy 76.77, Test_accy 76.33
2023-12-09 13:29:25,038 [foster.py] => Task 1, Epoch 158/170 => Loss 4.270, Loss_clf 0.562, Loss_fe 0.937, Loss_kd 2.309, Train_accy 77.18, Test_accy 76.57
2023-12-09 13:29:30,471 [foster.py] => Task 1, Epoch 159/170 => Loss 4.224, Loss_clf 0.549, Loss_fe 0.914, Loss_kd 2.301, Train_accy 77.10, Test_accy 76.75
2023-12-09 13:29:35,990 [foster.py] => Task 1, Epoch 160/170 => Loss 4.233, Loss_clf 0.553, Loss_fe 0.916, Loss_kd 2.303, Train_accy 76.80, Test_accy 76.72
2023-12-09 13:29:39,462 [foster.py] => Task 1, Epoch 161/170 => Loss 4.252, Loss_clf 0.566, Loss_fe 0.925, Loss_kd 2.301, Train_accy 75.97
2023-12-09 13:29:44,805 [foster.py] => Task 1, Epoch 162/170 => Loss 4.251, Loss_clf 0.565, Loss_fe 0.920, Loss_kd 2.305, Train_accy 76.45, Test_accy 76.67
2023-12-09 13:29:50,272 [foster.py] => Task 1, Epoch 163/170 => Loss 4.204, Loss_clf 0.526, Loss_fe 0.906, Loss_kd 2.310, Train_accy 76.85, Test_accy 76.68
2023-12-09 13:29:55,846 [foster.py] => Task 1, Epoch 164/170 => Loss 4.248, Loss_clf 0.552, Loss_fe 0.922, Loss_kd 2.311, Train_accy 76.57, Test_accy 76.55
2023-12-09 13:30:01,303 [foster.py] => Task 1, Epoch 165/170 => Loss 4.198, Loss_clf 0.538, Loss_fe 0.889, Loss_kd 2.309, Train_accy 76.95, Test_accy 76.68
2023-12-09 13:30:04,628 [foster.py] => Task 1, Epoch 166/170 => Loss 4.205, Loss_clf 0.544, Loss_fe 0.902, Loss_kd 2.299, Train_accy 76.85
2023-12-09 13:30:10,291 [foster.py] => Task 1, Epoch 167/170 => Loss 4.266, Loss_clf 0.556, Loss_fe 0.936, Loss_kd 2.312, Train_accy 76.73, Test_accy 76.75
2023-12-09 13:30:15,705 [foster.py] => Task 1, Epoch 168/170 => Loss 4.229, Loss_clf 0.549, Loss_fe 0.906, Loss_kd 2.311, Train_accy 76.77, Test_accy 76.68
2023-12-09 13:30:21,292 [foster.py] => Task 1, Epoch 169/170 => Loss 4.221, Loss_clf 0.546, Loss_fe 0.902, Loss_kd 2.311, Train_accy 77.07, Test_accy 76.65
2023-12-09 13:30:26,852 [foster.py] => Task 1, Epoch 170/170 => Loss 4.211, Loss_clf 0.548, Loss_fe 0.905, Loss_kd 2.299, Train_accy 77.28, Test_accy 76.75
2023-12-09 13:30:26,853 [foster.py] => do not weight align teacher!
2023-12-09 13:30:26,856 [foster.py] => per cls weights : [1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527 1.09966527
 1.09966527 1.09966527 0.50167364 0.50167364 0.50167364 0.50167364
 0.50167364 0.50167364 0.50167364 0.50167364 0.50167364 0.50167364]
2023-12-09 13:30:32,351 [foster.py] => SNet: Task 1, Epoch 1/130 => Loss 3.095,  Train_accy 31.53, Test_accy 64.05
2023-12-09 13:30:35,988 [foster.py] => SNet: Task 1, Epoch 2/130 => Loss 2.901,  Train_accy 48.48
2023-12-09 13:30:39,593 [foster.py] => SNet: Task 1, Epoch 3/130 => Loss 2.879,  Train_accy 51.33
2023-12-09 13:30:43,384 [foster.py] => SNet: Task 1, Epoch 4/130 => Loss 2.857,  Train_accy 52.98
2023-12-09 13:30:47,062 [foster.py] => SNet: Task 1, Epoch 5/130 => Loss 2.865,  Train_accy 54.82
2023-12-09 13:30:52,492 [foster.py] => SNet: Task 1, Epoch 6/130 => Loss 2.842,  Train_accy 54.87, Test_accy 70.55
2023-12-09 13:30:56,107 [foster.py] => SNet: Task 1, Epoch 7/130 => Loss 2.849,  Train_accy 55.05
2023-12-09 13:30:59,814 [foster.py] => SNet: Task 1, Epoch 8/130 => Loss 2.830,  Train_accy 55.63
2023-12-09 13:31:03,448 [foster.py] => SNet: Task 1, Epoch 9/130 => Loss 2.838,  Train_accy 55.87
2023-12-09 13:31:07,203 [foster.py] => SNet: Task 1, Epoch 10/130 => Loss 2.834,  Train_accy 56.70
2023-12-09 13:31:12,806 [foster.py] => SNet: Task 1, Epoch 11/130 => Loss 2.839,  Train_accy 55.07, Test_accy 71.43
2023-12-09 13:31:16,504 [foster.py] => SNet: Task 1, Epoch 12/130 => Loss 2.825,  Train_accy 57.28
2023-12-09 13:31:20,302 [foster.py] => SNet: Task 1, Epoch 13/130 => Loss 2.819,  Train_accy 56.87
2023-12-09 13:31:24,076 [foster.py] => SNet: Task 1, Epoch 14/130 => Loss 2.829,  Train_accy 57.10
2023-12-09 13:31:27,985 [foster.py] => SNet: Task 1, Epoch 15/130 => Loss 2.819,  Train_accy 57.20
2023-12-09 13:31:33,471 [foster.py] => SNet: Task 1, Epoch 16/130 => Loss 2.803,  Train_accy 58.50, Test_accy 69.00
2023-12-09 13:31:37,169 [foster.py] => SNet: Task 1, Epoch 17/130 => Loss 2.825,  Train_accy 57.20
2023-12-09 13:31:40,913 [foster.py] => SNet: Task 1, Epoch 18/130 => Loss 2.810,  Train_accy 57.37
2023-12-09 13:31:44,516 [foster.py] => SNet: Task 1, Epoch 19/130 => Loss 2.817,  Train_accy 58.22
2023-12-09 13:31:47,826 [foster.py] => SNet: Task 1, Epoch 20/130 => Loss 2.816,  Train_accy 58.17
2023-12-09 13:31:53,165 [foster.py] => SNet: Task 1, Epoch 21/130 => Loss 2.805,  Train_accy 56.83, Test_accy 71.58
2023-12-09 13:31:56,986 [foster.py] => SNet: Task 1, Epoch 22/130 => Loss 2.826,  Train_accy 58.22
2023-12-09 13:32:00,654 [foster.py] => SNet: Task 1, Epoch 23/130 => Loss 2.832,  Train_accy 57.85
2023-12-09 13:32:04,319 [foster.py] => SNet: Task 1, Epoch 24/130 => Loss 2.818,  Train_accy 58.85
2023-12-09 13:32:08,018 [foster.py] => SNet: Task 1, Epoch 25/130 => Loss 2.807,  Train_accy 58.97
2023-12-09 13:32:13,476 [foster.py] => SNet: Task 1, Epoch 26/130 => Loss 2.807,  Train_accy 59.20, Test_accy 73.45
2023-12-09 13:32:17,224 [foster.py] => SNet: Task 1, Epoch 27/130 => Loss 2.821,  Train_accy 59.13
2023-12-09 13:32:20,843 [foster.py] => SNet: Task 1, Epoch 28/130 => Loss 2.806,  Train_accy 58.45
2023-12-09 13:32:24,414 [foster.py] => SNet: Task 1, Epoch 29/130 => Loss 2.818,  Train_accy 59.10
2023-12-09 13:32:28,066 [foster.py] => SNet: Task 1, Epoch 30/130 => Loss 2.807,  Train_accy 58.85
2023-12-09 13:32:33,488 [foster.py] => SNet: Task 1, Epoch 31/130 => Loss 2.801,  Train_accy 58.22, Test_accy 72.77
2023-12-09 13:32:37,161 [foster.py] => SNet: Task 1, Epoch 32/130 => Loss 2.798,  Train_accy 59.20
2023-12-09 13:32:40,840 [foster.py] => SNet: Task 1, Epoch 33/130 => Loss 2.803,  Train_accy 59.07
2023-12-09 13:32:44,410 [foster.py] => SNet: Task 1, Epoch 34/130 => Loss 2.808,  Train_accy 58.95
2023-12-09 13:32:47,998 [foster.py] => SNet: Task 1, Epoch 35/130 => Loss 2.812,  Train_accy 58.62
2023-12-09 13:32:53,669 [foster.py] => SNet: Task 1, Epoch 36/130 => Loss 2.805,  Train_accy 59.68, Test_accy 73.43
2023-12-09 13:32:57,419 [foster.py] => SNet: Task 1, Epoch 37/130 => Loss 2.795,  Train_accy 58.83
2023-12-09 13:33:01,096 [foster.py] => SNet: Task 1, Epoch 38/130 => Loss 2.796,  Train_accy 59.07
2023-12-09 13:33:04,703 [foster.py] => SNet: Task 1, Epoch 39/130 => Loss 2.799,  Train_accy 59.32
2023-12-09 13:33:08,361 [foster.py] => SNet: Task 1, Epoch 40/130 => Loss 2.794,  Train_accy 60.47
2023-12-09 13:33:13,840 [foster.py] => SNet: Task 1, Epoch 41/130 => Loss 2.799,  Train_accy 58.90, Test_accy 73.48
2023-12-09 13:33:17,460 [foster.py] => SNet: Task 1, Epoch 42/130 => Loss 2.800,  Train_accy 59.30
2023-12-09 13:33:21,131 [foster.py] => SNet: Task 1, Epoch 43/130 => Loss 2.798,  Train_accy 60.45
2023-12-09 13:33:24,741 [foster.py] => SNet: Task 1, Epoch 44/130 => Loss 2.800,  Train_accy 59.67
2023-12-09 13:33:28,483 [foster.py] => SNet: Task 1, Epoch 45/130 => Loss 2.785,  Train_accy 60.62
2023-12-09 13:33:33,940 [foster.py] => SNet: Task 1, Epoch 46/130 => Loss 2.790,  Train_accy 60.37, Test_accy 74.25
2023-12-09 13:33:37,788 [foster.py] => SNet: Task 1, Epoch 47/130 => Loss 2.794,  Train_accy 59.57
2023-12-09 13:33:41,161 [foster.py] => SNet: Task 1, Epoch 48/130 => Loss 2.789,  Train_accy 59.80
2023-12-09 13:33:44,914 [foster.py] => SNet: Task 1, Epoch 49/130 => Loss 2.801,  Train_accy 59.08
2023-12-09 13:33:48,617 [foster.py] => SNet: Task 1, Epoch 50/130 => Loss 2.797,  Train_accy 60.35
2023-12-09 13:33:54,247 [foster.py] => SNet: Task 1, Epoch 51/130 => Loss 2.782,  Train_accy 60.03, Test_accy 73.68
2023-12-09 13:33:57,901 [foster.py] => SNet: Task 1, Epoch 52/130 => Loss 2.786,  Train_accy 59.80
2023-12-09 13:34:01,635 [foster.py] => SNet: Task 1, Epoch 53/130 => Loss 2.795,  Train_accy 58.53
2023-12-09 13:34:05,310 [foster.py] => SNet: Task 1, Epoch 54/130 => Loss 2.786,  Train_accy 58.97
2023-12-09 13:34:08,990 [foster.py] => SNet: Task 1, Epoch 55/130 => Loss 2.775,  Train_accy 59.67
2023-12-09 13:34:14,099 [foster.py] => SNet: Task 1, Epoch 56/130 => Loss 2.781,  Train_accy 58.97, Test_accy 74.38
2023-12-09 13:34:17,743 [foster.py] => SNet: Task 1, Epoch 57/130 => Loss 2.804,  Train_accy 60.18
2023-12-09 13:34:21,502 [foster.py] => SNet: Task 1, Epoch 58/130 => Loss 2.787,  Train_accy 60.40
2023-12-09 13:34:25,282 [foster.py] => SNet: Task 1, Epoch 59/130 => Loss 2.777,  Train_accy 60.08
2023-12-09 13:34:28,864 [foster.py] => SNet: Task 1, Epoch 60/130 => Loss 2.773,  Train_accy 60.22
2023-12-09 13:34:34,398 [foster.py] => SNet: Task 1, Epoch 61/130 => Loss 2.791,  Train_accy 59.92, Test_accy 74.70
2023-12-09 13:34:38,128 [foster.py] => SNet: Task 1, Epoch 62/130 => Loss 2.792,  Train_accy 60.23
2023-12-09 13:34:41,959 [foster.py] => SNet: Task 1, Epoch 63/130 => Loss 2.776,  Train_accy 59.60
2023-12-09 13:34:45,530 [foster.py] => SNet: Task 1, Epoch 64/130 => Loss 2.779,  Train_accy 60.50
2023-12-09 13:34:49,363 [foster.py] => SNet: Task 1, Epoch 65/130 => Loss 2.783,  Train_accy 61.05
2023-12-09 13:34:54,892 [foster.py] => SNet: Task 1, Epoch 66/130 => Loss 2.783,  Train_accy 58.90, Test_accy 75.07
2023-12-09 13:34:58,631 [foster.py] => SNet: Task 1, Epoch 67/130 => Loss 2.773,  Train_accy 60.28
2023-12-09 13:35:02,168 [foster.py] => SNet: Task 1, Epoch 68/130 => Loss 2.769,  Train_accy 61.00
2023-12-09 13:35:05,798 [foster.py] => SNet: Task 1, Epoch 69/130 => Loss 2.780,  Train_accy 59.78
2023-12-09 13:35:09,474 [foster.py] => SNet: Task 1, Epoch 70/130 => Loss 2.765,  Train_accy 60.58
2023-12-09 13:35:14,929 [foster.py] => SNet: Task 1, Epoch 71/130 => Loss 2.777,  Train_accy 60.20, Test_accy 75.25
2023-12-09 13:35:18,490 [foster.py] => SNet: Task 1, Epoch 72/130 => Loss 2.778,  Train_accy 59.53
2023-12-09 13:35:22,151 [foster.py] => SNet: Task 1, Epoch 73/130 => Loss 2.762,  Train_accy 60.63
2023-12-09 13:35:25,794 [foster.py] => SNet: Task 1, Epoch 74/130 => Loss 2.770,  Train_accy 60.55
2023-12-09 13:35:29,407 [foster.py] => SNet: Task 1, Epoch 75/130 => Loss 2.784,  Train_accy 59.37
2023-12-09 13:35:34,919 [foster.py] => SNet: Task 1, Epoch 76/130 => Loss 2.765,  Train_accy 60.32, Test_accy 75.35
2023-12-09 13:35:38,684 [foster.py] => SNet: Task 1, Epoch 77/130 => Loss 2.771,  Train_accy 61.22
2023-12-09 13:35:42,344 [foster.py] => SNet: Task 1, Epoch 78/130 => Loss 2.772,  Train_accy 60.75
2023-12-09 13:35:45,990 [foster.py] => SNet: Task 1, Epoch 79/130 => Loss 2.758,  Train_accy 60.55
2023-12-09 13:35:49,636 [foster.py] => SNet: Task 1, Epoch 80/130 => Loss 2.768,  Train_accy 60.53
2023-12-09 13:35:55,384 [foster.py] => SNet: Task 1, Epoch 81/130 => Loss 2.771,  Train_accy 59.93, Test_accy 75.90
2023-12-09 13:35:59,043 [foster.py] => SNet: Task 1, Epoch 82/130 => Loss 2.767,  Train_accy 60.33
2023-12-09 13:36:02,720 [foster.py] => SNet: Task 1, Epoch 83/130 => Loss 2.765,  Train_accy 60.68
2023-12-09 13:36:06,337 [foster.py] => SNet: Task 1, Epoch 84/130 => Loss 2.757,  Train_accy 60.65
2023-12-09 13:36:10,021 [foster.py] => SNet: Task 1, Epoch 85/130 => Loss 2.767,  Train_accy 59.97
2023-12-09 13:36:15,561 [foster.py] => SNet: Task 1, Epoch 86/130 => Loss 2.774,  Train_accy 61.12, Test_accy 75.55
2023-12-09 13:36:19,136 [foster.py] => SNet: Task 1, Epoch 87/130 => Loss 2.770,  Train_accy 60.95
2023-12-09 13:36:22,824 [foster.py] => SNet: Task 1, Epoch 88/130 => Loss 2.761,  Train_accy 60.60
2023-12-09 13:36:26,517 [foster.py] => SNet: Task 1, Epoch 89/130 => Loss 2.770,  Train_accy 60.50
2023-12-09 13:36:30,312 [foster.py] => SNet: Task 1, Epoch 90/130 => Loss 2.779,  Train_accy 60.55
2023-12-09 13:36:35,723 [foster.py] => SNet: Task 1, Epoch 91/130 => Loss 2.772,  Train_accy 60.08, Test_accy 75.68
2023-12-09 13:36:39,368 [foster.py] => SNet: Task 1, Epoch 92/130 => Loss 2.787,  Train_accy 59.47
2023-12-09 13:36:43,096 [foster.py] => SNet: Task 1, Epoch 93/130 => Loss 2.775,  Train_accy 59.97
2023-12-09 13:36:46,881 [foster.py] => SNet: Task 1, Epoch 94/130 => Loss 2.790,  Train_accy 60.72
2023-12-09 13:36:50,631 [foster.py] => SNet: Task 1, Epoch 95/130 => Loss 2.765,  Train_accy 60.53
2023-12-09 13:36:56,077 [foster.py] => SNet: Task 1, Epoch 96/130 => Loss 2.783,  Train_accy 60.35, Test_accy 75.97
2023-12-09 13:36:59,769 [foster.py] => SNet: Task 1, Epoch 97/130 => Loss 2.754,  Train_accy 61.60
2023-12-09 13:37:03,488 [foster.py] => SNet: Task 1, Epoch 98/130 => Loss 2.774,  Train_accy 60.27
2023-12-09 13:37:07,082 [foster.py] => SNet: Task 1, Epoch 99/130 => Loss 2.761,  Train_accy 60.22
2023-12-09 13:37:10,738 [foster.py] => SNet: Task 1, Epoch 100/130 => Loss 2.772,  Train_accy 60.53
2023-12-09 13:37:16,176 [foster.py] => SNet: Task 1, Epoch 101/130 => Loss 2.768,  Train_accy 60.70, Test_accy 76.02
2023-12-09 13:37:19,780 [foster.py] => SNet: Task 1, Epoch 102/130 => Loss 2.754,  Train_accy 60.97
2023-12-09 13:37:23,494 [foster.py] => SNet: Task 1, Epoch 103/130 => Loss 2.760,  Train_accy 60.28
2023-12-09 13:37:27,130 [foster.py] => SNet: Task 1, Epoch 104/130 => Loss 2.769,  Train_accy 61.15
2023-12-09 13:37:30,761 [foster.py] => SNet: Task 1, Epoch 105/130 => Loss 2.769,  Train_accy 61.38
2023-12-09 13:37:36,257 [foster.py] => SNet: Task 1, Epoch 106/130 => Loss 2.765,  Train_accy 61.05, Test_accy 75.68
2023-12-09 13:37:40,007 [foster.py] => SNet: Task 1, Epoch 107/130 => Loss 2.770,  Train_accy 60.72
2023-12-09 13:37:43,689 [foster.py] => SNet: Task 1, Epoch 108/130 => Loss 2.763,  Train_accy 60.72
2023-12-09 13:37:47,278 [foster.py] => SNet: Task 1, Epoch 109/130 => Loss 2.764,  Train_accy 60.58
2023-12-09 13:37:50,893 [foster.py] => SNet: Task 1, Epoch 110/130 => Loss 2.763,  Train_accy 60.50
2023-12-09 13:37:56,335 [foster.py] => SNet: Task 1, Epoch 111/130 => Loss 2.756,  Train_accy 60.78, Test_accy 76.05
2023-12-09 13:38:00,026 [foster.py] => SNet: Task 1, Epoch 112/130 => Loss 2.761,  Train_accy 61.28
2023-12-09 13:38:03,731 [foster.py] => SNet: Task 1, Epoch 113/130 => Loss 2.756,  Train_accy 61.55
2023-12-09 13:38:07,418 [foster.py] => SNet: Task 1, Epoch 114/130 => Loss 2.762,  Train_accy 61.47
2023-12-09 13:38:11,149 [foster.py] => SNet: Task 1, Epoch 115/130 => Loss 2.795,  Train_accy 59.73
2023-12-09 13:38:16,794 [foster.py] => SNet: Task 1, Epoch 116/130 => Loss 2.765,  Train_accy 60.52, Test_accy 75.82
2023-12-09 13:38:20,453 [foster.py] => SNet: Task 1, Epoch 117/130 => Loss 2.772,  Train_accy 60.52
2023-12-09 13:38:24,122 [foster.py] => SNet: Task 1, Epoch 118/130 => Loss 2.765,  Train_accy 61.58
2023-12-09 13:38:27,645 [foster.py] => SNet: Task 1, Epoch 119/130 => Loss 2.759,  Train_accy 60.58
2023-12-09 13:38:31,335 [foster.py] => SNet: Task 1, Epoch 120/130 => Loss 2.771,  Train_accy 60.72
2023-12-09 13:38:36,925 [foster.py] => SNet: Task 1, Epoch 121/130 => Loss 2.768,  Train_accy 61.60, Test_accy 76.05
2023-12-09 13:38:40,544 [foster.py] => SNet: Task 1, Epoch 122/130 => Loss 2.769,  Train_accy 60.28
2023-12-09 13:38:44,197 [foster.py] => SNet: Task 1, Epoch 123/130 => Loss 2.749,  Train_accy 61.00
2023-12-09 13:38:47,934 [foster.py] => SNet: Task 1, Epoch 124/130 => Loss 2.768,  Train_accy 61.42
2023-12-09 13:38:51,605 [foster.py] => SNet: Task 1, Epoch 125/130 => Loss 2.759,  Train_accy 61.32
2023-12-09 13:38:57,032 [foster.py] => SNet: Task 1, Epoch 126/130 => Loss 2.757,  Train_accy 60.68, Test_accy 76.03
2023-12-09 13:39:00,640 [foster.py] => SNet: Task 1, Epoch 127/130 => Loss 2.756,  Train_accy 60.43
2023-12-09 13:39:04,299 [foster.py] => SNet: Task 1, Epoch 128/130 => Loss 2.766,  Train_accy 61.65
2023-12-09 13:39:07,944 [foster.py] => SNet: Task 1, Epoch 129/130 => Loss 2.756,  Train_accy 60.90
2023-12-09 13:39:11,654 [foster.py] => SNet: Task 1, Epoch 130/130 => Loss 2.766,  Train_accy 60.35
2023-12-09 13:39:11,656 [foster.py] => do not weight align student!
2023-12-09 13:39:13,461 [foster.py] => darknet eval: 
2023-12-09 13:39:13,461 [foster.py] => CNN top1 curve: 75.8
2023-12-09 13:39:13,461 [foster.py] => CNN top5 curve: 94.73
2023-12-09 13:39:13,462 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 13:40:09,957 [foster.py] => Exemplar size: 1200
2023-12-09 13:40:09,957 [trainer.py] => CNN: {'total': 76.75, '00-09': 80.8, '10-19': 70.8, '20-29': 81.6, '30-39': 73.0, '40-49': 79.1, '50-59': 75.2, 'old': 77.06, 'new': 75.2}
2023-12-09 13:40:09,957 [trainer.py] => NME: {'total': 72.0, '00-09': 75.7, '10-19': 66.3, '20-29': 77.4, '30-39': 68.0, '40-49': 73.9, '50-59': 70.7, 'old': 72.26, 'new': 70.7}
2023-12-09 13:40:09,957 [trainer.py] => CNN top1 curve: [80.38, 76.75]
2023-12-09 13:40:09,957 [trainer.py] => CNN top5 curve: [96.48, 94.95]
2023-12-09 13:40:09,957 [trainer.py] => NME top1 curve: [78.66, 72.0]
2023-12-09 13:40:09,957 [trainer.py] => NME top5 curve: [96.2, 93.15]

2023-12-09 13:40:09,958 [trainer.py] => All params: 943198
2023-12-09 13:40:09,959 [trainer.py] => Trainable params: 475794
2023-12-09 13:40:09,982 [foster.py] => Learning on 60-70
2023-12-09 13:40:09,983 [foster.py] => All params: 945788
2023-12-09 13:40:09,984 [foster.py] => Trainable params: 477734
2023-12-09 13:40:10,076 [foster.py] => per cls weights : [1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559 1.04323559
 0.74058644 0.74058644 0.74058644 0.74058644 0.74058644 0.74058644
 0.74058644 0.74058644 0.74058644 0.74058644]
2023-12-09 13:40:13,507 [foster.py] => Task 2, Epoch 1/170 => Loss 7.885, Loss_clf 1.888, Loss_fe 3.015, Loss_kd 2.556, Train_accy 48.47
2023-12-09 13:40:19,243 [foster.py] => Task 2, Epoch 2/170 => Loss 6.912, Loss_clf 1.272, Loss_fe 2.688, Loss_kd 2.531, Train_accy 55.39, Test_accy 67.31
2023-12-09 13:40:25,149 [foster.py] => Task 2, Epoch 3/170 => Loss 6.459, Loss_clf 1.154, Loss_fe 2.356, Loss_kd 2.528, Train_accy 58.39, Test_accy 69.24
2023-12-09 13:40:30,870 [foster.py] => Task 2, Epoch 4/170 => Loss 6.335, Loss_clf 1.141, Loss_fe 2.232, Loss_kd 2.538, Train_accy 58.71, Test_accy 70.33
2023-12-09 13:40:36,492 [foster.py] => Task 2, Epoch 5/170 => Loss 6.143, Loss_clf 1.098, Loss_fe 2.094, Loss_kd 2.530, Train_accy 60.06, Test_accy 69.83
2023-12-09 13:40:39,898 [foster.py] => Task 2, Epoch 6/170 => Loss 6.048, Loss_clf 1.066, Loss_fe 2.032, Loss_kd 2.529, Train_accy 60.68
2023-12-09 13:40:45,570 [foster.py] => Task 2, Epoch 7/170 => Loss 5.977, Loss_clf 1.067, Loss_fe 1.950, Loss_kd 2.538, Train_accy 61.81, Test_accy 69.90
2023-12-09 13:40:51,175 [foster.py] => Task 2, Epoch 8/170 => Loss 5.937, Loss_clf 1.056, Loss_fe 1.925, Loss_kd 2.534, Train_accy 62.05, Test_accy 71.36
2023-12-09 13:40:56,984 [foster.py] => Task 2, Epoch 9/170 => Loss 5.771, Loss_clf 0.973, Loss_fe 1.848, Loss_kd 2.529, Train_accy 63.52, Test_accy 71.34
2023-12-09 13:41:02,624 [foster.py] => Task 2, Epoch 10/170 => Loss 5.794, Loss_clf 1.027, Loss_fe 1.819, Loss_kd 2.527, Train_accy 62.65, Test_accy 70.41
2023-12-09 13:41:05,980 [foster.py] => Task 2, Epoch 11/170 => Loss 5.795, Loss_clf 1.027, Loss_fe 1.817, Loss_kd 2.530, Train_accy 62.50
2023-12-09 13:41:11,651 [foster.py] => Task 2, Epoch 12/170 => Loss 5.742, Loss_clf 1.039, Loss_fe 1.736, Loss_kd 2.544, Train_accy 63.26, Test_accy 65.33
2023-12-09 13:41:17,369 [foster.py] => Task 2, Epoch 13/170 => Loss 5.665, Loss_clf 0.983, Loss_fe 1.718, Loss_kd 2.540, Train_accy 64.87, Test_accy 71.04
2023-12-09 13:41:23,136 [foster.py] => Task 2, Epoch 14/170 => Loss 5.665, Loss_clf 1.008, Loss_fe 1.691, Loss_kd 2.542, Train_accy 63.69, Test_accy 70.77
2023-12-09 13:41:28,828 [foster.py] => Task 2, Epoch 15/170 => Loss 5.557, Loss_clf 0.963, Loss_fe 1.638, Loss_kd 2.534, Train_accy 64.63, Test_accy 70.49
2023-12-09 13:41:32,223 [foster.py] => Task 2, Epoch 16/170 => Loss 5.601, Loss_clf 1.002, Loss_fe 1.642, Loss_kd 2.535, Train_accy 63.90
2023-12-09 13:41:38,096 [foster.py] => Task 2, Epoch 17/170 => Loss 5.632, Loss_clf 1.024, Loss_fe 1.649, Loss_kd 2.536, Train_accy 64.40, Test_accy 71.09
2023-12-09 13:41:43,754 [foster.py] => Task 2, Epoch 18/170 => Loss 5.553, Loss_clf 1.002, Loss_fe 1.602, Loss_kd 2.527, Train_accy 65.55, Test_accy 69.93
2023-12-09 13:41:49,438 [foster.py] => Task 2, Epoch 19/170 => Loss 5.538, Loss_clf 0.998, Loss_fe 1.590, Loss_kd 2.529, Train_accy 65.65, Test_accy 69.59
2023-12-09 13:41:55,152 [foster.py] => Task 2, Epoch 20/170 => Loss 5.509, Loss_clf 0.959, Loss_fe 1.587, Loss_kd 2.539, Train_accy 66.02, Test_accy 71.00
2023-12-09 13:41:58,527 [foster.py] => Task 2, Epoch 21/170 => Loss 5.385, Loss_clf 0.917, Loss_fe 1.527, Loss_kd 2.521, Train_accy 66.95
2023-12-09 13:42:04,300 [foster.py] => Task 2, Epoch 22/170 => Loss 5.359, Loss_clf 0.900, Loss_fe 1.503, Loss_kd 2.534, Train_accy 66.66, Test_accy 69.09
2023-12-09 13:42:10,015 [foster.py] => Task 2, Epoch 23/170 => Loss 5.415, Loss_clf 0.943, Loss_fe 1.521, Loss_kd 2.529, Train_accy 66.15, Test_accy 69.03
2023-12-09 13:42:15,858 [foster.py] => Task 2, Epoch 24/170 => Loss 5.360, Loss_clf 0.921, Loss_fe 1.489, Loss_kd 2.529, Train_accy 67.45, Test_accy 69.51
2023-12-09 13:42:21,518 [foster.py] => Task 2, Epoch 25/170 => Loss 5.324, Loss_clf 0.896, Loss_fe 1.479, Loss_kd 2.527, Train_accy 67.19, Test_accy 71.29
2023-12-09 13:42:24,639 [foster.py] => Task 2, Epoch 26/170 => Loss 5.313, Loss_clf 0.894, Loss_fe 1.473, Loss_kd 2.526, Train_accy 67.68
2023-12-09 13:42:30,453 [foster.py] => Task 2, Epoch 27/170 => Loss 5.335, Loss_clf 0.912, Loss_fe 1.466, Loss_kd 2.535, Train_accy 66.77, Test_accy 71.46
2023-12-09 13:42:36,320 [foster.py] => Task 2, Epoch 28/170 => Loss 5.313, Loss_clf 0.928, Loss_fe 1.443, Loss_kd 2.522, Train_accy 67.11, Test_accy 71.34
2023-12-09 13:42:41,954 [foster.py] => Task 2, Epoch 29/170 => Loss 5.283, Loss_clf 0.897, Loss_fe 1.428, Loss_kd 2.536, Train_accy 67.39, Test_accy 71.17
2023-12-09 13:42:47,804 [foster.py] => Task 2, Epoch 30/170 => Loss 5.194, Loss_clf 0.831, Loss_fe 1.399, Loss_kd 2.540, Train_accy 68.87, Test_accy 71.44
2023-12-09 13:42:51,180 [foster.py] => Task 2, Epoch 31/170 => Loss 5.296, Loss_clf 0.920, Loss_fe 1.419, Loss_kd 2.534, Train_accy 67.68
2023-12-09 13:42:56,815 [foster.py] => Task 2, Epoch 32/170 => Loss 5.250, Loss_clf 0.894, Loss_fe 1.402, Loss_kd 2.532, Train_accy 69.24, Test_accy 69.97
2023-12-09 13:43:02,463 [foster.py] => Task 2, Epoch 33/170 => Loss 5.234, Loss_clf 0.870, Loss_fe 1.404, Loss_kd 2.537, Train_accy 68.08, Test_accy 72.26
2023-12-09 13:43:08,286 [foster.py] => Task 2, Epoch 34/170 => Loss 5.308, Loss_clf 0.933, Loss_fe 1.422, Loss_kd 2.531, Train_accy 67.00, Test_accy 70.91
2023-12-09 13:43:13,973 [foster.py] => Task 2, Epoch 35/170 => Loss 5.262, Loss_clf 0.895, Loss_fe 1.403, Loss_kd 2.541, Train_accy 67.65, Test_accy 70.80
2023-12-09 13:43:17,396 [foster.py] => Task 2, Epoch 36/170 => Loss 5.249, Loss_clf 0.920, Loss_fe 1.382, Loss_kd 2.526, Train_accy 68.55
2023-12-09 13:43:22,981 [foster.py] => Task 2, Epoch 37/170 => Loss 5.251, Loss_clf 0.896, Loss_fe 1.393, Loss_kd 2.539, Train_accy 68.02, Test_accy 72.34
2023-12-09 13:43:28,684 [foster.py] => Task 2, Epoch 38/170 => Loss 5.186, Loss_clf 0.858, Loss_fe 1.368, Loss_kd 2.538, Train_accy 68.84, Test_accy 71.69
2023-12-09 13:43:34,288 [foster.py] => Task 2, Epoch 39/170 => Loss 5.239, Loss_clf 0.909, Loss_fe 1.370, Loss_kd 2.537, Train_accy 67.68, Test_accy 71.31
2023-12-09 13:43:40,008 [foster.py] => Task 2, Epoch 40/170 => Loss 5.151, Loss_clf 0.859, Loss_fe 1.336, Loss_kd 2.533, Train_accy 68.76, Test_accy 71.61
2023-12-09 13:43:43,379 [foster.py] => Task 2, Epoch 41/170 => Loss 5.056, Loss_clf 0.821, Loss_fe 1.288, Loss_kd 2.526, Train_accy 69.42
2023-12-09 13:43:49,076 [foster.py] => Task 2, Epoch 42/170 => Loss 5.096, Loss_clf 0.852, Loss_fe 1.292, Loss_kd 2.530, Train_accy 69.85, Test_accy 71.40
2023-12-09 13:43:54,795 [foster.py] => Task 2, Epoch 43/170 => Loss 5.174, Loss_clf 0.865, Loss_fe 1.353, Loss_kd 2.534, Train_accy 70.10, Test_accy 71.76
2023-12-09 13:44:00,478 [foster.py] => Task 2, Epoch 44/170 => Loss 5.087, Loss_clf 0.836, Loss_fe 1.299, Loss_kd 2.531, Train_accy 69.66, Test_accy 71.54
2023-12-09 13:44:06,044 [foster.py] => Task 2, Epoch 45/170 => Loss 5.071, Loss_clf 0.839, Loss_fe 1.277, Loss_kd 2.533, Train_accy 70.47, Test_accy 71.69
2023-12-09 13:44:09,454 [foster.py] => Task 2, Epoch 46/170 => Loss 5.109, Loss_clf 0.830, Loss_fe 1.327, Loss_kd 2.530, Train_accy 70.37
2023-12-09 13:44:15,118 [foster.py] => Task 2, Epoch 47/170 => Loss 5.085, Loss_clf 0.839, Loss_fe 1.299, Loss_kd 2.526, Train_accy 70.27, Test_accy 71.17
2023-12-09 13:44:20,862 [foster.py] => Task 2, Epoch 48/170 => Loss 5.036, Loss_clf 0.822, Loss_fe 1.264, Loss_kd 2.528, Train_accy 70.56, Test_accy 72.81
2023-12-09 13:44:26,685 [foster.py] => Task 2, Epoch 49/170 => Loss 5.065, Loss_clf 0.825, Loss_fe 1.284, Loss_kd 2.534, Train_accy 69.95, Test_accy 72.14
2023-12-09 13:44:32,268 [foster.py] => Task 2, Epoch 50/170 => Loss 5.021, Loss_clf 0.803, Loss_fe 1.260, Loss_kd 2.536, Train_accy 71.10, Test_accy 69.91
2023-12-09 13:44:35,747 [foster.py] => Task 2, Epoch 51/170 => Loss 4.990, Loss_clf 0.800, Loss_fe 1.232, Loss_kd 2.535, Train_accy 70.85
2023-12-09 13:44:41,454 [foster.py] => Task 2, Epoch 52/170 => Loss 5.057, Loss_clf 0.831, Loss_fe 1.265, Loss_kd 2.537, Train_accy 70.45, Test_accy 70.79
2023-12-09 13:44:47,120 [foster.py] => Task 2, Epoch 53/170 => Loss 5.029, Loss_clf 0.838, Loss_fe 1.233, Loss_kd 2.535, Train_accy 70.11, Test_accy 70.67
2023-12-09 13:44:52,841 [foster.py] => Task 2, Epoch 54/170 => Loss 4.925, Loss_clf 0.751, Loss_fe 1.217, Loss_kd 2.535, Train_accy 72.61, Test_accy 70.47
2023-12-09 13:44:58,513 [foster.py] => Task 2, Epoch 55/170 => Loss 4.968, Loss_clf 0.794, Loss_fe 1.221, Loss_kd 2.531, Train_accy 71.37, Test_accy 72.44
2023-12-09 13:45:01,922 [foster.py] => Task 2, Epoch 56/170 => Loss 4.936, Loss_clf 0.773, Loss_fe 1.206, Loss_kd 2.535, Train_accy 71.98
2023-12-09 13:45:07,545 [foster.py] => Task 2, Epoch 57/170 => Loss 5.010, Loss_clf 0.793, Loss_fe 1.249, Loss_kd 2.544, Train_accy 70.06, Test_accy 70.09
2023-12-09 13:45:13,343 [foster.py] => Task 2, Epoch 58/170 => Loss 4.931, Loss_clf 0.770, Loss_fe 1.208, Loss_kd 2.531, Train_accy 71.71, Test_accy 71.59
2023-12-09 13:45:18,995 [foster.py] => Task 2, Epoch 59/170 => Loss 4.939, Loss_clf 0.778, Loss_fe 1.197, Loss_kd 2.540, Train_accy 71.97, Test_accy 71.94
2023-12-09 13:45:24,689 [foster.py] => Task 2, Epoch 60/170 => Loss 4.919, Loss_clf 0.768, Loss_fe 1.195, Loss_kd 2.533, Train_accy 71.73, Test_accy 71.84
2023-12-09 13:45:28,091 [foster.py] => Task 2, Epoch 61/170 => Loss 4.969, Loss_clf 0.801, Loss_fe 1.201, Loss_kd 2.543, Train_accy 71.31
2023-12-09 13:45:33,750 [foster.py] => Task 2, Epoch 62/170 => Loss 4.912, Loss_clf 0.754, Loss_fe 1.197, Loss_kd 2.538, Train_accy 72.98, Test_accy 71.94
2023-12-09 13:45:39,463 [foster.py] => Task 2, Epoch 63/170 => Loss 4.881, Loss_clf 0.754, Loss_fe 1.174, Loss_kd 2.531, Train_accy 72.21, Test_accy 72.54
2023-12-09 13:45:45,049 [foster.py] => Task 2, Epoch 64/170 => Loss 4.898, Loss_clf 0.761, Loss_fe 1.176, Loss_kd 2.539, Train_accy 72.42, Test_accy 72.73
2023-12-09 13:45:50,827 [foster.py] => Task 2, Epoch 65/170 => Loss 4.802, Loss_clf 0.712, Loss_fe 1.138, Loss_kd 2.530, Train_accy 72.65, Test_accy 72.50
2023-12-09 13:45:54,219 [foster.py] => Task 2, Epoch 66/170 => Loss 4.855, Loss_clf 0.750, Loss_fe 1.148, Loss_kd 2.534, Train_accy 72.89
2023-12-09 13:45:59,802 [foster.py] => Task 2, Epoch 67/170 => Loss 4.873, Loss_clf 0.758, Loss_fe 1.160, Loss_kd 2.532, Train_accy 72.19, Test_accy 71.71
2023-12-09 13:46:05,425 [foster.py] => Task 2, Epoch 68/170 => Loss 4.795, Loss_clf 0.731, Loss_fe 1.120, Loss_kd 2.523, Train_accy 72.92, Test_accy 69.46
2023-12-09 13:46:11,405 [foster.py] => Task 2, Epoch 69/170 => Loss 4.846, Loss_clf 0.748, Loss_fe 1.130, Loss_kd 2.544, Train_accy 72.92, Test_accy 71.99
2023-12-09 13:46:17,248 [foster.py] => Task 2, Epoch 70/170 => Loss 4.819, Loss_clf 0.745, Loss_fe 1.120, Loss_kd 2.532, Train_accy 72.45, Test_accy 72.63
2023-12-09 13:46:20,676 [foster.py] => Task 2, Epoch 71/170 => Loss 4.892, Loss_clf 0.772, Loss_fe 1.160, Loss_kd 2.537, Train_accy 72.15
2023-12-09 13:46:26,749 [foster.py] => Task 2, Epoch 72/170 => Loss 4.747, Loss_clf 0.694, Loss_fe 1.102, Loss_kd 2.529, Train_accy 74.27, Test_accy 72.09
2023-12-09 13:46:32,266 [foster.py] => Task 2, Epoch 73/170 => Loss 4.810, Loss_clf 0.732, Loss_fe 1.126, Loss_kd 2.530, Train_accy 73.23, Test_accy 70.24
2023-12-09 13:46:38,231 [foster.py] => Task 2, Epoch 74/170 => Loss 4.743, Loss_clf 0.718, Loss_fe 1.083, Loss_kd 2.521, Train_accy 73.34, Test_accy 71.31
2023-12-09 13:46:44,432 [foster.py] => Task 2, Epoch 75/170 => Loss 4.755, Loss_clf 0.704, Loss_fe 1.094, Loss_kd 2.534, Train_accy 73.45, Test_accy 72.31
2023-12-09 13:46:48,046 [foster.py] => Task 2, Epoch 76/170 => Loss 4.748, Loss_clf 0.721, Loss_fe 1.074, Loss_kd 2.531, Train_accy 73.24
2023-12-09 13:46:54,245 [foster.py] => Task 2, Epoch 77/170 => Loss 4.637, Loss_clf 0.662, Loss_fe 1.033, Loss_kd 2.522, Train_accy 75.85, Test_accy 72.10
2023-12-09 13:47:00,650 [foster.py] => Task 2, Epoch 78/170 => Loss 4.675, Loss_clf 0.676, Loss_fe 1.053, Loss_kd 2.525, Train_accy 74.79, Test_accy 71.93
2023-12-09 13:47:06,923 [foster.py] => Task 2, Epoch 79/170 => Loss 4.738, Loss_clf 0.719, Loss_fe 1.061, Loss_kd 2.536, Train_accy 74.16, Test_accy 71.94
2023-12-09 13:47:12,906 [foster.py] => Task 2, Epoch 80/170 => Loss 4.741, Loss_clf 0.731, Loss_fe 1.051, Loss_kd 2.537, Train_accy 74.13, Test_accy 70.77
2023-12-09 13:47:16,502 [foster.py] => Task 2, Epoch 81/170 => Loss 4.757, Loss_clf 0.714, Loss_fe 1.092, Loss_kd 2.530, Train_accy 74.16
2023-12-09 13:47:22,362 [foster.py] => Task 2, Epoch 82/170 => Loss 4.690, Loss_clf 0.685, Loss_fe 1.051, Loss_kd 2.532, Train_accy 74.40, Test_accy 73.11
2023-12-09 13:47:28,504 [foster.py] => Task 2, Epoch 83/170 => Loss 4.659, Loss_clf 0.682, Loss_fe 1.024, Loss_kd 2.532, Train_accy 75.10, Test_accy 72.26
2023-12-09 13:47:34,773 [foster.py] => Task 2, Epoch 84/170 => Loss 4.640, Loss_clf 0.674, Loss_fe 1.024, Loss_kd 2.522, Train_accy 74.74, Test_accy 72.44
2023-12-09 13:47:40,956 [foster.py] => Task 2, Epoch 85/170 => Loss 4.631, Loss_clf 0.679, Loss_fe 1.002, Loss_kd 2.529, Train_accy 74.68, Test_accy 72.51
2023-12-09 13:47:44,585 [foster.py] => Task 2, Epoch 86/170 => Loss 4.666, Loss_clf 0.688, Loss_fe 1.031, Loss_kd 2.526, Train_accy 73.98
2023-12-09 13:47:50,897 [foster.py] => Task 2, Epoch 87/170 => Loss 4.572, Loss_clf 0.636, Loss_fe 0.984, Loss_kd 2.530, Train_accy 76.48, Test_accy 72.63
2023-12-09 13:47:57,346 [foster.py] => Task 2, Epoch 88/170 => Loss 4.611, Loss_clf 0.659, Loss_fe 0.994, Loss_kd 2.535, Train_accy 75.19, Test_accy 72.63
2023-12-09 13:48:03,746 [foster.py] => Task 2, Epoch 89/170 => Loss 4.597, Loss_clf 0.658, Loss_fe 0.989, Loss_kd 2.528, Train_accy 75.15, Test_accy 71.60
2023-12-09 13:48:10,216 [foster.py] => Task 2, Epoch 90/170 => Loss 4.664, Loss_clf 0.674, Loss_fe 1.018, Loss_kd 2.547, Train_accy 75.08, Test_accy 72.29
2023-12-09 13:48:14,060 [foster.py] => Task 2, Epoch 91/170 => Loss 4.583, Loss_clf 0.659, Loss_fe 0.970, Loss_kd 2.532, Train_accy 75.35
2023-12-09 13:48:20,328 [foster.py] => Task 2, Epoch 92/170 => Loss 4.540, Loss_clf 0.640, Loss_fe 0.944, Loss_kd 2.534, Train_accy 76.34, Test_accy 73.39
2023-12-09 13:48:26,786 [foster.py] => Task 2, Epoch 93/170 => Loss 4.588, Loss_clf 0.651, Loss_fe 0.982, Loss_kd 2.533, Train_accy 76.18, Test_accy 73.70
2023-12-09 13:48:32,991 [foster.py] => Task 2, Epoch 94/170 => Loss 4.545, Loss_clf 0.629, Loss_fe 0.963, Loss_kd 2.532, Train_accy 75.79, Test_accy 73.14
2023-12-09 13:48:39,102 [foster.py] => Task 2, Epoch 95/170 => Loss 4.505, Loss_clf 0.613, Loss_fe 0.936, Loss_kd 2.533, Train_accy 76.89, Test_accy 73.13
2023-12-09 13:48:42,702 [foster.py] => Task 2, Epoch 96/170 => Loss 4.541, Loss_clf 0.640, Loss_fe 0.941, Loss_kd 2.538, Train_accy 76.55
2023-12-09 13:48:48,920 [foster.py] => Task 2, Epoch 97/170 => Loss 4.510, Loss_clf 0.616, Loss_fe 0.936, Loss_kd 2.535, Train_accy 76.26, Test_accy 73.20
2023-12-09 13:48:55,354 [foster.py] => Task 2, Epoch 98/170 => Loss 4.561, Loss_clf 0.657, Loss_fe 0.947, Loss_kd 2.535, Train_accy 76.11, Test_accy 72.49
2023-12-09 13:49:01,766 [foster.py] => Task 2, Epoch 99/170 => Loss 4.478, Loss_clf 0.613, Loss_fe 0.920, Loss_kd 2.525, Train_accy 76.55, Test_accy 72.91
2023-12-09 13:49:07,739 [foster.py] => Task 2, Epoch 100/170 => Loss 4.509, Loss_clf 0.623, Loss_fe 0.922, Loss_kd 2.541, Train_accy 76.79, Test_accy 72.00
2023-12-09 13:49:11,261 [foster.py] => Task 2, Epoch 101/170 => Loss 4.490, Loss_clf 0.615, Loss_fe 0.912, Loss_kd 2.539, Train_accy 76.48
2023-12-09 13:49:17,538 [foster.py] => Task 2, Epoch 102/170 => Loss 4.473, Loss_clf 0.608, Loss_fe 0.909, Loss_kd 2.534, Train_accy 77.26, Test_accy 73.56
2023-12-09 13:49:23,794 [foster.py] => Task 2, Epoch 103/170 => Loss 4.403, Loss_clf 0.581, Loss_fe 0.874, Loss_kd 2.527, Train_accy 77.82, Test_accy 72.50
2023-12-09 13:49:30,280 [foster.py] => Task 2, Epoch 104/170 => Loss 4.444, Loss_clf 0.591, Loss_fe 0.897, Loss_kd 2.533, Train_accy 78.16, Test_accy 72.90
2023-12-09 13:49:36,736 [foster.py] => Task 2, Epoch 105/170 => Loss 4.351, Loss_clf 0.562, Loss_fe 0.841, Loss_kd 2.527, Train_accy 78.58, Test_accy 72.16
2023-12-09 13:49:40,470 [foster.py] => Task 2, Epoch 106/170 => Loss 4.362, Loss_clf 0.573, Loss_fe 0.846, Loss_kd 2.523, Train_accy 78.66
2023-12-09 13:49:46,674 [foster.py] => Task 2, Epoch 107/170 => Loss 4.377, Loss_clf 0.572, Loss_fe 0.853, Loss_kd 2.531, Train_accy 78.11, Test_accy 73.10
2023-12-09 13:49:53,031 [foster.py] => Task 2, Epoch 108/170 => Loss 4.386, Loss_clf 0.581, Loss_fe 0.849, Loss_kd 2.533, Train_accy 78.26, Test_accy 72.73
2023-12-09 13:49:59,497 [foster.py] => Task 2, Epoch 109/170 => Loss 4.324, Loss_clf 0.561, Loss_fe 0.815, Loss_kd 2.526, Train_accy 78.19, Test_accy 72.74
2023-12-09 13:50:06,043 [foster.py] => Task 2, Epoch 110/170 => Loss 4.336, Loss_clf 0.565, Loss_fe 0.825, Loss_kd 2.525, Train_accy 79.15, Test_accy 73.87
2023-12-09 13:50:09,928 [foster.py] => Task 2, Epoch 111/170 => Loss 4.297, Loss_clf 0.538, Loss_fe 0.811, Loss_kd 2.526, Train_accy 79.26
2023-12-09 13:50:16,204 [foster.py] => Task 2, Epoch 112/170 => Loss 4.317, Loss_clf 0.542, Loss_fe 0.821, Loss_kd 2.531, Train_accy 79.66, Test_accy 73.60
2023-12-09 13:50:22,428 [foster.py] => Task 2, Epoch 113/170 => Loss 4.282, Loss_clf 0.539, Loss_fe 0.791, Loss_kd 2.531, Train_accy 79.55, Test_accy 73.46
2023-12-09 13:50:28,738 [foster.py] => Task 2, Epoch 114/170 => Loss 4.321, Loss_clf 0.558, Loss_fe 0.801, Loss_kd 2.539, Train_accy 78.65, Test_accy 73.59
2023-12-09 13:50:34,922 [foster.py] => Task 2, Epoch 115/170 => Loss 4.241, Loss_clf 0.524, Loss_fe 0.767, Loss_kd 2.529, Train_accy 80.21, Test_accy 73.57
2023-12-09 13:50:38,505 [foster.py] => Task 2, Epoch 116/170 => Loss 4.256, Loss_clf 0.533, Loss_fe 0.773, Loss_kd 2.528, Train_accy 79.79
2023-12-09 13:50:44,533 [foster.py] => Task 2, Epoch 117/170 => Loss 4.266, Loss_clf 0.526, Loss_fe 0.782, Loss_kd 2.535, Train_accy 79.85, Test_accy 73.76
2023-12-09 13:50:50,649 [foster.py] => Task 2, Epoch 118/170 => Loss 4.219, Loss_clf 0.514, Loss_fe 0.751, Loss_kd 2.532, Train_accy 80.97, Test_accy 72.81
2023-12-09 13:50:56,469 [foster.py] => Task 2, Epoch 119/170 => Loss 4.288, Loss_clf 0.544, Loss_fe 0.776, Loss_kd 2.543, Train_accy 79.65, Test_accy 73.49
2023-12-09 13:51:02,236 [foster.py] => Task 2, Epoch 120/170 => Loss 4.237, Loss_clf 0.519, Loss_fe 0.762, Loss_kd 2.534, Train_accy 80.35, Test_accy 73.99
2023-12-09 13:51:05,692 [foster.py] => Task 2, Epoch 121/170 => Loss 4.219, Loss_clf 0.509, Loss_fe 0.751, Loss_kd 2.536, Train_accy 80.81
2023-12-09 13:51:11,502 [foster.py] => Task 2, Epoch 122/170 => Loss 4.174, Loss_clf 0.492, Loss_fe 0.733, Loss_kd 2.527, Train_accy 80.97, Test_accy 73.27
2023-12-09 13:51:17,218 [foster.py] => Task 2, Epoch 123/170 => Loss 4.194, Loss_clf 0.509, Loss_fe 0.736, Loss_kd 2.528, Train_accy 81.10, Test_accy 73.56
2023-12-09 13:51:22,831 [foster.py] => Task 2, Epoch 124/170 => Loss 4.119, Loss_clf 0.479, Loss_fe 0.696, Loss_kd 2.524, Train_accy 81.52, Test_accy 73.81
2023-12-09 13:51:28,522 [foster.py] => Task 2, Epoch 125/170 => Loss 4.153, Loss_clf 0.483, Loss_fe 0.706, Loss_kd 2.540, Train_accy 81.50, Test_accy 73.34
2023-12-09 13:51:32,012 [foster.py] => Task 2, Epoch 126/170 => Loss 4.136, Loss_clf 0.484, Loss_fe 0.698, Loss_kd 2.532, Train_accy 81.82
2023-12-09 13:51:37,774 [foster.py] => Task 2, Epoch 127/170 => Loss 4.114, Loss_clf 0.482, Loss_fe 0.693, Loss_kd 2.519, Train_accy 81.71, Test_accy 73.10
2023-12-09 13:51:43,626 [foster.py] => Task 2, Epoch 128/170 => Loss 4.106, Loss_clf 0.470, Loss_fe 0.676, Loss_kd 2.537, Train_accy 82.37, Test_accy 73.39
2023-12-09 13:51:49,669 [foster.py] => Task 2, Epoch 129/170 => Loss 4.062, Loss_clf 0.462, Loss_fe 0.655, Loss_kd 2.525, Train_accy 82.63, Test_accy 74.07
2023-12-09 13:51:55,436 [foster.py] => Task 2, Epoch 130/170 => Loss 4.099, Loss_clf 0.473, Loss_fe 0.675, Loss_kd 2.529, Train_accy 82.48, Test_accy 74.03
2023-12-09 13:51:58,839 [foster.py] => Task 2, Epoch 131/170 => Loss 4.077, Loss_clf 0.462, Loss_fe 0.666, Loss_kd 2.527, Train_accy 82.53
2023-12-09 13:52:04,494 [foster.py] => Task 2, Epoch 132/170 => Loss 4.077, Loss_clf 0.453, Loss_fe 0.660, Loss_kd 2.541, Train_accy 82.98, Test_accy 73.67
2023-12-09 13:52:10,163 [foster.py] => Task 2, Epoch 133/170 => Loss 4.019, Loss_clf 0.434, Loss_fe 0.630, Loss_kd 2.533, Train_accy 83.11, Test_accy 74.07
2023-12-09 13:52:15,886 [foster.py] => Task 2, Epoch 134/170 => Loss 4.066, Loss_clf 0.465, Loss_fe 0.656, Loss_kd 2.524, Train_accy 83.21, Test_accy 73.76
2023-12-09 13:52:21,519 [foster.py] => Task 2, Epoch 135/170 => Loss 4.011, Loss_clf 0.440, Loss_fe 0.619, Loss_kd 2.530, Train_accy 82.92, Test_accy 74.29
2023-12-09 13:52:25,070 [foster.py] => Task 2, Epoch 136/170 => Loss 4.043, Loss_clf 0.454, Loss_fe 0.631, Loss_kd 2.535, Train_accy 83.00
2023-12-09 13:52:31,018 [foster.py] => Task 2, Epoch 137/170 => Loss 3.983, Loss_clf 0.425, Loss_fe 0.604, Loss_kd 2.532, Train_accy 84.18, Test_accy 74.06
2023-12-09 13:52:36,921 [foster.py] => Task 2, Epoch 138/170 => Loss 4.014, Loss_clf 0.445, Loss_fe 0.622, Loss_kd 2.526, Train_accy 83.73, Test_accy 74.47
2023-12-09 13:52:43,069 [foster.py] => Task 2, Epoch 139/170 => Loss 3.994, Loss_clf 0.440, Loss_fe 0.607, Loss_kd 2.526, Train_accy 83.66, Test_accy 74.31
2023-12-09 13:52:49,287 [foster.py] => Task 2, Epoch 140/170 => Loss 3.998, Loss_clf 0.431, Loss_fe 0.611, Loss_kd 2.533, Train_accy 83.23, Test_accy 74.67
2023-12-09 13:52:52,917 [foster.py] => Task 2, Epoch 141/170 => Loss 3.991, Loss_clf 0.433, Loss_fe 0.607, Loss_kd 2.530, Train_accy 83.89
2023-12-09 13:52:58,690 [foster.py] => Task 2, Epoch 142/170 => Loss 3.960, Loss_clf 0.426, Loss_fe 0.587, Loss_kd 2.526, Train_accy 84.13, Test_accy 74.29
2023-12-09 13:53:04,527 [foster.py] => Task 2, Epoch 143/170 => Loss 3.937, Loss_clf 0.414, Loss_fe 0.577, Loss_kd 2.525, Train_accy 84.52, Test_accy 74.71
2023-12-09 13:53:10,310 [foster.py] => Task 2, Epoch 144/170 => Loss 3.903, Loss_clf 0.398, Loss_fe 0.556, Loss_kd 2.527, Train_accy 83.98, Test_accy 74.57
2023-12-09 13:53:16,040 [foster.py] => Task 2, Epoch 145/170 => Loss 3.927, Loss_clf 0.409, Loss_fe 0.563, Loss_kd 2.533, Train_accy 84.52, Test_accy 74.51
2023-12-09 13:53:19,708 [foster.py] => Task 2, Epoch 146/170 => Loss 3.915, Loss_clf 0.407, Loss_fe 0.557, Loss_kd 2.529, Train_accy 84.42
2023-12-09 13:53:25,477 [foster.py] => Task 2, Epoch 147/170 => Loss 3.898, Loss_clf 0.395, Loss_fe 0.554, Loss_kd 2.528, Train_accy 84.84, Test_accy 74.41
2023-12-09 13:53:31,279 [foster.py] => Task 2, Epoch 148/170 => Loss 3.897, Loss_clf 0.402, Loss_fe 0.545, Loss_kd 2.528, Train_accy 84.13, Test_accy 74.61
2023-12-09 13:53:37,055 [foster.py] => Task 2, Epoch 149/170 => Loss 3.868, Loss_clf 0.376, Loss_fe 0.535, Loss_kd 2.535, Train_accy 85.66, Test_accy 74.51
2023-12-09 13:53:42,946 [foster.py] => Task 2, Epoch 150/170 => Loss 3.886, Loss_clf 0.400, Loss_fe 0.538, Loss_kd 2.526, Train_accy 84.84, Test_accy 74.36
2023-12-09 13:53:46,626 [foster.py] => Task 2, Epoch 151/170 => Loss 3.866, Loss_clf 0.384, Loss_fe 0.523, Loss_kd 2.536, Train_accy 85.53
2023-12-09 13:53:53,027 [foster.py] => Task 2, Epoch 152/170 => Loss 3.871, Loss_clf 0.389, Loss_fe 0.534, Loss_kd 2.526, Train_accy 85.34, Test_accy 74.50
2023-12-09 13:53:59,124 [foster.py] => Task 2, Epoch 153/170 => Loss 3.875, Loss_clf 0.395, Loss_fe 0.533, Loss_kd 2.527, Train_accy 85.61, Test_accy 74.53
2023-12-09 13:54:04,788 [foster.py] => Task 2, Epoch 154/170 => Loss 3.881, Loss_clf 0.398, Loss_fe 0.536, Loss_kd 2.526, Train_accy 85.23, Test_accy 74.39
2023-12-09 13:54:10,501 [foster.py] => Task 2, Epoch 155/170 => Loss 3.843, Loss_clf 0.381, Loss_fe 0.521, Loss_kd 2.521, Train_accy 85.76, Test_accy 74.67
2023-12-09 13:54:13,923 [foster.py] => Task 2, Epoch 156/170 => Loss 3.810, Loss_clf 0.362, Loss_fe 0.498, Loss_kd 2.529, Train_accy 86.16
2023-12-09 13:54:19,611 [foster.py] => Task 2, Epoch 157/170 => Loss 3.841, Loss_clf 0.378, Loss_fe 0.519, Loss_kd 2.524, Train_accy 86.23, Test_accy 74.67
2023-12-09 13:54:25,283 [foster.py] => Task 2, Epoch 158/170 => Loss 3.863, Loss_clf 0.382, Loss_fe 0.528, Loss_kd 2.531, Train_accy 85.68, Test_accy 74.67
2023-12-09 13:54:30,922 [foster.py] => Task 2, Epoch 159/170 => Loss 3.869, Loss_clf 0.390, Loss_fe 0.528, Loss_kd 2.530, Train_accy 86.23, Test_accy 74.59
2023-12-09 13:54:36,675 [foster.py] => Task 2, Epoch 160/170 => Loss 3.856, Loss_clf 0.391, Loss_fe 0.522, Loss_kd 2.523, Train_accy 85.19, Test_accy 74.76
2023-12-09 13:54:40,278 [foster.py] => Task 2, Epoch 161/170 => Loss 3.846, Loss_clf 0.384, Loss_fe 0.511, Loss_kd 2.530, Train_accy 85.11
2023-12-09 13:54:46,424 [foster.py] => Task 2, Epoch 162/170 => Loss 3.892, Loss_clf 0.397, Loss_fe 0.536, Loss_kd 2.536, Train_accy 85.02, Test_accy 74.74
2023-12-09 13:54:52,866 [foster.py] => Task 2, Epoch 163/170 => Loss 3.829, Loss_clf 0.383, Loss_fe 0.504, Loss_kd 2.521, Train_accy 85.65, Test_accy 74.79
2023-12-09 13:54:59,043 [foster.py] => Task 2, Epoch 164/170 => Loss 3.876, Loss_clf 0.395, Loss_fe 0.520, Loss_kd 2.538, Train_accy 85.45, Test_accy 74.77
2023-12-09 13:55:04,974 [foster.py] => Task 2, Epoch 165/170 => Loss 3.821, Loss_clf 0.372, Loss_fe 0.504, Loss_kd 2.524, Train_accy 85.94, Test_accy 74.74
2023-12-09 13:55:08,517 [foster.py] => Task 2, Epoch 166/170 => Loss 3.849, Loss_clf 0.384, Loss_fe 0.513, Loss_kd 2.531, Train_accy 86.15
2023-12-09 13:55:14,211 [foster.py] => Task 2, Epoch 167/170 => Loss 3.853, Loss_clf 0.389, Loss_fe 0.520, Loss_kd 2.524, Train_accy 85.37, Test_accy 74.67
2023-12-09 13:55:19,890 [foster.py] => Task 2, Epoch 168/170 => Loss 3.870, Loss_clf 0.394, Loss_fe 0.525, Loss_kd 2.529, Train_accy 85.42, Test_accy 74.66
2023-12-09 13:55:25,707 [foster.py] => Task 2, Epoch 169/170 => Loss 3.805, Loss_clf 0.371, Loss_fe 0.493, Loss_kd 2.521, Train_accy 86.24, Test_accy 74.81
2023-12-09 13:55:31,543 [foster.py] => Task 2, Epoch 170/170 => Loss 3.824, Loss_clf 0.372, Loss_fe 0.507, Loss_kd 2.524, Train_accy 86.34, Test_accy 74.74
2023-12-09 13:55:31,544 [foster.py] => do not weight align teacher!
2023-12-09 13:55:31,546 [foster.py] => per cls weights : [1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814 1.08422814
 0.49463113 0.49463113 0.49463113 0.49463113 0.49463113 0.49463113
 0.49463113 0.49463113 0.49463113 0.49463113]
2023-12-09 13:55:37,864 [foster.py] => SNet: Task 2, Epoch 1/130 => Loss 3.139,  Train_accy 41.26, Test_accy 63.74
2023-12-09 13:55:41,851 [foster.py] => SNet: Task 2, Epoch 2/130 => Loss 2.834,  Train_accy 59.98
2023-12-09 13:55:45,675 [foster.py] => SNet: Task 2, Epoch 3/130 => Loss 2.828,  Train_accy 62.16
2023-12-09 13:55:49,703 [foster.py] => SNet: Task 2, Epoch 4/130 => Loss 2.794,  Train_accy 64.27
2023-12-09 13:55:53,531 [foster.py] => SNet: Task 2, Epoch 5/130 => Loss 2.788,  Train_accy 64.53
2023-12-09 13:55:59,708 [foster.py] => SNet: Task 2, Epoch 6/130 => Loss 2.788,  Train_accy 65.29, Test_accy 67.99
2023-12-09 13:56:03,982 [foster.py] => SNet: Task 2, Epoch 7/130 => Loss 2.777,  Train_accy 65.90
2023-12-09 13:56:08,137 [foster.py] => SNet: Task 2, Epoch 8/130 => Loss 2.784,  Train_accy 64.81
2023-12-09 13:56:12,318 [foster.py] => SNet: Task 2, Epoch 9/130 => Loss 2.754,  Train_accy 67.31
2023-12-09 13:56:16,068 [foster.py] => SNet: Task 2, Epoch 10/130 => Loss 2.771,  Train_accy 67.32
2023-12-09 13:56:21,851 [foster.py] => SNet: Task 2, Epoch 11/130 => Loss 2.760,  Train_accy 67.58, Test_accy 68.60
2023-12-09 13:56:25,778 [foster.py] => SNet: Task 2, Epoch 12/130 => Loss 2.747,  Train_accy 68.31
2023-12-09 13:56:29,948 [foster.py] => SNet: Task 2, Epoch 13/130 => Loss 2.764,  Train_accy 68.15
2023-12-09 13:56:33,924 [foster.py] => SNet: Task 2, Epoch 14/130 => Loss 2.735,  Train_accy 68.73
2023-12-09 13:56:37,776 [foster.py] => SNet: Task 2, Epoch 15/130 => Loss 2.749,  Train_accy 68.19
2023-12-09 13:56:43,758 [foster.py] => SNet: Task 2, Epoch 16/130 => Loss 2.762,  Train_accy 68.42, Test_accy 68.47
2023-12-09 13:56:47,638 [foster.py] => SNet: Task 2, Epoch 17/130 => Loss 2.741,  Train_accy 68.84
2023-12-09 13:56:51,791 [foster.py] => SNet: Task 2, Epoch 18/130 => Loss 2.738,  Train_accy 69.81
2023-12-09 13:56:56,083 [foster.py] => SNet: Task 2, Epoch 19/130 => Loss 2.705,  Train_accy 70.15
2023-12-09 13:57:00,244 [foster.py] => SNet: Task 2, Epoch 20/130 => Loss 2.740,  Train_accy 68.45
2023-12-09 13:57:06,531 [foster.py] => SNet: Task 2, Epoch 21/130 => Loss 2.742,  Train_accy 68.68, Test_accy 68.43
2023-12-09 13:57:10,716 [foster.py] => SNet: Task 2, Epoch 22/130 => Loss 2.748,  Train_accy 68.98
2023-12-09 13:57:14,942 [foster.py] => SNet: Task 2, Epoch 23/130 => Loss 2.744,  Train_accy 69.73
2023-12-09 13:57:19,109 [foster.py] => SNet: Task 2, Epoch 24/130 => Loss 2.727,  Train_accy 69.10
2023-12-09 13:57:23,116 [foster.py] => SNet: Task 2, Epoch 25/130 => Loss 2.718,  Train_accy 70.50
2023-12-09 13:57:29,301 [foster.py] => SNet: Task 2, Epoch 26/130 => Loss 2.734,  Train_accy 69.92, Test_accy 69.70
2023-12-09 13:57:33,438 [foster.py] => SNet: Task 2, Epoch 27/130 => Loss 2.715,  Train_accy 69.82
2023-12-09 13:57:37,516 [foster.py] => SNet: Task 2, Epoch 28/130 => Loss 2.741,  Train_accy 69.29
2023-12-09 13:57:41,464 [foster.py] => SNet: Task 2, Epoch 29/130 => Loss 2.727,  Train_accy 69.84
2023-12-09 13:57:45,461 [foster.py] => SNet: Task 2, Epoch 30/130 => Loss 2.721,  Train_accy 69.05
2023-12-09 13:57:51,658 [foster.py] => SNet: Task 2, Epoch 31/130 => Loss 2.733,  Train_accy 70.79, Test_accy 70.01
2023-12-09 13:57:55,828 [foster.py] => SNet: Task 2, Epoch 32/130 => Loss 2.739,  Train_accy 69.73
2023-12-09 13:57:59,826 [foster.py] => SNet: Task 2, Epoch 33/130 => Loss 2.736,  Train_accy 70.61
2023-12-09 13:58:03,991 [foster.py] => SNet: Task 2, Epoch 34/130 => Loss 2.718,  Train_accy 70.39
2023-12-09 13:58:07,869 [foster.py] => SNet: Task 2, Epoch 35/130 => Loss 2.704,  Train_accy 71.76
2023-12-09 13:58:14,114 [foster.py] => SNet: Task 2, Epoch 36/130 => Loss 2.700,  Train_accy 70.71, Test_accy 69.79
2023-12-09 13:58:18,230 [foster.py] => SNet: Task 2, Epoch 37/130 => Loss 2.704,  Train_accy 71.13
2023-12-09 13:58:22,244 [foster.py] => SNet: Task 2, Epoch 38/130 => Loss 2.716,  Train_accy 70.60
2023-12-09 13:58:26,162 [foster.py] => SNet: Task 2, Epoch 39/130 => Loss 2.703,  Train_accy 70.37
2023-12-09 13:58:30,123 [foster.py] => SNet: Task 2, Epoch 40/130 => Loss 2.721,  Train_accy 71.03
2023-12-09 13:58:36,398 [foster.py] => SNet: Task 2, Epoch 41/130 => Loss 2.713,  Train_accy 71.65, Test_accy 70.21
2023-12-09 13:58:40,391 [foster.py] => SNet: Task 2, Epoch 42/130 => Loss 2.709,  Train_accy 70.05
2023-12-09 13:58:44,362 [foster.py] => SNet: Task 2, Epoch 43/130 => Loss 2.707,  Train_accy 71.03
2023-12-09 13:58:48,382 [foster.py] => SNet: Task 2, Epoch 44/130 => Loss 2.698,  Train_accy 71.45
2023-12-09 13:58:52,391 [foster.py] => SNet: Task 2, Epoch 45/130 => Loss 2.706,  Train_accy 71.23
2023-12-09 13:58:58,803 [foster.py] => SNet: Task 2, Epoch 46/130 => Loss 2.706,  Train_accy 71.32, Test_accy 71.01
2023-12-09 13:59:02,928 [foster.py] => SNet: Task 2, Epoch 47/130 => Loss 2.725,  Train_accy 70.81
2023-12-09 13:59:07,177 [foster.py] => SNet: Task 2, Epoch 48/130 => Loss 2.717,  Train_accy 71.19
2023-12-09 13:59:11,280 [foster.py] => SNet: Task 2, Epoch 49/130 => Loss 2.702,  Train_accy 70.94
2023-12-09 13:59:15,316 [foster.py] => SNet: Task 2, Epoch 50/130 => Loss 2.712,  Train_accy 71.69
2023-12-09 13:59:21,351 [foster.py] => SNet: Task 2, Epoch 51/130 => Loss 2.694,  Train_accy 71.63, Test_accy 71.71
2023-12-09 13:59:25,204 [foster.py] => SNet: Task 2, Epoch 52/130 => Loss 2.696,  Train_accy 71.98
2023-12-09 13:59:29,052 [foster.py] => SNet: Task 2, Epoch 53/130 => Loss 2.708,  Train_accy 71.65
2023-12-09 13:59:33,020 [foster.py] => SNet: Task 2, Epoch 54/130 => Loss 2.691,  Train_accy 71.48
2023-12-09 13:59:36,755 [foster.py] => SNet: Task 2, Epoch 55/130 => Loss 2.707,  Train_accy 71.23
2023-12-09 13:59:42,909 [foster.py] => SNet: Task 2, Epoch 56/130 => Loss 2.698,  Train_accy 71.15, Test_accy 71.19
2023-12-09 13:59:46,999 [foster.py] => SNet: Task 2, Epoch 57/130 => Loss 2.684,  Train_accy 71.85
2023-12-09 13:59:50,718 [foster.py] => SNet: Task 2, Epoch 58/130 => Loss 2.710,  Train_accy 71.15
2023-12-09 13:59:54,548 [foster.py] => SNet: Task 2, Epoch 59/130 => Loss 2.700,  Train_accy 71.31
2023-12-09 13:59:58,214 [foster.py] => SNet: Task 2, Epoch 60/130 => Loss 2.700,  Train_accy 70.82
2023-12-09 14:00:03,841 [foster.py] => SNet: Task 2, Epoch 61/130 => Loss 2.686,  Train_accy 71.65, Test_accy 71.36
2023-12-09 14:00:07,632 [foster.py] => SNet: Task 2, Epoch 62/130 => Loss 2.685,  Train_accy 72.53
2023-12-09 14:00:11,411 [foster.py] => SNet: Task 2, Epoch 63/130 => Loss 2.689,  Train_accy 71.18
2023-12-09 14:00:15,171 [foster.py] => SNet: Task 2, Epoch 64/130 => Loss 2.698,  Train_accy 72.50
2023-12-09 14:00:18,845 [foster.py] => SNet: Task 2, Epoch 65/130 => Loss 2.707,  Train_accy 71.63
2023-12-09 14:00:24,164 [foster.py] => SNet: Task 2, Epoch 66/130 => Loss 2.705,  Train_accy 71.08, Test_accy 72.71
2023-12-09 14:00:27,848 [foster.py] => SNet: Task 2, Epoch 67/130 => Loss 2.687,  Train_accy 71.68
2023-12-09 14:00:31,683 [foster.py] => SNet: Task 2, Epoch 68/130 => Loss 2.685,  Train_accy 72.94
2023-12-09 14:00:35,525 [foster.py] => SNet: Task 2, Epoch 69/130 => Loss 2.683,  Train_accy 72.79
2023-12-09 14:00:39,226 [foster.py] => SNet: Task 2, Epoch 70/130 => Loss 2.703,  Train_accy 72.19
2023-12-09 14:00:44,767 [foster.py] => SNet: Task 2, Epoch 71/130 => Loss 2.695,  Train_accy 72.32, Test_accy 72.44
2023-12-09 14:00:48,493 [foster.py] => SNet: Task 2, Epoch 72/130 => Loss 2.699,  Train_accy 71.79
2023-12-09 14:00:52,392 [foster.py] => SNet: Task 2, Epoch 73/130 => Loss 2.676,  Train_accy 73.06
2023-12-09 14:00:56,091 [foster.py] => SNet: Task 2, Epoch 74/130 => Loss 2.681,  Train_accy 71.81
2023-12-09 14:00:59,735 [foster.py] => SNet: Task 2, Epoch 75/130 => Loss 2.698,  Train_accy 71.74
2023-12-09 14:01:05,559 [foster.py] => SNet: Task 2, Epoch 76/130 => Loss 2.686,  Train_accy 72.95, Test_accy 72.30
2023-12-09 14:01:09,334 [foster.py] => SNet: Task 2, Epoch 77/130 => Loss 2.687,  Train_accy 72.61
2023-12-09 14:01:13,037 [foster.py] => SNet: Task 2, Epoch 78/130 => Loss 2.687,  Train_accy 72.73
2023-12-09 14:01:16,744 [foster.py] => SNet: Task 2, Epoch 79/130 => Loss 2.677,  Train_accy 73.29
2023-12-09 14:01:20,562 [foster.py] => SNet: Task 2, Epoch 80/130 => Loss 2.698,  Train_accy 72.58
2023-12-09 14:01:26,077 [foster.py] => SNet: Task 2, Epoch 81/130 => Loss 2.686,  Train_accy 72.35, Test_accy 72.46
2023-12-09 14:01:29,818 [foster.py] => SNet: Task 2, Epoch 82/130 => Loss 2.682,  Train_accy 72.13
2023-12-09 14:01:33,516 [foster.py] => SNet: Task 2, Epoch 83/130 => Loss 2.692,  Train_accy 72.90
2023-12-09 14:01:37,181 [foster.py] => SNet: Task 2, Epoch 84/130 => Loss 2.688,  Train_accy 72.69
2023-12-09 14:01:41,032 [foster.py] => SNet: Task 2, Epoch 85/130 => Loss 2.689,  Train_accy 72.53
2023-12-09 14:01:46,690 [foster.py] => SNet: Task 2, Epoch 86/130 => Loss 2.678,  Train_accy 72.76, Test_accy 72.61
2023-12-09 14:01:50,372 [foster.py] => SNet: Task 2, Epoch 87/130 => Loss 2.686,  Train_accy 72.66
2023-12-09 14:01:54,068 [foster.py] => SNet: Task 2, Epoch 88/130 => Loss 2.705,  Train_accy 72.23
2023-12-09 14:01:57,819 [foster.py] => SNet: Task 2, Epoch 89/130 => Loss 2.690,  Train_accy 72.65
2023-12-09 14:02:01,706 [foster.py] => SNet: Task 2, Epoch 90/130 => Loss 2.678,  Train_accy 72.94
2023-12-09 14:02:07,314 [foster.py] => SNet: Task 2, Epoch 91/130 => Loss 2.687,  Train_accy 72.73, Test_accy 72.71
2023-12-09 14:02:11,067 [foster.py] => SNet: Task 2, Epoch 92/130 => Loss 2.677,  Train_accy 73.81
2023-12-09 14:02:14,924 [foster.py] => SNet: Task 2, Epoch 93/130 => Loss 2.679,  Train_accy 72.82
2023-12-09 14:02:18,657 [foster.py] => SNet: Task 2, Epoch 94/130 => Loss 2.668,  Train_accy 72.56
2023-12-09 14:02:22,401 [foster.py] => SNet: Task 2, Epoch 95/130 => Loss 2.698,  Train_accy 72.68
2023-12-09 14:02:28,112 [foster.py] => SNet: Task 2, Epoch 96/130 => Loss 2.685,  Train_accy 72.40, Test_accy 73.23
2023-12-09 14:02:31,843 [foster.py] => SNet: Task 2, Epoch 97/130 => Loss 2.695,  Train_accy 72.45
2023-12-09 14:02:35,670 [foster.py] => SNet: Task 2, Epoch 98/130 => Loss 2.699,  Train_accy 72.10
2023-12-09 14:02:39,596 [foster.py] => SNet: Task 2, Epoch 99/130 => Loss 2.671,  Train_accy 73.24
2023-12-09 14:02:43,263 [foster.py] => SNet: Task 2, Epoch 100/130 => Loss 2.688,  Train_accy 72.76
2023-12-09 14:02:48,842 [foster.py] => SNet: Task 2, Epoch 101/130 => Loss 2.671,  Train_accy 72.18, Test_accy 73.27
2023-12-09 14:02:52,695 [foster.py] => SNet: Task 2, Epoch 102/130 => Loss 2.673,  Train_accy 72.55
2023-12-09 14:02:56,391 [foster.py] => SNet: Task 2, Epoch 103/130 => Loss 2.692,  Train_accy 72.66
2023-12-09 14:03:00,093 [foster.py] => SNet: Task 2, Epoch 104/130 => Loss 2.684,  Train_accy 73.06
2023-12-09 14:03:03,728 [foster.py] => SNet: Task 2, Epoch 105/130 => Loss 2.689,  Train_accy 73.31
2023-12-09 14:03:09,371 [foster.py] => SNet: Task 2, Epoch 106/130 => Loss 2.691,  Train_accy 72.52, Test_accy 73.37
2023-12-09 14:03:13,112 [foster.py] => SNet: Task 2, Epoch 107/130 => Loss 2.696,  Train_accy 72.03
2023-12-09 14:03:16,607 [foster.py] => SNet: Task 2, Epoch 108/130 => Loss 2.693,  Train_accy 72.37
2023-12-09 14:03:20,357 [foster.py] => SNet: Task 2, Epoch 109/130 => Loss 2.675,  Train_accy 73.05
2023-12-09 14:03:24,142 [foster.py] => SNet: Task 2, Epoch 110/130 => Loss 2.676,  Train_accy 73.48
2023-12-09 14:03:29,934 [foster.py] => SNet: Task 2, Epoch 111/130 => Loss 2.690,  Train_accy 73.32, Test_accy 73.30
2023-12-09 14:03:33,730 [foster.py] => SNet: Task 2, Epoch 112/130 => Loss 2.664,  Train_accy 72.74
2023-12-09 14:03:37,370 [foster.py] => SNet: Task 2, Epoch 113/130 => Loss 2.683,  Train_accy 73.37
2023-12-09 14:03:41,116 [foster.py] => SNet: Task 2, Epoch 114/130 => Loss 2.685,  Train_accy 73.44
2023-12-09 14:03:44,899 [foster.py] => SNet: Task 2, Epoch 115/130 => Loss 2.680,  Train_accy 72.69
2023-12-09 14:03:50,446 [foster.py] => SNet: Task 2, Epoch 116/130 => Loss 2.674,  Train_accy 73.08, Test_accy 73.53
2023-12-09 14:03:54,321 [foster.py] => SNet: Task 2, Epoch 117/130 => Loss 2.685,  Train_accy 73.44
2023-12-09 14:03:58,087 [foster.py] => SNet: Task 2, Epoch 118/130 => Loss 2.678,  Train_accy 72.77
2023-12-09 14:04:01,922 [foster.py] => SNet: Task 2, Epoch 119/130 => Loss 2.690,  Train_accy 72.77
2023-12-09 14:04:05,649 [foster.py] => SNet: Task 2, Epoch 120/130 => Loss 2.687,  Train_accy 71.89
2023-12-09 14:04:11,293 [foster.py] => SNet: Task 2, Epoch 121/130 => Loss 2.664,  Train_accy 73.65, Test_accy 73.44
2023-12-09 14:04:15,072 [foster.py] => SNet: Task 2, Epoch 122/130 => Loss 2.686,  Train_accy 73.39
2023-12-09 14:04:18,946 [foster.py] => SNet: Task 2, Epoch 123/130 => Loss 2.679,  Train_accy 72.89
2023-12-09 14:04:22,650 [foster.py] => SNet: Task 2, Epoch 124/130 => Loss 2.684,  Train_accy 73.47
2023-12-09 14:04:26,329 [foster.py] => SNet: Task 2, Epoch 125/130 => Loss 2.675,  Train_accy 72.84
2023-12-09 14:04:31,943 [foster.py] => SNet: Task 2, Epoch 126/130 => Loss 2.668,  Train_accy 72.90, Test_accy 73.30
2023-12-09 14:04:35,677 [foster.py] => SNet: Task 2, Epoch 127/130 => Loss 2.681,  Train_accy 72.84
2023-12-09 14:04:39,372 [foster.py] => SNet: Task 2, Epoch 128/130 => Loss 2.685,  Train_accy 72.42
2023-12-09 14:04:43,047 [foster.py] => SNet: Task 2, Epoch 129/130 => Loss 2.671,  Train_accy 73.05
2023-12-09 14:04:46,749 [foster.py] => SNet: Task 2, Epoch 130/130 => Loss 2.678,  Train_accy 72.60
2023-12-09 14:04:46,751 [foster.py] => do not weight align student!
2023-12-09 14:04:48,618 [foster.py] => darknet eval: 
2023-12-09 14:04:48,618 [foster.py] => CNN top1 curve: 73.53
2023-12-09 14:04:48,619 [foster.py] => CNN top5 curve: 93.73
2023-12-09 14:04:48,619 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 14:05:54,674 [foster.py] => Exemplar size: 1400
2023-12-09 14:05:54,674 [trainer.py] => CNN: {'total': 74.74, '00-09': 78.0, '10-19': 68.5, '20-29': 79.1, '30-39': 71.7, '40-49': 77.0, '50-59': 66.1, '60-69': 82.8, 'old': 73.4, 'new': 82.8}
2023-12-09 14:05:54,674 [trainer.py] => NME: {'total': 65.59, '00-09': 68.5, '10-19': 57.0, '20-29': 69.6, '30-39': 60.5, '40-49': 66.8, '50-59': 57.0, '60-69': 79.7, 'old': 63.23, 'new': 79.7}
2023-12-09 14:05:54,674 [trainer.py] => CNN top1 curve: [80.38, 76.75, 74.74]
2023-12-09 14:05:54,674 [trainer.py] => CNN top5 curve: [96.48, 94.95, 94.13]
2023-12-09 14:05:54,674 [trainer.py] => NME top1 curve: [78.66, 72.0, 65.59]
2023-12-09 14:05:54,674 [trainer.py] => NME top5 curve: [96.2, 93.15, 90.0]

2023-12-09 14:05:54,675 [trainer.py] => All params: 945788
2023-12-09 14:05:54,676 [trainer.py] => Trainable params: 477734
2023-12-09 14:05:54,700 [foster.py] => Learning on 70-80
2023-12-09 14:05:54,701 [foster.py] => All params: 948378
2023-12-09 14:05:54,701 [foster.py] => Trainable params: 479674
2023-12-09 14:05:54,799 [foster.py] => per cls weights : [1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779 1.03762779
 1.03762779 1.03762779 1.03762779 1.03762779 0.73660549 0.73660549
 0.73660549 0.73660549 0.73660549 0.73660549 0.73660549 0.73660549
 0.73660549 0.73660549]
2023-12-09 14:05:58,342 [foster.py] => Task 3, Epoch 1/170 => Loss 8.362, Loss_clf 2.218, Loss_fe 3.039, Loss_kd 2.717, Train_accy 47.05
2023-12-09 14:06:04,256 [foster.py] => Task 3, Epoch 2/170 => Loss 6.773, Loss_clf 1.211, Loss_fe 2.519, Loss_kd 2.663, Train_accy 56.98, Test_accy 62.25
2023-12-09 14:06:10,064 [foster.py] => Task 3, Epoch 3/170 => Loss 6.495, Loss_clf 1.124, Loss_fe 2.322, Loss_kd 2.669, Train_accy 57.16, Test_accy 65.24
2023-12-09 14:06:15,969 [foster.py] => Task 3, Epoch 4/170 => Loss 6.386, Loss_clf 1.174, Loss_fe 2.162, Loss_kd 2.669, Train_accy 57.05, Test_accy 64.39
2023-12-09 14:06:21,896 [foster.py] => Task 3, Epoch 5/170 => Loss 6.269, Loss_clf 1.110, Loss_fe 2.096, Loss_kd 2.680, Train_accy 57.09, Test_accy 66.99
2023-12-09 14:06:25,370 [foster.py] => Task 3, Epoch 6/170 => Loss 6.181, Loss_clf 1.106, Loss_fe 2.030, Loss_kd 2.664, Train_accy 57.61
2023-12-09 14:06:31,267 [foster.py] => Task 3, Epoch 7/170 => Loss 6.082, Loss_clf 1.078, Loss_fe 1.956, Loss_kd 2.667, Train_accy 59.09, Test_accy 65.03
2023-12-09 14:06:37,227 [foster.py] => Task 3, Epoch 8/170 => Loss 6.093, Loss_clf 1.118, Loss_fe 1.921, Loss_kd 2.673, Train_accy 58.78, Test_accy 66.78
2023-12-09 14:06:43,092 [foster.py] => Task 3, Epoch 9/170 => Loss 6.021, Loss_clf 1.077, Loss_fe 1.894, Loss_kd 2.668, Train_accy 59.36, Test_accy 67.64
2023-12-09 14:06:48,915 [foster.py] => Task 3, Epoch 10/170 => Loss 6.026, Loss_clf 1.118, Loss_fe 1.863, Loss_kd 2.665, Train_accy 58.45, Test_accy 65.16
2023-12-09 14:06:52,256 [foster.py] => Task 3, Epoch 11/170 => Loss 5.920, Loss_clf 1.041, Loss_fe 1.828, Loss_kd 2.669, Train_accy 59.92
2023-12-09 14:06:58,170 [foster.py] => Task 3, Epoch 12/170 => Loss 5.941, Loss_clf 1.102, Loss_fe 1.792, Loss_kd 2.666, Train_accy 59.00, Test_accy 66.14
2023-12-09 14:07:04,208 [foster.py] => Task 3, Epoch 13/170 => Loss 5.841, Loss_clf 1.031, Loss_fe 1.764, Loss_kd 2.666, Train_accy 61.14, Test_accy 63.42
2023-12-09 14:07:10,373 [foster.py] => Task 3, Epoch 14/170 => Loss 5.983, Loss_clf 1.179, Loss_fe 1.755, Loss_kd 2.668, Train_accy 59.11, Test_accy 64.84
2023-12-09 14:07:16,353 [foster.py] => Task 3, Epoch 15/170 => Loss 5.823, Loss_clf 1.027, Loss_fe 1.740, Loss_kd 2.673, Train_accy 61.17, Test_accy 66.61
2023-12-09 14:07:19,859 [foster.py] => Task 3, Epoch 16/170 => Loss 5.766, Loss_clf 1.014, Loss_fe 1.699, Loss_kd 2.672, Train_accy 60.50
2023-12-09 14:07:25,749 [foster.py] => Task 3, Epoch 17/170 => Loss 5.692, Loss_clf 0.984, Loss_fe 1.650, Loss_kd 2.675, Train_accy 62.05, Test_accy 67.24
2023-12-09 14:07:31,598 [foster.py] => Task 3, Epoch 18/170 => Loss 5.688, Loss_clf 0.973, Loss_fe 1.666, Loss_kd 2.668, Train_accy 62.25, Test_accy 67.29
2023-12-09 14:07:37,512 [foster.py] => Task 3, Epoch 19/170 => Loss 5.694, Loss_clf 0.998, Loss_fe 1.644, Loss_kd 2.671, Train_accy 60.86, Test_accy 65.68
2023-12-09 14:07:43,273 [foster.py] => Task 3, Epoch 20/170 => Loss 5.739, Loss_clf 1.044, Loss_fe 1.639, Loss_kd 2.674, Train_accy 60.72, Test_accy 67.54
2023-12-09 14:07:46,753 [foster.py] => Task 3, Epoch 21/170 => Loss 5.651, Loss_clf 0.969, Loss_fe 1.638, Loss_kd 2.663, Train_accy 62.38
2023-12-09 14:07:52,742 [foster.py] => Task 3, Epoch 22/170 => Loss 5.723, Loss_clf 1.063, Loss_fe 1.619, Loss_kd 2.661, Train_accy 60.16, Test_accy 67.00
2023-12-09 14:07:58,736 [foster.py] => Task 3, Epoch 23/170 => Loss 5.717, Loss_clf 1.023, Loss_fe 1.636, Loss_kd 2.676, Train_accy 61.98, Test_accy 66.47
2023-12-09 14:08:04,624 [foster.py] => Task 3, Epoch 24/170 => Loss 5.687, Loss_clf 1.051, Loss_fe 1.588, Loss_kd 2.667, Train_accy 61.31, Test_accy 66.34
2023-12-09 14:08:10,464 [foster.py] => Task 3, Epoch 25/170 => Loss 5.611, Loss_clf 0.994, Loss_fe 1.568, Loss_kd 2.669, Train_accy 62.30, Test_accy 67.50
2023-12-09 14:08:14,123 [foster.py] => Task 3, Epoch 26/170 => Loss 5.678, Loss_clf 1.066, Loss_fe 1.568, Loss_kd 2.663, Train_accy 61.41
2023-12-09 14:08:19,923 [foster.py] => Task 3, Epoch 27/170 => Loss 5.612, Loss_clf 0.985, Loss_fe 1.583, Loss_kd 2.663, Train_accy 61.95, Test_accy 67.56
2023-12-09 14:08:25,645 [foster.py] => Task 3, Epoch 28/170 => Loss 5.497, Loss_clf 0.934, Loss_fe 1.527, Loss_kd 2.656, Train_accy 63.38, Test_accy 65.96
2023-12-09 14:08:31,476 [foster.py] => Task 3, Epoch 29/170 => Loss 5.551, Loss_clf 0.994, Loss_fe 1.517, Loss_kd 2.659, Train_accy 61.78, Test_accy 66.76
2023-12-09 14:08:37,516 [foster.py] => Task 3, Epoch 30/170 => Loss 5.585, Loss_clf 0.995, Loss_fe 1.533, Loss_kd 2.674, Train_accy 62.12, Test_accy 68.22
2023-12-09 14:08:40,995 [foster.py] => Task 3, Epoch 31/170 => Loss 5.608, Loss_clf 1.053, Loss_fe 1.512, Loss_kd 2.663, Train_accy 61.73
2023-12-09 14:08:46,744 [foster.py] => Task 3, Epoch 32/170 => Loss 5.675, Loss_clf 1.056, Loss_fe 1.573, Loss_kd 2.665, Train_accy 62.47, Test_accy 67.88
2023-12-09 14:08:52,517 [foster.py] => Task 3, Epoch 33/170 => Loss 5.517, Loss_clf 0.974, Loss_fe 1.495, Loss_kd 2.667, Train_accy 62.72, Test_accy 64.76
2023-12-09 14:08:58,418 [foster.py] => Task 3, Epoch 34/170 => Loss 5.510, Loss_clf 0.958, Loss_fe 1.495, Loss_kd 2.675, Train_accy 63.72, Test_accy 67.09
2023-12-09 14:09:04,288 [foster.py] => Task 3, Epoch 35/170 => Loss 5.562, Loss_clf 1.015, Loss_fe 1.502, Loss_kd 2.664, Train_accy 61.88, Test_accy 67.18
2023-12-09 14:09:07,748 [foster.py] => Task 3, Epoch 36/170 => Loss 5.456, Loss_clf 0.942, Loss_fe 1.456, Loss_kd 2.676, Train_accy 63.92
2023-12-09 14:09:13,631 [foster.py] => Task 3, Epoch 37/170 => Loss 5.506, Loss_clf 0.969, Loss_fe 1.475, Loss_kd 2.679, Train_accy 63.80, Test_accy 67.00
2023-12-09 14:09:19,506 [foster.py] => Task 3, Epoch 38/170 => Loss 5.398, Loss_clf 0.902, Loss_fe 1.451, Loss_kd 2.664, Train_accy 63.77, Test_accy 66.04
2023-12-09 14:09:25,533 [foster.py] => Task 3, Epoch 39/170 => Loss 5.480, Loss_clf 0.975, Loss_fe 1.441, Loss_kd 2.681, Train_accy 63.48, Test_accy 65.21
2023-12-09 14:09:31,547 [foster.py] => Task 3, Epoch 40/170 => Loss 5.460, Loss_clf 0.960, Loss_fe 1.446, Loss_kd 2.673, Train_accy 63.97, Test_accy 67.21
2023-12-09 14:09:34,996 [foster.py] => Task 3, Epoch 41/170 => Loss 5.412, Loss_clf 0.924, Loss_fe 1.434, Loss_kd 2.672, Train_accy 65.17
2023-12-09 14:09:40,891 [foster.py] => Task 3, Epoch 42/170 => Loss 5.383, Loss_clf 0.917, Loss_fe 1.417, Loss_kd 2.668, Train_accy 64.09, Test_accy 67.00
2023-12-09 14:09:46,666 [foster.py] => Task 3, Epoch 43/170 => Loss 5.376, Loss_clf 0.924, Loss_fe 1.413, Loss_kd 2.659, Train_accy 65.30, Test_accy 67.68
2023-12-09 14:09:52,595 [foster.py] => Task 3, Epoch 44/170 => Loss 5.340, Loss_clf 0.889, Loss_fe 1.394, Loss_kd 2.675, Train_accy 64.77, Test_accy 65.76
2023-12-09 14:09:58,431 [foster.py] => Task 3, Epoch 45/170 => Loss 5.309, Loss_clf 0.906, Loss_fe 1.366, Loss_kd 2.657, Train_accy 65.70, Test_accy 67.49
2023-12-09 14:10:01,950 [foster.py] => Task 3, Epoch 46/170 => Loss 5.440, Loss_clf 0.958, Loss_fe 1.421, Loss_kd 2.679, Train_accy 64.14
2023-12-09 14:10:07,878 [foster.py] => Task 3, Epoch 47/170 => Loss 5.375, Loss_clf 0.917, Loss_fe 1.400, Loss_kd 2.676, Train_accy 63.84, Test_accy 66.41
2023-12-09 14:10:13,949 [foster.py] => Task 3, Epoch 48/170 => Loss 5.281, Loss_clf 0.874, Loss_fe 1.357, Loss_kd 2.668, Train_accy 64.67, Test_accy 67.81
2023-12-09 14:10:19,720 [foster.py] => Task 3, Epoch 49/170 => Loss 5.318, Loss_clf 0.886, Loss_fe 1.372, Loss_kd 2.678, Train_accy 64.19, Test_accy 67.42
2023-12-09 14:10:25,551 [foster.py] => Task 3, Epoch 50/170 => Loss 5.298, Loss_clf 0.880, Loss_fe 1.373, Loss_kd 2.664, Train_accy 64.81, Test_accy 67.38
2023-12-09 14:10:29,075 [foster.py] => Task 3, Epoch 51/170 => Loss 5.265, Loss_clf 0.867, Loss_fe 1.347, Loss_kd 2.670, Train_accy 66.09
2023-12-09 14:10:34,922 [foster.py] => Task 3, Epoch 52/170 => Loss 5.307, Loss_clf 0.903, Loss_fe 1.361, Loss_kd 2.662, Train_accy 65.48, Test_accy 68.10
2023-12-09 14:10:40,825 [foster.py] => Task 3, Epoch 53/170 => Loss 5.241, Loss_clf 0.889, Loss_fe 1.324, Loss_kd 2.649, Train_accy 65.42, Test_accy 67.99
2023-12-09 14:10:46,722 [foster.py] => Task 3, Epoch 54/170 => Loss 5.229, Loss_clf 0.869, Loss_fe 1.318, Loss_kd 2.662, Train_accy 65.25, Test_accy 67.61
2023-12-09 14:10:53,035 [foster.py] => Task 3, Epoch 55/170 => Loss 5.268, Loss_clf 0.898, Loss_fe 1.315, Loss_kd 2.673, Train_accy 64.95, Test_accy 66.35
2023-12-09 14:10:56,497 [foster.py] => Task 3, Epoch 56/170 => Loss 5.284, Loss_clf 0.895, Loss_fe 1.343, Loss_kd 2.665, Train_accy 65.22
2023-12-09 14:11:02,332 [foster.py] => Task 3, Epoch 57/170 => Loss 5.303, Loss_clf 0.910, Loss_fe 1.345, Loss_kd 2.667, Train_accy 65.88, Test_accy 63.95
2023-12-09 14:11:08,248 [foster.py] => Task 3, Epoch 58/170 => Loss 5.276, Loss_clf 0.916, Loss_fe 1.319, Loss_kd 2.661, Train_accy 65.52, Test_accy 68.45
2023-12-09 14:11:14,113 [foster.py] => Task 3, Epoch 59/170 => Loss 5.183, Loss_clf 0.848, Loss_fe 1.295, Loss_kd 2.659, Train_accy 66.52, Test_accy 68.01
2023-12-09 14:11:20,051 [foster.py] => Task 3, Epoch 60/170 => Loss 5.198, Loss_clf 0.844, Loss_fe 1.302, Loss_kd 2.670, Train_accy 65.44, Test_accy 67.45
2023-12-09 14:11:23,614 [foster.py] => Task 3, Epoch 61/170 => Loss 5.178, Loss_clf 0.834, Loss_fe 1.285, Loss_kd 2.677, Train_accy 67.53
2023-12-09 14:11:29,547 [foster.py] => Task 3, Epoch 62/170 => Loss 5.138, Loss_clf 0.818, Loss_fe 1.274, Loss_kd 2.665, Train_accy 67.30, Test_accy 68.96
2023-12-09 14:11:35,493 [foster.py] => Task 3, Epoch 63/170 => Loss 5.166, Loss_clf 0.846, Loss_fe 1.267, Loss_kd 2.671, Train_accy 65.77, Test_accy 66.55
2023-12-09 14:11:41,541 [foster.py] => Task 3, Epoch 64/170 => Loss 5.124, Loss_clf 0.820, Loss_fe 1.265, Loss_kd 2.659, Train_accy 67.39, Test_accy 65.81
2023-12-09 14:11:47,395 [foster.py] => Task 3, Epoch 65/170 => Loss 5.126, Loss_clf 0.825, Loss_fe 1.257, Loss_kd 2.663, Train_accy 67.19, Test_accy 66.54
2023-12-09 14:11:50,841 [foster.py] => Task 3, Epoch 66/170 => Loss 5.104, Loss_clf 0.824, Loss_fe 1.240, Loss_kd 2.661, Train_accy 68.00
2023-12-09 14:11:56,858 [foster.py] => Task 3, Epoch 67/170 => Loss 5.220, Loss_clf 0.890, Loss_fe 1.291, Loss_kd 2.659, Train_accy 66.17, Test_accy 68.80
2023-12-09 14:12:02,742 [foster.py] => Task 3, Epoch 68/170 => Loss 5.087, Loss_clf 0.810, Loss_fe 1.244, Loss_kd 2.654, Train_accy 67.06, Test_accy 67.62
2023-12-09 14:12:08,638 [foster.py] => Task 3, Epoch 69/170 => Loss 5.081, Loss_clf 0.818, Loss_fe 1.225, Loss_kd 2.658, Train_accy 67.92, Test_accy 66.42
2023-12-09 14:12:14,641 [foster.py] => Task 3, Epoch 70/170 => Loss 5.132, Loss_clf 0.832, Loss_fe 1.247, Loss_kd 2.671, Train_accy 67.09, Test_accy 67.28
2023-12-09 14:12:18,253 [foster.py] => Task 3, Epoch 71/170 => Loss 5.082, Loss_clf 0.814, Loss_fe 1.226, Loss_kd 2.662, Train_accy 67.12
2023-12-09 14:12:24,116 [foster.py] => Task 3, Epoch 72/170 => Loss 5.092, Loss_clf 0.814, Loss_fe 1.226, Loss_kd 2.671, Train_accy 67.88, Test_accy 68.25
2023-12-09 14:12:29,961 [foster.py] => Task 3, Epoch 73/170 => Loss 5.023, Loss_clf 0.790, Loss_fe 1.196, Loss_kd 2.658, Train_accy 68.55, Test_accy 68.20
2023-12-09 14:12:36,014 [foster.py] => Task 3, Epoch 74/170 => Loss 5.013, Loss_clf 0.791, Loss_fe 1.173, Loss_kd 2.668, Train_accy 68.25, Test_accy 67.91
2023-12-09 14:12:41,921 [foster.py] => Task 3, Epoch 75/170 => Loss 5.025, Loss_clf 0.800, Loss_fe 1.178, Loss_kd 2.666, Train_accy 67.75, Test_accy 66.12
2023-12-09 14:12:45,345 [foster.py] => Task 3, Epoch 76/170 => Loss 5.040, Loss_clf 0.809, Loss_fe 1.182, Loss_kd 2.669, Train_accy 68.72
2023-12-09 14:12:51,297 [foster.py] => Task 3, Epoch 77/170 => Loss 5.015, Loss_clf 0.783, Loss_fe 1.183, Loss_kd 2.667, Train_accy 68.98, Test_accy 67.75
2023-12-09 14:12:57,195 [foster.py] => Task 3, Epoch 78/170 => Loss 5.032, Loss_clf 0.825, Loss_fe 1.175, Loss_kd 2.652, Train_accy 68.11, Test_accy 67.58
2023-12-09 14:13:03,185 [foster.py] => Task 3, Epoch 79/170 => Loss 5.076, Loss_clf 0.819, Loss_fe 1.200, Loss_kd 2.674, Train_accy 68.08, Test_accy 67.91
2023-12-09 14:13:09,019 [foster.py] => Task 3, Epoch 80/170 => Loss 4.989, Loss_clf 0.776, Loss_fe 1.157, Loss_kd 2.674, Train_accy 68.67, Test_accy 67.19
2023-12-09 14:13:12,450 [foster.py] => Task 3, Epoch 81/170 => Loss 4.963, Loss_clf 0.760, Loss_fe 1.156, Loss_kd 2.666, Train_accy 69.91
2023-12-09 14:13:18,383 [foster.py] => Task 3, Epoch 82/170 => Loss 4.964, Loss_clf 0.769, Loss_fe 1.154, Loss_kd 2.661, Train_accy 69.38, Test_accy 67.66
2023-12-09 14:13:24,230 [foster.py] => Task 3, Epoch 83/170 => Loss 4.981, Loss_clf 0.804, Loss_fe 1.130, Loss_kd 2.666, Train_accy 69.48, Test_accy 68.34
2023-12-09 14:13:30,165 [foster.py] => Task 3, Epoch 84/170 => Loss 4.944, Loss_clf 0.749, Loss_fe 1.142, Loss_kd 2.671, Train_accy 70.12, Test_accy 68.00
2023-12-09 14:13:36,071 [foster.py] => Task 3, Epoch 85/170 => Loss 4.877, Loss_clf 0.735, Loss_fe 1.108, Loss_kd 2.655, Train_accy 70.16, Test_accy 67.14
2023-12-09 14:13:39,453 [foster.py] => Task 3, Epoch 86/170 => Loss 4.904, Loss_clf 0.745, Loss_fe 1.108, Loss_kd 2.670, Train_accy 69.89
2023-12-09 14:13:45,341 [foster.py] => Task 3, Epoch 87/170 => Loss 4.884, Loss_clf 0.741, Loss_fe 1.109, Loss_kd 2.655, Train_accy 70.25, Test_accy 68.32
2023-12-09 14:13:51,175 [foster.py] => Task 3, Epoch 88/170 => Loss 4.861, Loss_clf 0.730, Loss_fe 1.087, Loss_kd 2.664, Train_accy 69.59, Test_accy 68.58
2023-12-09 14:13:57,209 [foster.py] => Task 3, Epoch 89/170 => Loss 4.815, Loss_clf 0.715, Loss_fe 1.064, Loss_kd 2.657, Train_accy 70.89, Test_accy 68.26
2023-12-09 14:14:03,047 [foster.py] => Task 3, Epoch 90/170 => Loss 4.876, Loss_clf 0.731, Loss_fe 1.093, Loss_kd 2.670, Train_accy 70.39, Test_accy 68.74
2023-12-09 14:14:06,490 [foster.py] => Task 3, Epoch 91/170 => Loss 4.809, Loss_clf 0.710, Loss_fe 1.050, Loss_kd 2.668, Train_accy 70.36
2023-12-09 14:14:12,321 [foster.py] => Task 3, Epoch 92/170 => Loss 4.844, Loss_clf 0.722, Loss_fe 1.070, Loss_kd 2.670, Train_accy 70.66, Test_accy 68.90
2023-12-09 14:14:18,138 [foster.py] => Task 3, Epoch 93/170 => Loss 4.809, Loss_clf 0.729, Loss_fe 1.038, Loss_kd 2.661, Train_accy 70.84, Test_accy 67.99
2023-12-09 14:14:24,175 [foster.py] => Task 3, Epoch 94/170 => Loss 4.790, Loss_clf 0.712, Loss_fe 1.038, Loss_kd 2.660, Train_accy 71.78, Test_accy 67.47
2023-12-09 14:14:30,057 [foster.py] => Task 3, Epoch 95/170 => Loss 4.814, Loss_clf 0.700, Loss_fe 1.054, Loss_kd 2.677, Train_accy 71.48, Test_accy 68.96
2023-12-09 14:14:33,512 [foster.py] => Task 3, Epoch 96/170 => Loss 4.766, Loss_clf 0.705, Loss_fe 1.024, Loss_kd 2.657, Train_accy 71.62
2023-12-09 14:14:39,572 [foster.py] => Task 3, Epoch 97/170 => Loss 4.797, Loss_clf 0.714, Loss_fe 1.039, Loss_kd 2.664, Train_accy 71.09, Test_accy 67.18
2023-12-09 14:14:45,461 [foster.py] => Task 3, Epoch 98/170 => Loss 4.742, Loss_clf 0.691, Loss_fe 1.021, Loss_kd 2.651, Train_accy 71.77, Test_accy 68.69
2023-12-09 14:14:51,341 [foster.py] => Task 3, Epoch 99/170 => Loss 4.811, Loss_clf 0.725, Loss_fe 1.030, Loss_kd 2.673, Train_accy 71.12, Test_accy 68.39
2023-12-09 14:14:57,338 [foster.py] => Task 3, Epoch 100/170 => Loss 4.714, Loss_clf 0.682, Loss_fe 0.994, Loss_kd 2.658, Train_accy 72.17, Test_accy 67.36
2023-12-09 14:15:00,855 [foster.py] => Task 3, Epoch 101/170 => Loss 4.747, Loss_clf 0.696, Loss_fe 1.003, Loss_kd 2.667, Train_accy 71.14
2023-12-09 14:15:06,806 [foster.py] => Task 3, Epoch 102/170 => Loss 4.706, Loss_clf 0.672, Loss_fe 0.988, Loss_kd 2.665, Train_accy 72.19, Test_accy 69.16
2023-12-09 14:15:12,708 [foster.py] => Task 3, Epoch 103/170 => Loss 4.655, Loss_clf 0.654, Loss_fe 0.969, Loss_kd 2.653, Train_accy 73.33, Test_accy 69.21
2023-12-09 14:15:18,995 [foster.py] => Task 3, Epoch 104/170 => Loss 4.617, Loss_clf 0.634, Loss_fe 0.941, Loss_kd 2.662, Train_accy 73.39, Test_accy 68.26
2023-12-09 14:15:25,108 [foster.py] => Task 3, Epoch 105/170 => Loss 4.723, Loss_clf 0.685, Loss_fe 0.991, Loss_kd 2.666, Train_accy 71.95, Test_accy 69.01
2023-12-09 14:15:28,632 [foster.py] => Task 3, Epoch 106/170 => Loss 4.616, Loss_clf 0.638, Loss_fe 0.941, Loss_kd 2.657, Train_accy 73.58
2023-12-09 14:15:34,926 [foster.py] => Task 3, Epoch 107/170 => Loss 4.583, Loss_clf 0.633, Loss_fe 0.917, Loss_kd 2.654, Train_accy 73.69, Test_accy 69.36
2023-12-09 14:15:41,241 [foster.py] => Task 3, Epoch 108/170 => Loss 4.616, Loss_clf 0.644, Loss_fe 0.932, Loss_kd 2.660, Train_accy 73.98, Test_accy 68.28
2023-12-09 14:15:47,676 [foster.py] => Task 3, Epoch 109/170 => Loss 4.582, Loss_clf 0.632, Loss_fe 0.918, Loss_kd 2.654, Train_accy 73.73, Test_accy 68.61
2023-12-09 14:15:54,540 [foster.py] => Task 3, Epoch 110/170 => Loss 4.654, Loss_clf 0.662, Loss_fe 0.943, Loss_kd 2.668, Train_accy 73.80, Test_accy 68.97
2023-12-09 14:15:58,336 [foster.py] => Task 3, Epoch 111/170 => Loss 4.589, Loss_clf 0.627, Loss_fe 0.926, Loss_kd 2.657, Train_accy 73.69
2023-12-09 14:16:04,549 [foster.py] => Task 3, Epoch 112/170 => Loss 4.537, Loss_clf 0.615, Loss_fe 0.879, Loss_kd 2.663, Train_accy 73.92, Test_accy 68.94
2023-12-09 14:16:10,758 [foster.py] => Task 3, Epoch 113/170 => Loss 4.627, Loss_clf 0.653, Loss_fe 0.921, Loss_kd 2.671, Train_accy 73.98, Test_accy 68.49
2023-12-09 14:16:16,852 [foster.py] => Task 3, Epoch 114/170 => Loss 4.536, Loss_clf 0.613, Loss_fe 0.879, Loss_kd 2.663, Train_accy 74.53, Test_accy 69.70
2023-12-09 14:16:23,111 [foster.py] => Task 3, Epoch 115/170 => Loss 4.518, Loss_clf 0.605, Loss_fe 0.873, Loss_kd 2.660, Train_accy 74.98, Test_accy 68.58
2023-12-09 14:16:26,725 [foster.py] => Task 3, Epoch 116/170 => Loss 4.522, Loss_clf 0.599, Loss_fe 0.879, Loss_kd 2.663, Train_accy 75.02
2023-12-09 14:16:32,766 [foster.py] => Task 3, Epoch 117/170 => Loss 4.457, Loss_clf 0.587, Loss_fe 0.835, Loss_kd 2.656, Train_accy 75.11, Test_accy 68.68
2023-12-09 14:16:38,531 [foster.py] => Task 3, Epoch 118/170 => Loss 4.498, Loss_clf 0.601, Loss_fe 0.847, Loss_kd 2.669, Train_accy 75.55, Test_accy 69.08
2023-12-09 14:16:44,594 [foster.py] => Task 3, Epoch 119/170 => Loss 4.462, Loss_clf 0.587, Loss_fe 0.843, Loss_kd 2.654, Train_accy 75.52, Test_accy 69.03
2023-12-09 14:16:50,348 [foster.py] => Task 3, Epoch 120/170 => Loss 4.464, Loss_clf 0.592, Loss_fe 0.835, Loss_kd 2.658, Train_accy 75.39, Test_accy 69.32
2023-12-09 14:16:53,774 [foster.py] => Task 3, Epoch 121/170 => Loss 4.426, Loss_clf 0.571, Loss_fe 0.815, Loss_kd 2.660, Train_accy 75.97
2023-12-09 14:16:59,717 [foster.py] => Task 3, Epoch 122/170 => Loss 4.405, Loss_clf 0.569, Loss_fe 0.803, Loss_kd 2.654, Train_accy 76.62, Test_accy 69.06
2023-12-09 14:17:05,622 [foster.py] => Task 3, Epoch 123/170 => Loss 4.403, Loss_clf 0.564, Loss_fe 0.795, Loss_kd 2.663, Train_accy 77.17, Test_accy 69.24
2023-12-09 14:17:11,416 [foster.py] => Task 3, Epoch 124/170 => Loss 4.374, Loss_clf 0.565, Loss_fe 0.777, Loss_kd 2.654, Train_accy 76.53, Test_accy 69.24
2023-12-09 14:17:17,206 [foster.py] => Task 3, Epoch 125/170 => Loss 4.372, Loss_clf 0.548, Loss_fe 0.781, Loss_kd 2.662, Train_accy 77.66, Test_accy 69.55
2023-12-09 14:17:20,729 [foster.py] => Task 3, Epoch 126/170 => Loss 4.354, Loss_clf 0.550, Loss_fe 0.771, Loss_kd 2.653, Train_accy 76.59
2023-12-09 14:17:26,671 [foster.py] => Task 3, Epoch 127/170 => Loss 4.334, Loss_clf 0.539, Loss_fe 0.762, Loss_kd 2.654, Train_accy 77.34, Test_accy 69.64
2023-12-09 14:17:32,487 [foster.py] => Task 3, Epoch 128/170 => Loss 4.350, Loss_clf 0.551, Loss_fe 0.760, Loss_kd 2.659, Train_accy 77.27, Test_accy 69.35
2023-12-09 14:17:38,342 [foster.py] => Task 3, Epoch 129/170 => Loss 4.339, Loss_clf 0.541, Loss_fe 0.748, Loss_kd 2.669, Train_accy 77.59, Test_accy 69.59
2023-12-09 14:17:44,433 [foster.py] => Task 3, Epoch 130/170 => Loss 4.286, Loss_clf 0.529, Loss_fe 0.727, Loss_kd 2.652, Train_accy 78.36, Test_accy 68.59
2023-12-09 14:17:47,908 [foster.py] => Task 3, Epoch 131/170 => Loss 4.303, Loss_clf 0.530, Loss_fe 0.726, Loss_kd 2.667, Train_accy 78.61
2023-12-09 14:17:54,200 [foster.py] => Task 3, Epoch 132/170 => Loss 4.316, Loss_clf 0.536, Loss_fe 0.733, Loss_kd 2.666, Train_accy 78.03, Test_accy 69.65
2023-12-09 14:18:00,234 [foster.py] => Task 3, Epoch 133/170 => Loss 4.224, Loss_clf 0.493, Loss_fe 0.696, Loss_kd 2.657, Train_accy 79.03, Test_accy 69.71
2023-12-09 14:18:06,305 [foster.py] => Task 3, Epoch 134/170 => Loss 4.244, Loss_clf 0.508, Loss_fe 0.698, Loss_kd 2.658, Train_accy 78.12, Test_accy 70.16
2023-12-09 14:18:12,604 [foster.py] => Task 3, Epoch 135/170 => Loss 4.191, Loss_clf 0.488, Loss_fe 0.669, Loss_kd 2.655, Train_accy 79.33, Test_accy 69.15
2023-12-09 14:18:16,168 [foster.py] => Task 3, Epoch 136/170 => Loss 4.257, Loss_clf 0.511, Loss_fe 0.694, Loss_kd 2.670, Train_accy 78.95
2023-12-09 14:18:22,148 [foster.py] => Task 3, Epoch 137/170 => Loss 4.272, Loss_clf 0.519, Loss_fe 0.704, Loss_kd 2.667, Train_accy 79.17, Test_accy 69.40
2023-12-09 14:18:28,214 [foster.py] => Task 3, Epoch 138/170 => Loss 4.225, Loss_clf 0.506, Loss_fe 0.687, Loss_kd 2.653, Train_accy 79.39, Test_accy 69.92
2023-12-09 14:18:34,227 [foster.py] => Task 3, Epoch 139/170 => Loss 4.200, Loss_clf 0.493, Loss_fe 0.667, Loss_kd 2.659, Train_accy 79.97, Test_accy 69.78
2023-12-09 14:18:40,082 [foster.py] => Task 3, Epoch 140/170 => Loss 4.156, Loss_clf 0.469, Loss_fe 0.652, Loss_kd 2.656, Train_accy 79.72, Test_accy 69.42
2023-12-09 14:18:43,575 [foster.py] => Task 3, Epoch 141/170 => Loss 4.149, Loss_clf 0.465, Loss_fe 0.646, Loss_kd 2.658, Train_accy 80.95
2023-12-09 14:18:49,722 [foster.py] => Task 3, Epoch 142/170 => Loss 4.170, Loss_clf 0.479, Loss_fe 0.643, Loss_kd 2.667, Train_accy 80.88, Test_accy 69.90
2023-12-09 14:18:56,197 [foster.py] => Task 3, Epoch 143/170 => Loss 4.118, Loss_clf 0.455, Loss_fe 0.625, Loss_kd 2.658, Train_accy 81.12, Test_accy 69.62
2023-12-09 14:19:02,366 [foster.py] => Task 3, Epoch 144/170 => Loss 4.184, Loss_clf 0.485, Loss_fe 0.651, Loss_kd 2.667, Train_accy 79.81, Test_accy 69.78
2023-12-09 14:19:08,636 [foster.py] => Task 3, Epoch 145/170 => Loss 4.163, Loss_clf 0.480, Loss_fe 0.634, Loss_kd 2.667, Train_accy 79.94, Test_accy 69.44
2023-12-09 14:19:12,178 [foster.py] => Task 3, Epoch 146/170 => Loss 4.149, Loss_clf 0.474, Loss_fe 0.629, Loss_kd 2.665, Train_accy 80.59
2023-12-09 14:19:18,046 [foster.py] => Task 3, Epoch 147/170 => Loss 4.079, Loss_clf 0.450, Loss_fe 0.606, Loss_kd 2.646, Train_accy 80.53, Test_accy 69.91
2023-12-09 14:19:24,282 [foster.py] => Task 3, Epoch 148/170 => Loss 4.109, Loss_clf 0.451, Loss_fe 0.614, Loss_kd 2.663, Train_accy 81.66, Test_accy 70.15
2023-12-09 14:19:30,745 [foster.py] => Task 3, Epoch 149/170 => Loss 4.095, Loss_clf 0.453, Loss_fe 0.601, Loss_kd 2.661, Train_accy 81.22, Test_accy 69.82
2023-12-09 14:19:37,260 [foster.py] => Task 3, Epoch 150/170 => Loss 4.081, Loss_clf 0.441, Loss_fe 0.595, Loss_kd 2.664, Train_accy 82.81, Test_accy 70.06
2023-12-09 14:19:40,987 [foster.py] => Task 3, Epoch 151/170 => Loss 4.124, Loss_clf 0.461, Loss_fe 0.606, Loss_kd 2.675, Train_accy 80.94
2023-12-09 14:19:47,146 [foster.py] => Task 3, Epoch 152/170 => Loss 4.088, Loss_clf 0.450, Loss_fe 0.590, Loss_kd 2.667, Train_accy 81.30, Test_accy 69.96
2023-12-09 14:19:53,165 [foster.py] => Task 3, Epoch 153/170 => Loss 4.055, Loss_clf 0.434, Loss_fe 0.576, Loss_kd 2.664, Train_accy 81.89, Test_accy 70.20
2023-12-09 14:19:59,138 [foster.py] => Task 3, Epoch 154/170 => Loss 4.099, Loss_clf 0.450, Loss_fe 0.594, Loss_kd 2.673, Train_accy 81.52, Test_accy 69.82
2023-12-09 14:20:05,074 [foster.py] => Task 3, Epoch 155/170 => Loss 4.057, Loss_clf 0.427, Loss_fe 0.575, Loss_kd 2.673, Train_accy 82.53, Test_accy 70.11
2023-12-09 14:20:08,647 [foster.py] => Task 3, Epoch 156/170 => Loss 4.098, Loss_clf 0.455, Loss_fe 0.593, Loss_kd 2.669, Train_accy 81.95
2023-12-09 14:20:15,045 [foster.py] => Task 3, Epoch 157/170 => Loss 4.052, Loss_clf 0.435, Loss_fe 0.570, Loss_kd 2.667, Train_accy 82.14, Test_accy 70.20
2023-12-09 14:20:21,598 [foster.py] => Task 3, Epoch 158/170 => Loss 4.024, Loss_clf 0.427, Loss_fe 0.559, Loss_kd 2.658, Train_accy 82.33, Test_accy 70.21
2023-12-09 14:20:27,637 [foster.py] => Task 3, Epoch 159/170 => Loss 4.059, Loss_clf 0.437, Loss_fe 0.573, Loss_kd 2.667, Train_accy 82.17, Test_accy 70.09
2023-12-09 14:20:33,784 [foster.py] => Task 3, Epoch 160/170 => Loss 4.019, Loss_clf 0.421, Loss_fe 0.561, Loss_kd 2.658, Train_accy 82.59, Test_accy 70.18
2023-12-09 14:20:37,368 [foster.py] => Task 3, Epoch 161/170 => Loss 4.031, Loss_clf 0.425, Loss_fe 0.563, Loss_kd 2.663, Train_accy 82.66
2023-12-09 14:20:43,317 [foster.py] => Task 3, Epoch 162/170 => Loss 4.053, Loss_clf 0.433, Loss_fe 0.571, Loss_kd 2.668, Train_accy 82.28, Test_accy 70.04
2023-12-09 14:20:49,451 [foster.py] => Task 3, Epoch 163/170 => Loss 4.029, Loss_clf 0.424, Loss_fe 0.570, Loss_kd 2.655, Train_accy 82.31, Test_accy 70.10
2023-12-09 14:20:55,578 [foster.py] => Task 3, Epoch 164/170 => Loss 4.024, Loss_clf 0.421, Loss_fe 0.556, Loss_kd 2.666, Train_accy 82.38, Test_accy 70.14
2023-12-09 14:21:02,093 [foster.py] => Task 3, Epoch 165/170 => Loss 4.041, Loss_clf 0.432, Loss_fe 0.571, Loss_kd 2.659, Train_accy 82.00, Test_accy 70.06
2023-12-09 14:21:06,020 [foster.py] => Task 3, Epoch 166/170 => Loss 4.003, Loss_clf 0.421, Loss_fe 0.550, Loss_kd 2.653, Train_accy 82.16
2023-12-09 14:21:12,513 [foster.py] => Task 3, Epoch 167/170 => Loss 4.024, Loss_clf 0.422, Loss_fe 0.559, Loss_kd 2.663, Train_accy 82.55, Test_accy 70.09
2023-12-09 14:21:19,133 [foster.py] => Task 3, Epoch 168/170 => Loss 3.991, Loss_clf 0.408, Loss_fe 0.540, Loss_kd 2.663, Train_accy 83.11, Test_accy 70.06
2023-12-09 14:21:25,652 [foster.py] => Task 3, Epoch 169/170 => Loss 4.007, Loss_clf 0.423, Loss_fe 0.552, Loss_kd 2.653, Train_accy 82.67, Test_accy 70.20
2023-12-09 14:21:32,418 [foster.py] => Task 3, Epoch 170/170 => Loss 4.031, Loss_clf 0.428, Loss_fe 0.560, Loss_kd 2.663, Train_accy 82.83, Test_accy 70.15
2023-12-09 14:21:32,420 [foster.py] => do not weight align teacher!
2023-12-09 14:21:32,421 [foster.py] => per cls weights : [1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176 1.07293176
 1.07293176 1.07293176 1.07293176 1.07293176 0.48947766 0.48947766
 0.48947766 0.48947766 0.48947766 0.48947766 0.48947766 0.48947766
 0.48947766 0.48947766]
2023-12-09 14:21:38,652 [foster.py] => SNet: Task 3, Epoch 1/130 => Loss 3.198,  Train_accy 39.03, Test_accy 61.66
2023-12-09 14:21:42,780 [foster.py] => SNet: Task 3, Epoch 2/130 => Loss 2.970,  Train_accy 56.52
2023-12-09 14:21:46,965 [foster.py] => SNet: Task 3, Epoch 3/130 => Loss 2.936,  Train_accy 57.27
2023-12-09 14:21:51,277 [foster.py] => SNet: Task 3, Epoch 4/130 => Loss 2.947,  Train_accy 58.00
2023-12-09 14:21:55,715 [foster.py] => SNet: Task 3, Epoch 5/130 => Loss 2.922,  Train_accy 59.42
2023-12-09 14:22:02,188 [foster.py] => SNet: Task 3, Epoch 6/130 => Loss 2.923,  Train_accy 59.91, Test_accy 64.76
2023-12-09 14:22:06,646 [foster.py] => SNet: Task 3, Epoch 7/130 => Loss 2.898,  Train_accy 60.61
2023-12-09 14:22:10,866 [foster.py] => SNet: Task 3, Epoch 8/130 => Loss 2.899,  Train_accy 60.44
2023-12-09 14:22:14,955 [foster.py] => SNet: Task 3, Epoch 9/130 => Loss 2.907,  Train_accy 60.53
2023-12-09 14:22:19,288 [foster.py] => SNet: Task 3, Epoch 10/130 => Loss 2.900,  Train_accy 61.88
2023-12-09 14:22:25,859 [foster.py] => SNet: Task 3, Epoch 11/130 => Loss 2.907,  Train_accy 60.59, Test_accy 66.04
2023-12-09 14:22:30,154 [foster.py] => SNet: Task 3, Epoch 12/130 => Loss 2.902,  Train_accy 60.84
2023-12-09 14:22:34,464 [foster.py] => SNet: Task 3, Epoch 13/130 => Loss 2.889,  Train_accy 62.11
2023-12-09 14:22:38,534 [foster.py] => SNet: Task 3, Epoch 14/130 => Loss 2.897,  Train_accy 61.38
2023-12-09 14:22:42,666 [foster.py] => SNet: Task 3, Epoch 15/130 => Loss 2.902,  Train_accy 61.48
2023-12-09 14:22:48,947 [foster.py] => SNet: Task 3, Epoch 16/130 => Loss 2.877,  Train_accy 62.92, Test_accy 66.61
2023-12-09 14:22:53,205 [foster.py] => SNet: Task 3, Epoch 17/130 => Loss 2.903,  Train_accy 61.75
2023-12-09 14:22:57,224 [foster.py] => SNet: Task 3, Epoch 18/130 => Loss 2.886,  Train_accy 62.00
2023-12-09 14:23:01,287 [foster.py] => SNet: Task 3, Epoch 19/130 => Loss 2.881,  Train_accy 63.00
2023-12-09 14:23:05,708 [foster.py] => SNet: Task 3, Epoch 20/130 => Loss 2.886,  Train_accy 63.30
2023-12-09 14:23:12,244 [foster.py] => SNet: Task 3, Epoch 21/130 => Loss 2.887,  Train_accy 62.33, Test_accy 66.31
2023-12-09 14:23:16,587 [foster.py] => SNet: Task 3, Epoch 22/130 => Loss 2.872,  Train_accy 62.98
2023-12-09 14:23:20,922 [foster.py] => SNet: Task 3, Epoch 23/130 => Loss 2.882,  Train_accy 63.25
2023-12-09 14:23:25,036 [foster.py] => SNet: Task 3, Epoch 24/130 => Loss 2.872,  Train_accy 63.61
2023-12-09 14:23:28,932 [foster.py] => SNet: Task 3, Epoch 25/130 => Loss 2.883,  Train_accy 63.08
2023-12-09 14:23:35,255 [foster.py] => SNet: Task 3, Epoch 26/130 => Loss 2.874,  Train_accy 63.45, Test_accy 67.39
2023-12-09 14:23:39,204 [foster.py] => SNet: Task 3, Epoch 27/130 => Loss 2.854,  Train_accy 63.70
2023-12-09 14:23:43,273 [foster.py] => SNet: Task 3, Epoch 28/130 => Loss 2.862,  Train_accy 63.41
2023-12-09 14:23:47,387 [foster.py] => SNet: Task 3, Epoch 29/130 => Loss 2.883,  Train_accy 63.06
2023-12-09 14:23:51,766 [foster.py] => SNet: Task 3, Epoch 30/130 => Loss 2.878,  Train_accy 62.72
2023-12-09 14:23:58,619 [foster.py] => SNet: Task 3, Epoch 31/130 => Loss 2.862,  Train_accy 64.14, Test_accy 66.99
2023-12-09 14:24:02,991 [foster.py] => SNet: Task 3, Epoch 32/130 => Loss 2.882,  Train_accy 63.38
2023-12-09 14:24:06,944 [foster.py] => SNet: Task 3, Epoch 33/130 => Loss 2.866,  Train_accy 64.20
2023-12-09 14:24:11,135 [foster.py] => SNet: Task 3, Epoch 34/130 => Loss 2.882,  Train_accy 63.55
2023-12-09 14:24:15,579 [foster.py] => SNet: Task 3, Epoch 35/130 => Loss 2.880,  Train_accy 62.92
2023-12-09 14:24:21,782 [foster.py] => SNet: Task 3, Epoch 36/130 => Loss 2.876,  Train_accy 63.89, Test_accy 67.76
2023-12-09 14:24:25,812 [foster.py] => SNet: Task 3, Epoch 37/130 => Loss 2.873,  Train_accy 63.41
2023-12-09 14:24:30,284 [foster.py] => SNet: Task 3, Epoch 38/130 => Loss 2.878,  Train_accy 64.23
2023-12-09 14:24:34,608 [foster.py] => SNet: Task 3, Epoch 39/130 => Loss 2.875,  Train_accy 63.94
2023-12-09 14:24:38,715 [foster.py] => SNet: Task 3, Epoch 40/130 => Loss 2.878,  Train_accy 64.28
2023-12-09 14:24:44,914 [foster.py] => SNet: Task 3, Epoch 41/130 => Loss 2.875,  Train_accy 64.67, Test_accy 67.03
2023-12-09 14:24:49,077 [foster.py] => SNet: Task 3, Epoch 42/130 => Loss 2.858,  Train_accy 64.30
2023-12-09 14:24:53,161 [foster.py] => SNet: Task 3, Epoch 43/130 => Loss 2.863,  Train_accy 64.75
2023-12-09 14:24:57,519 [foster.py] => SNet: Task 3, Epoch 44/130 => Loss 2.876,  Train_accy 64.73
2023-12-09 14:25:01,816 [foster.py] => SNet: Task 3, Epoch 45/130 => Loss 2.875,  Train_accy 64.41
2023-12-09 14:25:08,362 [foster.py] => SNet: Task 3, Epoch 46/130 => Loss 2.878,  Train_accy 64.38, Test_accy 67.64
2023-12-09 14:25:12,466 [foster.py] => SNet: Task 3, Epoch 47/130 => Loss 2.866,  Train_accy 65.44
2023-12-09 14:25:16,415 [foster.py] => SNet: Task 3, Epoch 48/130 => Loss 2.877,  Train_accy 64.42
2023-12-09 14:25:20,586 [foster.py] => SNet: Task 3, Epoch 49/130 => Loss 2.855,  Train_accy 64.48
2023-12-09 14:25:24,907 [foster.py] => SNet: Task 3, Epoch 50/130 => Loss 2.843,  Train_accy 65.39
2023-12-09 14:25:31,513 [foster.py] => SNet: Task 3, Epoch 51/130 => Loss 2.868,  Train_accy 64.77, Test_accy 67.74
2023-12-09 14:25:35,818 [foster.py] => SNet: Task 3, Epoch 52/130 => Loss 2.877,  Train_accy 64.22
2023-12-09 14:25:40,024 [foster.py] => SNet: Task 3, Epoch 53/130 => Loss 2.855,  Train_accy 64.34
2023-12-09 14:25:43,926 [foster.py] => SNet: Task 3, Epoch 54/130 => Loss 2.881,  Train_accy 64.28
2023-12-09 14:25:47,956 [foster.py] => SNet: Task 3, Epoch 55/130 => Loss 2.868,  Train_accy 65.14
2023-12-09 14:25:54,267 [foster.py] => SNet: Task 3, Epoch 56/130 => Loss 2.864,  Train_accy 65.11, Test_accy 67.75
2023-12-09 14:25:58,651 [foster.py] => SNet: Task 3, Epoch 57/130 => Loss 2.863,  Train_accy 65.08
2023-12-09 14:26:02,821 [foster.py] => SNet: Task 3, Epoch 58/130 => Loss 2.869,  Train_accy 65.47
2023-12-09 14:26:06,798 [foster.py] => SNet: Task 3, Epoch 59/130 => Loss 2.859,  Train_accy 65.05
2023-12-09 14:26:11,086 [foster.py] => SNet: Task 3, Epoch 60/130 => Loss 2.850,  Train_accy 65.44
2023-12-09 14:26:17,751 [foster.py] => SNet: Task 3, Epoch 61/130 => Loss 2.865,  Train_accy 65.45, Test_accy 67.94
2023-12-09 14:26:22,269 [foster.py] => SNet: Task 3, Epoch 62/130 => Loss 2.863,  Train_accy 65.23
2023-12-09 14:26:26,525 [foster.py] => SNet: Task 3, Epoch 63/130 => Loss 2.847,  Train_accy 65.23
2023-12-09 14:26:30,641 [foster.py] => SNet: Task 3, Epoch 64/130 => Loss 2.853,  Train_accy 65.23
2023-12-09 14:26:34,828 [foster.py] => SNet: Task 3, Epoch 65/130 => Loss 2.859,  Train_accy 65.62
2023-12-09 14:26:41,159 [foster.py] => SNet: Task 3, Epoch 66/130 => Loss 2.848,  Train_accy 66.22, Test_accy 68.79
2023-12-09 14:26:45,259 [foster.py] => SNet: Task 3, Epoch 67/130 => Loss 2.881,  Train_accy 64.80
2023-12-09 14:26:49,332 [foster.py] => SNet: Task 3, Epoch 68/130 => Loss 2.864,  Train_accy 65.78
2023-12-09 14:26:53,609 [foster.py] => SNet: Task 3, Epoch 69/130 => Loss 2.851,  Train_accy 65.61
2023-12-09 14:26:57,939 [foster.py] => SNet: Task 3, Epoch 70/130 => Loss 2.839,  Train_accy 65.41
2023-12-09 14:27:04,348 [foster.py] => SNet: Task 3, Epoch 71/130 => Loss 2.834,  Train_accy 65.44, Test_accy 68.39
2023-12-09 14:27:08,598 [foster.py] => SNet: Task 3, Epoch 72/130 => Loss 2.857,  Train_accy 65.98
2023-12-09 14:27:12,891 [foster.py] => SNet: Task 3, Epoch 73/130 => Loss 2.871,  Train_accy 64.59
2023-12-09 14:27:17,121 [foster.py] => SNet: Task 3, Epoch 74/130 => Loss 2.874,  Train_accy 65.94
2023-12-09 14:27:21,545 [foster.py] => SNet: Task 3, Epoch 75/130 => Loss 2.864,  Train_accy 65.80
2023-12-09 14:27:28,208 [foster.py] => SNet: Task 3, Epoch 76/130 => Loss 2.865,  Train_accy 65.92, Test_accy 68.42
2023-12-09 14:27:32,518 [foster.py] => SNet: Task 3, Epoch 77/130 => Loss 2.872,  Train_accy 64.80
2023-12-09 14:27:36,897 [foster.py] => SNet: Task 3, Epoch 78/130 => Loss 2.860,  Train_accy 65.30
2023-12-09 14:27:41,038 [foster.py] => SNet: Task 3, Epoch 79/130 => Loss 2.842,  Train_accy 66.66
2023-12-09 14:27:44,844 [foster.py] => SNet: Task 3, Epoch 80/130 => Loss 2.835,  Train_accy 66.06
2023-12-09 14:27:51,231 [foster.py] => SNet: Task 3, Epoch 81/130 => Loss 2.842,  Train_accy 66.48, Test_accy 68.59
2023-12-09 14:27:55,509 [foster.py] => SNet: Task 3, Epoch 82/130 => Loss 2.849,  Train_accy 65.91
2023-12-09 14:27:59,937 [foster.py] => SNet: Task 3, Epoch 83/130 => Loss 2.856,  Train_accy 65.75
2023-12-09 14:28:04,279 [foster.py] => SNet: Task 3, Epoch 84/130 => Loss 2.854,  Train_accy 66.42
2023-12-09 14:28:08,508 [foster.py] => SNet: Task 3, Epoch 85/130 => Loss 2.861,  Train_accy 65.67
2023-12-09 14:28:15,132 [foster.py] => SNet: Task 3, Epoch 86/130 => Loss 2.844,  Train_accy 66.78, Test_accy 68.45
2023-12-09 14:28:19,212 [foster.py] => SNet: Task 3, Epoch 87/130 => Loss 2.855,  Train_accy 65.70
2023-12-09 14:28:23,625 [foster.py] => SNet: Task 3, Epoch 88/130 => Loss 2.851,  Train_accy 66.42
2023-12-09 14:28:27,978 [foster.py] => SNet: Task 3, Epoch 89/130 => Loss 2.843,  Train_accy 66.36
2023-12-09 14:28:32,364 [foster.py] => SNet: Task 3, Epoch 90/130 => Loss 2.851,  Train_accy 65.80
2023-12-09 14:28:38,804 [foster.py] => SNet: Task 3, Epoch 91/130 => Loss 2.858,  Train_accy 66.70, Test_accy 68.61
2023-12-09 14:28:42,981 [foster.py] => SNet: Task 3, Epoch 92/130 => Loss 2.848,  Train_accy 66.17
2023-12-09 14:28:47,128 [foster.py] => SNet: Task 3, Epoch 93/130 => Loss 2.847,  Train_accy 66.28
2023-12-09 14:28:51,371 [foster.py] => SNet: Task 3, Epoch 94/130 => Loss 2.857,  Train_accy 65.91
2023-12-09 14:28:55,523 [foster.py] => SNet: Task 3, Epoch 95/130 => Loss 2.859,  Train_accy 65.91
2023-12-09 14:29:02,290 [foster.py] => SNet: Task 3, Epoch 96/130 => Loss 2.845,  Train_accy 66.52, Test_accy 68.78
2023-12-09 14:29:06,585 [foster.py] => SNet: Task 3, Epoch 97/130 => Loss 2.858,  Train_accy 65.86
2023-12-09 14:29:10,840 [foster.py] => SNet: Task 3, Epoch 98/130 => Loss 2.847,  Train_accy 65.69
2023-12-09 14:29:15,178 [foster.py] => SNet: Task 3, Epoch 99/130 => Loss 2.857,  Train_accy 66.20
2023-12-09 14:29:19,593 [foster.py] => SNet: Task 3, Epoch 100/130 => Loss 2.836,  Train_accy 66.78
2023-12-09 14:29:26,227 [foster.py] => SNet: Task 3, Epoch 101/130 => Loss 2.842,  Train_accy 67.20, Test_accy 69.29
2023-12-09 14:29:30,679 [foster.py] => SNet: Task 3, Epoch 102/130 => Loss 2.853,  Train_accy 65.83
2023-12-09 14:29:34,794 [foster.py] => SNet: Task 3, Epoch 103/130 => Loss 2.851,  Train_accy 66.50
2023-12-09 14:29:39,153 [foster.py] => SNet: Task 3, Epoch 104/130 => Loss 2.858,  Train_accy 65.20
2023-12-09 14:29:43,264 [foster.py] => SNet: Task 3, Epoch 105/130 => Loss 2.853,  Train_accy 65.72
2023-12-09 14:29:49,592 [foster.py] => SNet: Task 3, Epoch 106/130 => Loss 2.842,  Train_accy 66.38, Test_accy 68.89
2023-12-09 14:29:53,875 [foster.py] => SNet: Task 3, Epoch 107/130 => Loss 2.838,  Train_accy 66.61
2023-12-09 14:29:58,066 [foster.py] => SNet: Task 3, Epoch 108/130 => Loss 2.855,  Train_accy 66.12
2023-12-09 14:30:02,207 [foster.py] => SNet: Task 3, Epoch 109/130 => Loss 2.851,  Train_accy 65.77
2023-12-09 14:30:06,280 [foster.py] => SNet: Task 3, Epoch 110/130 => Loss 2.835,  Train_accy 66.80
2023-12-09 14:30:12,963 [foster.py] => SNet: Task 3, Epoch 111/130 => Loss 2.859,  Train_accy 66.69, Test_accy 68.85
2023-12-09 14:30:17,328 [foster.py] => SNet: Task 3, Epoch 112/130 => Loss 2.840,  Train_accy 66.23
2023-12-09 14:30:21,589 [foster.py] => SNet: Task 3, Epoch 113/130 => Loss 2.846,  Train_accy 65.83
2023-12-09 14:30:25,700 [foster.py] => SNet: Task 3, Epoch 114/130 => Loss 2.847,  Train_accy 66.69
2023-12-09 14:30:29,991 [foster.py] => SNet: Task 3, Epoch 115/130 => Loss 2.848,  Train_accy 65.73
2023-12-09 14:30:36,339 [foster.py] => SNet: Task 3, Epoch 116/130 => Loss 2.850,  Train_accy 65.31, Test_accy 68.81
2023-12-09 14:30:40,507 [foster.py] => SNet: Task 3, Epoch 117/130 => Loss 2.840,  Train_accy 65.47
2023-12-09 14:30:44,584 [foster.py] => SNet: Task 3, Epoch 118/130 => Loss 2.836,  Train_accy 66.55
2023-12-09 14:30:48,674 [foster.py] => SNet: Task 3, Epoch 119/130 => Loss 2.835,  Train_accy 66.02
2023-12-09 14:30:52,992 [foster.py] => SNet: Task 3, Epoch 120/130 => Loss 2.828,  Train_accy 67.41
2023-12-09 14:30:59,537 [foster.py] => SNet: Task 3, Epoch 121/130 => Loss 2.837,  Train_accy 67.17, Test_accy 68.84
2023-12-09 14:31:03,952 [foster.py] => SNet: Task 3, Epoch 122/130 => Loss 2.832,  Train_accy 66.52
2023-12-09 14:31:08,029 [foster.py] => SNet: Task 3, Epoch 123/130 => Loss 2.848,  Train_accy 66.66
2023-12-09 14:31:11,906 [foster.py] => SNet: Task 3, Epoch 124/130 => Loss 2.847,  Train_accy 66.41
2023-12-09 14:31:15,868 [foster.py] => SNet: Task 3, Epoch 125/130 => Loss 2.859,  Train_accy 65.48
2023-12-09 14:31:22,264 [foster.py] => SNet: Task 3, Epoch 126/130 => Loss 2.840,  Train_accy 67.11, Test_accy 68.78
2023-12-09 14:31:26,596 [foster.py] => SNet: Task 3, Epoch 127/130 => Loss 2.865,  Train_accy 66.86
2023-12-09 14:31:30,712 [foster.py] => SNet: Task 3, Epoch 128/130 => Loss 2.852,  Train_accy 66.55
2023-12-09 14:31:35,044 [foster.py] => SNet: Task 3, Epoch 129/130 => Loss 2.843,  Train_accy 66.53
2023-12-09 14:31:39,235 [foster.py] => SNet: Task 3, Epoch 130/130 => Loss 2.851,  Train_accy 66.19
2023-12-09 14:31:39,236 [foster.py] => do not weight align student!
2023-12-09 14:31:41,466 [foster.py] => darknet eval: 
2023-12-09 14:31:41,466 [foster.py] => CNN top1 curve: 68.97
2023-12-09 14:31:41,466 [foster.py] => CNN top5 curve: 92.54
2023-12-09 14:31:41,467 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 14:33:01,074 [foster.py] => Exemplar size: 1600
2023-12-09 14:33:01,074 [trainer.py] => CNN: {'total': 70.15, '00-09': 76.1, '10-19': 63.6, '20-29': 75.2, '30-39': 66.1, '40-49': 73.7, '50-59': 58.4, '60-69': 75.2, '70-79': 72.9, 'old': 69.76, 'new': 72.9}
2023-12-09 14:33:01,074 [trainer.py] => NME: {'total': 60.14, '00-09': 66.0, '10-19': 51.8, '20-29': 65.5, '30-39': 56.0, '40-49': 62.1, '50-59': 45.8, '60-69': 58.2, '70-79': 75.7, 'old': 57.91, 'new': 75.7}
2023-12-09 14:33:01,075 [trainer.py] => CNN top1 curve: [80.38, 76.75, 74.74, 70.15]
2023-12-09 14:33:01,075 [trainer.py] => CNN top5 curve: [96.48, 94.95, 94.13, 92.79]
2023-12-09 14:33:01,075 [trainer.py] => NME top1 curve: [78.66, 72.0, 65.59, 60.14]
2023-12-09 14:33:01,075 [trainer.py] => NME top5 curve: [96.2, 93.15, 90.0, 87.55]

2023-12-09 14:33:01,076 [trainer.py] => All params: 948378
2023-12-09 14:33:01,076 [trainer.py] => Trainable params: 479674
2023-12-09 14:33:01,102 [foster.py] => Learning on 80-90
2023-12-09 14:33:01,104 [foster.py] => All params: 950968
2023-12-09 14:33:01,104 [foster.py] => Trainable params: 481614
2023-12-09 14:33:01,211 [foster.py] => per cls weights : [1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767 1.03330767
 1.03330767 1.03330767 0.73353866 0.73353866 0.73353866 0.73353866
 0.73353866 0.73353866 0.73353866 0.73353866 0.73353866 0.73353866]
2023-12-09 14:33:05,236 [foster.py] => Task 4, Epoch 1/170 => Loss 8.694, Loss_clf 2.424, Loss_fe 3.077, Loss_kd 2.838, Train_accy 46.89
2023-12-09 14:33:12,393 [foster.py] => Task 4, Epoch 2/170 => Loss 7.085, Loss_clf 1.316, Loss_fe 2.619, Loss_kd 2.800, Train_accy 53.36, Test_accy 61.50
2023-12-09 14:33:19,243 [foster.py] => Task 4, Epoch 3/170 => Loss 6.756, Loss_clf 1.244, Loss_fe 2.355, Loss_kd 2.805, Train_accy 55.76, Test_accy 62.93
2023-12-09 14:33:25,504 [foster.py] => Task 4, Epoch 4/170 => Loss 6.564, Loss_clf 1.209, Loss_fe 2.201, Loss_kd 2.804, Train_accy 56.56, Test_accy 63.01
2023-12-09 14:33:31,898 [foster.py] => Task 4, Epoch 5/170 => Loss 6.524, Loss_clf 1.233, Loss_fe 2.138, Loss_kd 2.803, Train_accy 56.83, Test_accy 63.57
2023-12-09 14:33:35,487 [foster.py] => Task 4, Epoch 6/170 => Loss 6.557, Loss_clf 1.319, Loss_fe 2.085, Loss_kd 2.803, Train_accy 57.00
2023-12-09 14:33:42,082 [foster.py] => Task 4, Epoch 7/170 => Loss 6.356, Loss_clf 1.187, Loss_fe 2.016, Loss_kd 2.802, Train_accy 58.08, Test_accy 61.66
2023-12-09 14:33:48,621 [foster.py] => Task 4, Epoch 8/170 => Loss 6.463, Loss_clf 1.318, Loss_fe 1.987, Loss_kd 2.808, Train_accy 57.18, Test_accy 61.54
2023-12-09 14:33:55,150 [foster.py] => Task 4, Epoch 9/170 => Loss 6.367, Loss_clf 1.239, Loss_fe 1.966, Loss_kd 2.810, Train_accy 57.91, Test_accy 63.21
2023-12-09 14:34:01,349 [foster.py] => Task 4, Epoch 10/170 => Loss 6.210, Loss_clf 1.148, Loss_fe 1.905, Loss_kd 2.806, Train_accy 58.89, Test_accy 62.99
2023-12-09 14:34:05,119 [foster.py] => Task 4, Epoch 11/170 => Loss 6.412, Loss_clf 1.336, Loss_fe 1.902, Loss_kd 2.822, Train_accy 58.08
2023-12-09 14:34:11,706 [foster.py] => Task 4, Epoch 12/170 => Loss 6.235, Loss_clf 1.168, Loss_fe 1.908, Loss_kd 2.808, Train_accy 59.18, Test_accy 63.42
2023-12-09 14:34:18,022 [foster.py] => Task 4, Epoch 13/170 => Loss 6.209, Loss_clf 1.190, Loss_fe 1.864, Loss_kd 2.804, Train_accy 59.21, Test_accy 62.52
2023-12-09 14:34:24,413 [foster.py] => Task 4, Epoch 14/170 => Loss 6.100, Loss_clf 1.142, Loss_fe 1.815, Loss_kd 2.794, Train_accy 59.91, Test_accy 63.89
2023-12-09 14:34:31,292 [foster.py] => Task 4, Epoch 15/170 => Loss 6.097, Loss_clf 1.138, Loss_fe 1.803, Loss_kd 2.805, Train_accy 60.86, Test_accy 57.12
2023-12-09 14:34:35,284 [foster.py] => Task 4, Epoch 16/170 => Loss 5.997, Loss_clf 1.093, Loss_fe 1.751, Loss_kd 2.803, Train_accy 61.41
2023-12-09 14:34:41,626 [foster.py] => Task 4, Epoch 17/170 => Loss 5.970, Loss_clf 1.062, Loss_fe 1.748, Loss_kd 2.808, Train_accy 60.38, Test_accy 63.01
2023-12-09 14:34:48,264 [foster.py] => Task 4, Epoch 18/170 => Loss 5.961, Loss_clf 1.080, Loss_fe 1.723, Loss_kd 2.807, Train_accy 61.38, Test_accy 63.08
2023-12-09 14:34:55,262 [foster.py] => Task 4, Epoch 19/170 => Loss 6.055, Loss_clf 1.172, Loss_fe 1.718, Loss_kd 2.814, Train_accy 59.77, Test_accy 63.14
2023-12-09 14:35:02,259 [foster.py] => Task 4, Epoch 20/170 => Loss 6.044, Loss_clf 1.146, Loss_fe 1.734, Loss_kd 2.812, Train_accy 60.55, Test_accy 62.99
2023-12-09 14:35:06,401 [foster.py] => Task 4, Epoch 21/170 => Loss 6.017, Loss_clf 1.172, Loss_fe 1.698, Loss_kd 2.797, Train_accy 60.73
2023-12-09 14:35:13,289 [foster.py] => Task 4, Epoch 22/170 => Loss 5.952, Loss_clf 1.094, Loss_fe 1.701, Loss_kd 2.806, Train_accy 62.82, Test_accy 63.66
2023-12-09 14:35:19,867 [foster.py] => Task 4, Epoch 23/170 => Loss 5.969, Loss_clf 1.147, Loss_fe 1.657, Loss_kd 2.813, Train_accy 61.62, Test_accy 63.57
2023-12-09 14:35:26,670 [foster.py] => Task 4, Epoch 24/170 => Loss 5.812, Loss_clf 1.015, Loss_fe 1.642, Loss_kd 2.805, Train_accy 63.14, Test_accy 63.60
2023-12-09 14:35:33,679 [foster.py] => Task 4, Epoch 25/170 => Loss 5.806, Loss_clf 1.017, Loss_fe 1.642, Loss_kd 2.798, Train_accy 63.15, Test_accy 63.16
2023-12-09 14:35:37,757 [foster.py] => Task 4, Epoch 26/170 => Loss 5.925, Loss_clf 1.143, Loss_fe 1.630, Loss_kd 2.802, Train_accy 61.48
2023-12-09 14:35:44,115 [foster.py] => Task 4, Epoch 27/170 => Loss 5.856, Loss_clf 1.059, Loss_fe 1.633, Loss_kd 2.812, Train_accy 61.64, Test_accy 62.00
2023-12-09 14:35:50,923 [foster.py] => Task 4, Epoch 28/170 => Loss 5.908, Loss_clf 1.111, Loss_fe 1.643, Loss_kd 2.804, Train_accy 61.58, Test_accy 62.28
2023-12-09 14:35:57,970 [foster.py] => Task 4, Epoch 29/170 => Loss 5.688, Loss_clf 0.982, Loss_fe 1.564, Loss_kd 2.792, Train_accy 63.77, Test_accy 62.03
2023-12-09 14:36:04,986 [foster.py] => Task 4, Epoch 30/170 => Loss 5.983, Loss_clf 1.192, Loss_fe 1.634, Loss_kd 2.805, Train_accy 61.59, Test_accy 60.27
2023-12-09 14:36:09,101 [foster.py] => Task 4, Epoch 31/170 => Loss 5.910, Loss_clf 1.140, Loss_fe 1.619, Loss_kd 2.800, Train_accy 62.48
2023-12-09 14:36:15,827 [foster.py] => Task 4, Epoch 32/170 => Loss 5.868, Loss_clf 1.090, Loss_fe 1.619, Loss_kd 2.808, Train_accy 62.03, Test_accy 61.23
2023-12-09 14:36:22,596 [foster.py] => Task 4, Epoch 33/170 => Loss 5.924, Loss_clf 1.179, Loss_fe 1.587, Loss_kd 2.807, Train_accy 62.77, Test_accy 64.80
2023-12-09 14:36:29,242 [foster.py] => Task 4, Epoch 34/170 => Loss 5.939, Loss_clf 1.161, Loss_fe 1.615, Loss_kd 2.812, Train_accy 62.73, Test_accy 63.68
2023-12-09 14:36:35,915 [foster.py] => Task 4, Epoch 35/170 => Loss 5.730, Loss_clf 1.012, Loss_fe 1.565, Loss_kd 2.803, Train_accy 64.08, Test_accy 63.56
2023-12-09 14:36:39,482 [foster.py] => Task 4, Epoch 36/170 => Loss 5.683, Loss_clf 0.997, Loss_fe 1.541, Loss_kd 2.796, Train_accy 63.30
2023-12-09 14:36:46,149 [foster.py] => Task 4, Epoch 37/170 => Loss 5.714, Loss_clf 1.009, Loss_fe 1.556, Loss_kd 2.799, Train_accy 64.32, Test_accy 62.59
2023-12-09 14:36:52,734 [foster.py] => Task 4, Epoch 38/170 => Loss 5.607, Loss_clf 0.963, Loss_fe 1.514, Loss_kd 2.782, Train_accy 64.48, Test_accy 64.29
2023-12-09 14:36:59,724 [foster.py] => Task 4, Epoch 39/170 => Loss 5.673, Loss_clf 1.006, Loss_fe 1.513, Loss_kd 2.803, Train_accy 63.89, Test_accy 60.76
2023-12-09 14:37:06,298 [foster.py] => Task 4, Epoch 40/170 => Loss 5.748, Loss_clf 1.089, Loss_fe 1.507, Loss_kd 2.802, Train_accy 64.05, Test_accy 63.26
2023-12-09 14:37:10,222 [foster.py] => Task 4, Epoch 41/170 => Loss 5.682, Loss_clf 0.998, Loss_fe 1.526, Loss_kd 2.807, Train_accy 63.83
2023-12-09 14:37:16,735 [foster.py] => Task 4, Epoch 42/170 => Loss 5.636, Loss_clf 1.003, Loss_fe 1.489, Loss_kd 2.795, Train_accy 64.53, Test_accy 62.87
2023-12-09 14:37:23,379 [foster.py] => Task 4, Epoch 43/170 => Loss 5.701, Loss_clf 1.041, Loss_fe 1.499, Loss_kd 2.810, Train_accy 64.71, Test_accy 64.70
2023-12-09 14:37:30,128 [foster.py] => Task 4, Epoch 44/170 => Loss 5.657, Loss_clf 0.987, Loss_fe 1.516, Loss_kd 2.803, Train_accy 65.38, Test_accy 63.77
2023-12-09 14:37:37,005 [foster.py] => Task 4, Epoch 45/170 => Loss 5.539, Loss_clf 0.946, Loss_fe 1.451, Loss_kd 2.793, Train_accy 64.95, Test_accy 62.90
2023-12-09 14:37:40,850 [foster.py] => Task 4, Epoch 46/170 => Loss 5.654, Loss_clf 1.031, Loss_fe 1.473, Loss_kd 2.800, Train_accy 63.36
2023-12-09 14:37:47,537 [foster.py] => Task 4, Epoch 47/170 => Loss 5.553, Loss_clf 0.954, Loss_fe 1.461, Loss_kd 2.789, Train_accy 64.36, Test_accy 61.22
2023-12-09 14:37:54,290 [foster.py] => Task 4, Epoch 48/170 => Loss 5.575, Loss_clf 0.969, Loss_fe 1.469, Loss_kd 2.789, Train_accy 64.88, Test_accy 64.31
2023-12-09 14:38:01,378 [foster.py] => Task 4, Epoch 49/170 => Loss 5.541, Loss_clf 0.931, Loss_fe 1.455, Loss_kd 2.805, Train_accy 64.74, Test_accy 64.77
2023-12-09 14:38:07,787 [foster.py] => Task 4, Epoch 50/170 => Loss 5.522, Loss_clf 0.934, Loss_fe 1.436, Loss_kd 2.802, Train_accy 65.36, Test_accy 64.28
2023-12-09 14:38:11,683 [foster.py] => Task 4, Epoch 51/170 => Loss 5.707, Loss_clf 1.079, Loss_fe 1.456, Loss_kd 2.820, Train_accy 64.61
2023-12-09 14:38:18,377 [foster.py] => Task 4, Epoch 52/170 => Loss 5.538, Loss_clf 0.953, Loss_fe 1.428, Loss_kd 2.806, Train_accy 66.42, Test_accy 64.16
2023-12-09 14:38:25,257 [foster.py] => Task 4, Epoch 53/170 => Loss 5.597, Loss_clf 1.013, Loss_fe 1.440, Loss_kd 2.795, Train_accy 64.58, Test_accy 62.54
2023-12-09 14:38:32,255 [foster.py] => Task 4, Epoch 54/170 => Loss 5.505, Loss_clf 0.930, Loss_fe 1.428, Loss_kd 2.797, Train_accy 66.58, Test_accy 64.38
2023-12-09 14:38:38,871 [foster.py] => Task 4, Epoch 55/170 => Loss 5.540, Loss_clf 0.952, Loss_fe 1.429, Loss_kd 2.808, Train_accy 65.35, Test_accy 56.22
2023-12-09 14:38:42,662 [foster.py] => Task 4, Epoch 56/170 => Loss 5.443, Loss_clf 0.907, Loss_fe 1.392, Loss_kd 2.794, Train_accy 67.27
2023-12-09 14:38:49,349 [foster.py] => Task 4, Epoch 57/170 => Loss 5.395, Loss_clf 0.868, Loss_fe 1.382, Loss_kd 2.796, Train_accy 67.11, Test_accy 64.70
2023-12-09 14:38:56,401 [foster.py] => Task 4, Epoch 58/170 => Loss 5.391, Loss_clf 0.883, Loss_fe 1.361, Loss_kd 2.798, Train_accy 66.50, Test_accy 65.03
2023-12-09 14:39:02,998 [foster.py] => Task 4, Epoch 59/170 => Loss 5.441, Loss_clf 0.902, Loss_fe 1.390, Loss_kd 2.799, Train_accy 65.97, Test_accy 64.23
2023-12-09 14:39:10,086 [foster.py] => Task 4, Epoch 60/170 => Loss 5.428, Loss_clf 0.915, Loss_fe 1.363, Loss_kd 2.800, Train_accy 66.65, Test_accy 64.56
2023-12-09 14:39:14,123 [foster.py] => Task 4, Epoch 61/170 => Loss 5.455, Loss_clf 0.929, Loss_fe 1.378, Loss_kd 2.798, Train_accy 66.27
2023-12-09 14:39:20,967 [foster.py] => Task 4, Epoch 62/170 => Loss 5.440, Loss_clf 0.924, Loss_fe 1.374, Loss_kd 2.794, Train_accy 66.62, Test_accy 64.69
2023-12-09 14:39:27,749 [foster.py] => Task 4, Epoch 63/170 => Loss 5.299, Loss_clf 0.827, Loss_fe 1.331, Loss_kd 2.792, Train_accy 67.71, Test_accy 64.33
2023-12-09 14:39:33,969 [foster.py] => Task 4, Epoch 64/170 => Loss 5.427, Loss_clf 0.929, Loss_fe 1.353, Loss_kd 2.796, Train_accy 66.32, Test_accy 64.11
2023-12-09 14:39:40,245 [foster.py] => Task 4, Epoch 65/170 => Loss 5.384, Loss_clf 0.896, Loss_fe 1.337, Loss_kd 2.801, Train_accy 67.12, Test_accy 64.97
2023-12-09 14:39:44,044 [foster.py] => Task 4, Epoch 66/170 => Loss 5.331, Loss_clf 0.860, Loss_fe 1.329, Loss_kd 2.793, Train_accy 67.64
2023-12-09 14:39:50,778 [foster.py] => Task 4, Epoch 67/170 => Loss 5.411, Loss_clf 0.932, Loss_fe 1.349, Loss_kd 2.783, Train_accy 66.24, Test_accy 62.12
2023-12-09 14:39:57,740 [foster.py] => Task 4, Epoch 68/170 => Loss 5.376, Loss_clf 0.893, Loss_fe 1.327, Loss_kd 2.805, Train_accy 66.86, Test_accy 64.69
2023-12-09 14:40:04,800 [foster.py] => Task 4, Epoch 69/170 => Loss 5.371, Loss_clf 0.897, Loss_fe 1.312, Loss_kd 2.810, Train_accy 66.45, Test_accy 64.11
2023-12-09 14:40:11,348 [foster.py] => Task 4, Epoch 70/170 => Loss 5.272, Loss_clf 0.853, Loss_fe 1.290, Loss_kd 2.782, Train_accy 68.30, Test_accy 64.92
2023-12-09 14:40:15,005 [foster.py] => Task 4, Epoch 71/170 => Loss 5.234, Loss_clf 0.832, Loss_fe 1.273, Loss_kd 2.781, Train_accy 68.44
2023-12-09 14:40:21,316 [foster.py] => Task 4, Epoch 72/170 => Loss 5.355, Loss_clf 0.905, Loss_fe 1.302, Loss_kd 2.799, Train_accy 67.47, Test_accy 64.84
2023-12-09 14:40:27,703 [foster.py] => Task 4, Epoch 73/170 => Loss 5.292, Loss_clf 0.860, Loss_fe 1.282, Loss_kd 2.800, Train_accy 67.27, Test_accy 64.08
2023-12-09 14:40:34,236 [foster.py] => Task 4, Epoch 74/170 => Loss 5.252, Loss_clf 0.834, Loss_fe 1.279, Loss_kd 2.790, Train_accy 68.85, Test_accy 65.68
2023-12-09 14:40:40,817 [foster.py] => Task 4, Epoch 75/170 => Loss 5.283, Loss_clf 0.864, Loss_fe 1.271, Loss_kd 2.799, Train_accy 68.67, Test_accy 64.67
2023-12-09 14:40:44,554 [foster.py] => Task 4, Epoch 76/170 => Loss 5.286, Loss_clf 0.869, Loss_fe 1.267, Loss_kd 2.800, Train_accy 68.70
2023-12-09 14:40:51,012 [foster.py] => Task 4, Epoch 77/170 => Loss 5.189, Loss_clf 0.807, Loss_fe 1.247, Loss_kd 2.787, Train_accy 69.62, Test_accy 64.36
2023-12-09 14:40:57,397 [foster.py] => Task 4, Epoch 78/170 => Loss 5.217, Loss_clf 0.811, Loss_fe 1.260, Loss_kd 2.796, Train_accy 68.55, Test_accy 63.81
2023-12-09 14:41:03,622 [foster.py] => Task 4, Epoch 79/170 => Loss 5.182, Loss_clf 0.801, Loss_fe 1.238, Loss_kd 2.794, Train_accy 69.12, Test_accy 64.52
2023-12-09 14:41:09,919 [foster.py] => Task 4, Epoch 80/170 => Loss 5.153, Loss_clf 0.800, Loss_fe 1.210, Loss_kd 2.794, Train_accy 69.58, Test_accy 63.78
2023-12-09 14:41:13,526 [foster.py] => Task 4, Epoch 81/170 => Loss 5.249, Loss_clf 0.861, Loss_fe 1.239, Loss_kd 2.799, Train_accy 67.68
2023-12-09 14:41:20,001 [foster.py] => Task 4, Epoch 82/170 => Loss 5.213, Loss_clf 0.826, Loss_fe 1.232, Loss_kd 2.804, Train_accy 69.38, Test_accy 64.16
2023-12-09 14:41:26,837 [foster.py] => Task 4, Epoch 83/170 => Loss 5.219, Loss_clf 0.825, Loss_fe 1.254, Loss_kd 2.791, Train_accy 69.52, Test_accy 65.83
2023-12-09 14:41:33,795 [foster.py] => Task 4, Epoch 84/170 => Loss 5.131, Loss_clf 0.784, Loss_fe 1.204, Loss_kd 2.794, Train_accy 69.14, Test_accy 64.97
2023-12-09 14:41:40,223 [foster.py] => Task 4, Epoch 85/170 => Loss 5.156, Loss_clf 0.815, Loss_fe 1.190, Loss_kd 2.801, Train_accy 69.42, Test_accy 64.88
2023-12-09 14:41:43,742 [foster.py] => Task 4, Epoch 86/170 => Loss 5.098, Loss_clf 0.786, Loss_fe 1.180, Loss_kd 2.784, Train_accy 69.23
2023-12-09 14:41:50,337 [foster.py] => Task 4, Epoch 87/170 => Loss 5.039, Loss_clf 0.767, Loss_fe 1.150, Loss_kd 2.776, Train_accy 70.06, Test_accy 64.78
2023-12-09 14:41:56,533 [foster.py] => Task 4, Epoch 88/170 => Loss 5.084, Loss_clf 0.763, Loss_fe 1.184, Loss_kd 2.788, Train_accy 70.39, Test_accy 64.90
2023-12-09 14:42:02,921 [foster.py] => Task 4, Epoch 89/170 => Loss 5.107, Loss_clf 0.780, Loss_fe 1.173, Loss_kd 2.803, Train_accy 70.95, Test_accy 65.97
2023-12-09 14:42:09,385 [foster.py] => Task 4, Epoch 90/170 => Loss 5.022, Loss_clf 0.747, Loss_fe 1.144, Loss_kd 2.784, Train_accy 70.59, Test_accy 64.54
2023-12-09 14:42:13,264 [foster.py] => Task 4, Epoch 91/170 => Loss 5.051, Loss_clf 0.765, Loss_fe 1.134, Loss_kd 2.802, Train_accy 71.08
2023-12-09 14:42:19,381 [foster.py] => Task 4, Epoch 92/170 => Loss 5.047, Loss_clf 0.766, Loss_fe 1.126, Loss_kd 2.805, Train_accy 70.86, Test_accy 65.40
2023-12-09 14:42:25,541 [foster.py] => Task 4, Epoch 93/170 => Loss 5.090, Loss_clf 0.786, Loss_fe 1.150, Loss_kd 2.804, Train_accy 70.80, Test_accy 65.23
2023-12-09 14:42:31,761 [foster.py] => Task 4, Epoch 94/170 => Loss 5.071, Loss_clf 0.779, Loss_fe 1.139, Loss_kd 2.803, Train_accy 70.53, Test_accy 65.94
2023-12-09 14:42:38,226 [foster.py] => Task 4, Epoch 95/170 => Loss 4.993, Loss_clf 0.739, Loss_fe 1.107, Loss_kd 2.798, Train_accy 70.98, Test_accy 65.91
2023-12-09 14:42:41,858 [foster.py] => Task 4, Epoch 96/170 => Loss 4.943, Loss_clf 0.719, Loss_fe 1.085, Loss_kd 2.791, Train_accy 71.79
2023-12-09 14:42:48,222 [foster.py] => Task 4, Epoch 97/170 => Loss 4.882, Loss_clf 0.690, Loss_fe 1.052, Loss_kd 2.790, Train_accy 72.42, Test_accy 65.78
2023-12-09 14:42:54,766 [foster.py] => Task 4, Epoch 98/170 => Loss 4.905, Loss_clf 0.703, Loss_fe 1.070, Loss_kd 2.784, Train_accy 72.09, Test_accy 66.04
2023-12-09 14:43:01,779 [foster.py] => Task 4, Epoch 99/170 => Loss 4.880, Loss_clf 0.692, Loss_fe 1.053, Loss_kd 2.787, Train_accy 72.12, Test_accy 65.58
2023-12-09 14:43:07,944 [foster.py] => Task 4, Epoch 100/170 => Loss 4.928, Loss_clf 0.730, Loss_fe 1.053, Loss_kd 2.796, Train_accy 72.03, Test_accy 63.73
2023-12-09 14:43:11,636 [foster.py] => Task 4, Epoch 101/170 => Loss 4.892, Loss_clf 0.694, Loss_fe 1.050, Loss_kd 2.799, Train_accy 73.12
2023-12-09 14:43:17,749 [foster.py] => Task 4, Epoch 102/170 => Loss 4.919, Loss_clf 0.713, Loss_fe 1.060, Loss_kd 2.797, Train_accy 71.97, Test_accy 65.29
2023-12-09 14:43:23,902 [foster.py] => Task 4, Epoch 103/170 => Loss 4.867, Loss_clf 0.699, Loss_fe 1.029, Loss_kd 2.791, Train_accy 72.58, Test_accy 65.24
2023-12-09 14:43:30,436 [foster.py] => Task 4, Epoch 104/170 => Loss 4.916, Loss_clf 0.719, Loss_fe 1.043, Loss_kd 2.803, Train_accy 72.38, Test_accy 65.88
2023-12-09 14:43:36,766 [foster.py] => Task 4, Epoch 105/170 => Loss 4.877, Loss_clf 0.702, Loss_fe 1.029, Loss_kd 2.796, Train_accy 72.30, Test_accy 65.63
2023-12-09 14:43:40,606 [foster.py] => Task 4, Epoch 106/170 => Loss 4.869, Loss_clf 0.705, Loss_fe 1.018, Loss_kd 2.797, Train_accy 73.29
2023-12-09 14:43:47,240 [foster.py] => Task 4, Epoch 107/170 => Loss 4.856, Loss_clf 0.697, Loss_fe 1.017, Loss_kd 2.793, Train_accy 73.03, Test_accy 66.17
2023-12-09 14:43:53,980 [foster.py] => Task 4, Epoch 108/170 => Loss 4.803, Loss_clf 0.671, Loss_fe 0.992, Loss_kd 2.791, Train_accy 73.52, Test_accy 66.14
2023-12-09 14:44:00,717 [foster.py] => Task 4, Epoch 109/170 => Loss 4.746, Loss_clf 0.645, Loss_fe 0.961, Loss_kd 2.791, Train_accy 74.82, Test_accy 66.34
2023-12-09 14:44:07,248 [foster.py] => Task 4, Epoch 110/170 => Loss 4.787, Loss_clf 0.659, Loss_fe 0.988, Loss_kd 2.792, Train_accy 74.41, Test_accy 65.67
2023-12-09 14:44:10,914 [foster.py] => Task 4, Epoch 111/170 => Loss 4.790, Loss_clf 0.681, Loss_fe 0.972, Loss_kd 2.789, Train_accy 73.59
2023-12-09 14:44:17,244 [foster.py] => Task 4, Epoch 112/170 => Loss 4.813, Loss_clf 0.667, Loss_fe 0.999, Loss_kd 2.797, Train_accy 73.79, Test_accy 65.81
2023-12-09 14:44:23,466 [foster.py] => Task 4, Epoch 113/170 => Loss 4.784, Loss_clf 0.669, Loss_fe 0.969, Loss_kd 2.796, Train_accy 74.35, Test_accy 66.40
2023-12-09 14:44:29,887 [foster.py] => Task 4, Epoch 114/170 => Loss 4.640, Loss_clf 0.592, Loss_fe 0.911, Loss_kd 2.788, Train_accy 76.12, Test_accy 66.56
2023-12-09 14:44:36,549 [foster.py] => Task 4, Epoch 115/170 => Loss 4.644, Loss_clf 0.611, Loss_fe 0.896, Loss_kd 2.789, Train_accy 75.86, Test_accy 65.94
2023-12-09 14:44:40,412 [foster.py] => Task 4, Epoch 116/170 => Loss 4.696, Loss_clf 0.636, Loss_fe 0.924, Loss_kd 2.787, Train_accy 75.09
2023-12-09 14:44:46,917 [foster.py] => Task 4, Epoch 117/170 => Loss 4.707, Loss_clf 0.637, Loss_fe 0.930, Loss_kd 2.791, Train_accy 75.55, Test_accy 66.26
2023-12-09 14:44:53,297 [foster.py] => Task 4, Epoch 118/170 => Loss 4.723, Loss_clf 0.648, Loss_fe 0.936, Loss_kd 2.789, Train_accy 74.89, Test_accy 66.21
2023-12-09 14:45:00,067 [foster.py] => Task 4, Epoch 119/170 => Loss 4.654, Loss_clf 0.603, Loss_fe 0.899, Loss_kd 2.801, Train_accy 76.20, Test_accy 65.93
2023-12-09 14:45:06,869 [foster.py] => Task 4, Epoch 120/170 => Loss 4.635, Loss_clf 0.611, Loss_fe 0.883, Loss_kd 2.792, Train_accy 75.97, Test_accy 65.80
2023-12-09 14:45:10,907 [foster.py] => Task 4, Epoch 121/170 => Loss 4.557, Loss_clf 0.572, Loss_fe 0.844, Loss_kd 2.792, Train_accy 77.21
2023-12-09 14:45:17,946 [foster.py] => Task 4, Epoch 122/170 => Loss 4.633, Loss_clf 0.610, Loss_fe 0.886, Loss_kd 2.789, Train_accy 76.52, Test_accy 66.58
2023-12-09 14:45:24,682 [foster.py] => Task 4, Epoch 123/170 => Loss 4.570, Loss_clf 0.586, Loss_fe 0.854, Loss_kd 2.782, Train_accy 76.94, Test_accy 65.57
2023-12-09 14:45:31,653 [foster.py] => Task 4, Epoch 124/170 => Loss 4.607, Loss_clf 0.601, Loss_fe 0.871, Loss_kd 2.787, Train_accy 76.39, Test_accy 66.61
2023-12-09 14:45:38,352 [foster.py] => Task 4, Epoch 125/170 => Loss 4.560, Loss_clf 0.588, Loss_fe 0.836, Loss_kd 2.787, Train_accy 76.91, Test_accy 66.10
2023-12-09 14:45:42,224 [foster.py] => Task 4, Epoch 126/170 => Loss 4.556, Loss_clf 0.581, Loss_fe 0.825, Loss_kd 2.801, Train_accy 77.21
2023-12-09 14:45:48,754 [foster.py] => Task 4, Epoch 127/170 => Loss 4.532, Loss_clf 0.559, Loss_fe 0.826, Loss_kd 2.796, Train_accy 77.89, Test_accy 66.73
2023-12-09 14:45:55,621 [foster.py] => Task 4, Epoch 128/170 => Loss 4.544, Loss_clf 0.572, Loss_fe 0.835, Loss_kd 2.788, Train_accy 77.30, Test_accy 66.37
2023-12-09 14:46:02,386 [foster.py] => Task 4, Epoch 129/170 => Loss 4.494, Loss_clf 0.559, Loss_fe 0.802, Loss_kd 2.786, Train_accy 78.33, Test_accy 66.72
2023-12-09 14:46:09,362 [foster.py] => Task 4, Epoch 130/170 => Loss 4.461, Loss_clf 0.544, Loss_fe 0.786, Loss_kd 2.783, Train_accy 78.26, Test_accy 66.44
2023-12-09 14:46:13,439 [foster.py] => Task 4, Epoch 131/170 => Loss 4.492, Loss_clf 0.558, Loss_fe 0.791, Loss_kd 2.793, Train_accy 78.48
2023-12-09 14:46:20,452 [foster.py] => Task 4, Epoch 132/170 => Loss 4.505, Loss_clf 0.560, Loss_fe 0.807, Loss_kd 2.790, Train_accy 78.05, Test_accy 66.20
2023-12-09 14:46:27,458 [foster.py] => Task 4, Epoch 133/170 => Loss 4.456, Loss_clf 0.547, Loss_fe 0.774, Loss_kd 2.787, Train_accy 78.48, Test_accy 66.51
2023-12-09 14:46:34,331 [foster.py] => Task 4, Epoch 134/170 => Loss 4.445, Loss_clf 0.537, Loss_fe 0.767, Loss_kd 2.791, Train_accy 79.44, Test_accy 66.47
2023-12-09 14:46:41,289 [foster.py] => Task 4, Epoch 135/170 => Loss 4.413, Loss_clf 0.530, Loss_fe 0.753, Loss_kd 2.782, Train_accy 78.80, Test_accy 67.11
2023-12-09 14:46:45,155 [foster.py] => Task 4, Epoch 136/170 => Loss 4.405, Loss_clf 0.529, Loss_fe 0.741, Loss_kd 2.786, Train_accy 78.79
2023-12-09 14:46:51,966 [foster.py] => Task 4, Epoch 137/170 => Loss 4.431, Loss_clf 0.534, Loss_fe 0.750, Loss_kd 2.797, Train_accy 79.50, Test_accy 66.57
2023-12-09 14:46:59,004 [foster.py] => Task 4, Epoch 138/170 => Loss 4.409, Loss_clf 0.528, Loss_fe 0.739, Loss_kd 2.793, Train_accy 79.09, Test_accy 67.31
2023-12-09 14:47:05,936 [foster.py] => Task 4, Epoch 139/170 => Loss 4.384, Loss_clf 0.517, Loss_fe 0.725, Loss_kd 2.793, Train_accy 80.02, Test_accy 66.78
2023-12-09 14:47:13,042 [foster.py] => Task 4, Epoch 140/170 => Loss 4.348, Loss_clf 0.513, Loss_fe 0.710, Loss_kd 2.778, Train_accy 79.06, Test_accy 67.17
2023-12-09 14:47:17,081 [foster.py] => Task 4, Epoch 141/170 => Loss 4.354, Loss_clf 0.508, Loss_fe 0.712, Loss_kd 2.787, Train_accy 79.89
2023-12-09 14:47:24,014 [foster.py] => Task 4, Epoch 142/170 => Loss 4.341, Loss_clf 0.508, Loss_fe 0.703, Loss_kd 2.782, Train_accy 80.06, Test_accy 66.76
2023-12-09 14:47:31,068 [foster.py] => Task 4, Epoch 143/170 => Loss 4.336, Loss_clf 0.510, Loss_fe 0.699, Loss_kd 2.779, Train_accy 80.33, Test_accy 67.11
2023-12-09 14:47:37,910 [foster.py] => Task 4, Epoch 144/170 => Loss 4.314, Loss_clf 0.495, Loss_fe 0.680, Loss_kd 2.790, Train_accy 81.24, Test_accy 67.31
2023-12-09 14:47:44,280 [foster.py] => Task 4, Epoch 145/170 => Loss 4.329, Loss_clf 0.496, Loss_fe 0.696, Loss_kd 2.789, Train_accy 80.97, Test_accy 66.79
2023-12-09 14:47:47,893 [foster.py] => Task 4, Epoch 146/170 => Loss 4.350, Loss_clf 0.502, Loss_fe 0.695, Loss_kd 2.802, Train_accy 80.21
2023-12-09 14:47:54,906 [foster.py] => Task 4, Epoch 147/170 => Loss 4.293, Loss_clf 0.477, Loss_fe 0.670, Loss_kd 2.797, Train_accy 80.71, Test_accy 67.09
2023-12-09 14:48:01,934 [foster.py] => Task 4, Epoch 148/170 => Loss 4.254, Loss_clf 0.467, Loss_fe 0.657, Loss_kd 2.782, Train_accy 81.06, Test_accy 67.30
2023-12-09 14:48:08,882 [foster.py] => Task 4, Epoch 149/170 => Loss 4.338, Loss_clf 0.504, Loss_fe 0.686, Loss_kd 2.799, Train_accy 80.39, Test_accy 66.97
2023-12-09 14:48:15,786 [foster.py] => Task 4, Epoch 150/170 => Loss 4.228, Loss_clf 0.460, Loss_fe 0.642, Loss_kd 2.779, Train_accy 82.26, Test_accy 66.91
2023-12-09 14:48:19,855 [foster.py] => Task 4, Epoch 151/170 => Loss 4.273, Loss_clf 0.485, Loss_fe 0.654, Loss_kd 2.785, Train_accy 81.98
2023-12-09 14:48:26,747 [foster.py] => Task 4, Epoch 152/170 => Loss 4.255, Loss_clf 0.465, Loss_fe 0.645, Loss_kd 2.795, Train_accy 81.65, Test_accy 66.88
2023-12-09 14:48:33,344 [foster.py] => Task 4, Epoch 153/170 => Loss 4.218, Loss_clf 0.454, Loss_fe 0.629, Loss_kd 2.787, Train_accy 81.50, Test_accy 66.97
2023-12-09 14:48:40,119 [foster.py] => Task 4, Epoch 154/170 => Loss 4.231, Loss_clf 0.469, Loss_fe 0.638, Loss_kd 2.777, Train_accy 81.53, Test_accy 67.30
2023-12-09 14:48:46,852 [foster.py] => Task 4, Epoch 155/170 => Loss 4.285, Loss_clf 0.486, Loss_fe 0.651, Loss_kd 2.798, Train_accy 81.38, Test_accy 67.21
2023-12-09 14:48:50,709 [foster.py] => Task 4, Epoch 156/170 => Loss 4.278, Loss_clf 0.488, Loss_fe 0.643, Loss_kd 2.797, Train_accy 81.48
2023-12-09 14:48:57,640 [foster.py] => Task 4, Epoch 157/170 => Loss 4.280, Loss_clf 0.478, Loss_fe 0.648, Loss_kd 2.803, Train_accy 81.27, Test_accy 67.04
2023-12-09 14:49:04,834 [foster.py] => Task 4, Epoch 158/170 => Loss 4.201, Loss_clf 0.450, Loss_fe 0.617, Loss_kd 2.786, Train_accy 81.92, Test_accy 67.17
2023-12-09 14:49:11,654 [foster.py] => Task 4, Epoch 159/170 => Loss 4.233, Loss_clf 0.465, Loss_fe 0.627, Loss_kd 2.792, Train_accy 81.92, Test_accy 67.21
2023-12-09 14:49:18,030 [foster.py] => Task 4, Epoch 160/170 => Loss 4.253, Loss_clf 0.474, Loss_fe 0.628, Loss_kd 2.801, Train_accy 81.89, Test_accy 67.09
2023-12-09 14:49:21,682 [foster.py] => Task 4, Epoch 161/170 => Loss 4.193, Loss_clf 0.450, Loss_fe 0.610, Loss_kd 2.786, Train_accy 81.85
2023-12-09 14:49:28,111 [foster.py] => Task 4, Epoch 162/170 => Loss 4.212, Loss_clf 0.462, Loss_fe 0.621, Loss_kd 2.782, Train_accy 81.85, Test_accy 67.18
2023-12-09 14:49:34,774 [foster.py] => Task 4, Epoch 163/170 => Loss 4.239, Loss_clf 0.469, Loss_fe 0.631, Loss_kd 2.791, Train_accy 81.58, Test_accy 67.04
2023-12-09 14:49:41,200 [foster.py] => Task 4, Epoch 164/170 => Loss 4.200, Loss_clf 0.450, Loss_fe 0.613, Loss_kd 2.789, Train_accy 82.59, Test_accy 67.03
2023-12-09 14:49:47,942 [foster.py] => Task 4, Epoch 165/170 => Loss 4.185, Loss_clf 0.449, Loss_fe 0.601, Loss_kd 2.787, Train_accy 82.44, Test_accy 67.03
2023-12-09 14:49:51,993 [foster.py] => Task 4, Epoch 166/170 => Loss 4.252, Loss_clf 0.473, Loss_fe 0.631, Loss_kd 2.798, Train_accy 81.65
2023-12-09 14:49:58,924 [foster.py] => Task 4, Epoch 167/170 => Loss 4.175, Loss_clf 0.442, Loss_fe 0.595, Loss_kd 2.790, Train_accy 82.86, Test_accy 67.03
2023-12-09 14:50:05,658 [foster.py] => Task 4, Epoch 168/170 => Loss 4.194, Loss_clf 0.455, Loss_fe 0.609, Loss_kd 2.782, Train_accy 82.35, Test_accy 67.16
2023-12-09 14:50:12,574 [foster.py] => Task 4, Epoch 169/170 => Loss 4.149, Loss_clf 0.436, Loss_fe 0.592, Loss_kd 2.775, Train_accy 82.62, Test_accy 67.07
2023-12-09 14:50:19,057 [foster.py] => Task 4, Epoch 170/170 => Loss 4.217, Loss_clf 0.453, Loss_fe 0.613, Loss_kd 2.801, Train_accy 82.24, Test_accy 67.13
2023-12-09 14:50:19,058 [foster.py] => do not weight align teacher!
2023-12-09 14:50:19,059 [foster.py] => per cls weights : [1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712 1.06430712
 1.06430712 1.06430712 0.48554305 0.48554305 0.48554305 0.48554305
 0.48554305 0.48554305 0.48554305 0.48554305 0.48554305 0.48554305]
2023-12-09 14:50:26,064 [foster.py] => SNet: Task 4, Epoch 1/130 => Loss 3.274,  Train_accy 37.94, Test_accy 59.01
2023-12-09 14:50:30,470 [foster.py] => SNet: Task 4, Epoch 2/130 => Loss 3.060,  Train_accy 53.92
2023-12-09 14:50:34,818 [foster.py] => SNet: Task 4, Epoch 3/130 => Loss 3.038,  Train_accy 56.50
2023-12-09 14:50:39,168 [foster.py] => SNet: Task 4, Epoch 4/130 => Loss 3.038,  Train_accy 57.42
2023-12-09 14:50:43,443 [foster.py] => SNet: Task 4, Epoch 5/130 => Loss 2.996,  Train_accy 58.48
2023-12-09 14:50:50,030 [foster.py] => SNet: Task 4, Epoch 6/130 => Loss 2.992,  Train_accy 59.06, Test_accy 61.48
2023-12-09 14:50:54,050 [foster.py] => SNet: Task 4, Epoch 7/130 => Loss 2.989,  Train_accy 59.71
2023-12-09 14:50:58,153 [foster.py] => SNet: Task 4, Epoch 8/130 => Loss 2.987,  Train_accy 61.08
2023-12-09 14:51:02,222 [foster.py] => SNet: Task 4, Epoch 9/130 => Loss 2.975,  Train_accy 61.32
2023-12-09 14:51:06,312 [foster.py] => SNet: Task 4, Epoch 10/130 => Loss 2.995,  Train_accy 60.29
2023-12-09 14:51:12,538 [foster.py] => SNet: Task 4, Epoch 11/130 => Loss 2.989,  Train_accy 61.02, Test_accy 62.43
2023-12-09 14:51:16,843 [foster.py] => SNet: Task 4, Epoch 12/130 => Loss 2.978,  Train_accy 61.14
2023-12-09 14:51:21,247 [foster.py] => SNet: Task 4, Epoch 13/130 => Loss 2.982,  Train_accy 60.71
2023-12-09 14:51:25,473 [foster.py] => SNet: Task 4, Epoch 14/130 => Loss 2.974,  Train_accy 61.24
2023-12-09 14:51:29,415 [foster.py] => SNet: Task 4, Epoch 15/130 => Loss 2.969,  Train_accy 62.11
2023-12-09 14:51:35,564 [foster.py] => SNet: Task 4, Epoch 16/130 => Loss 2.972,  Train_accy 61.44, Test_accy 61.48
2023-12-09 14:51:39,821 [foster.py] => SNet: Task 4, Epoch 17/130 => Loss 2.986,  Train_accy 61.82
2023-12-09 14:51:43,968 [foster.py] => SNet: Task 4, Epoch 18/130 => Loss 2.965,  Train_accy 61.95
2023-12-09 14:51:47,915 [foster.py] => SNet: Task 4, Epoch 19/130 => Loss 2.974,  Train_accy 62.17
2023-12-09 14:51:51,944 [foster.py] => SNet: Task 4, Epoch 20/130 => Loss 2.966,  Train_accy 63.18
2023-12-09 14:51:58,119 [foster.py] => SNet: Task 4, Epoch 21/130 => Loss 2.961,  Train_accy 63.06, Test_accy 64.09
2023-12-09 14:52:02,133 [foster.py] => SNet: Task 4, Epoch 22/130 => Loss 2.947,  Train_accy 62.98
2023-12-09 14:52:06,042 [foster.py] => SNet: Task 4, Epoch 23/130 => Loss 2.966,  Train_accy 63.05
2023-12-09 14:52:10,165 [foster.py] => SNet: Task 4, Epoch 24/130 => Loss 2.970,  Train_accy 62.32
2023-12-09 14:52:14,355 [foster.py] => SNet: Task 4, Epoch 25/130 => Loss 2.961,  Train_accy 63.70
2023-12-09 14:52:21,151 [foster.py] => SNet: Task 4, Epoch 26/130 => Loss 2.944,  Train_accy 64.05, Test_accy 64.20
2023-12-09 14:52:25,246 [foster.py] => SNet: Task 4, Epoch 27/130 => Loss 2.952,  Train_accy 63.80
2023-12-09 14:52:29,237 [foster.py] => SNet: Task 4, Epoch 28/130 => Loss 2.964,  Train_accy 63.03
2023-12-09 14:52:33,313 [foster.py] => SNet: Task 4, Epoch 29/130 => Loss 2.950,  Train_accy 62.98
2023-12-09 14:52:37,403 [foster.py] => SNet: Task 4, Epoch 30/130 => Loss 2.959,  Train_accy 64.48
2023-12-09 14:52:43,452 [foster.py] => SNet: Task 4, Epoch 31/130 => Loss 2.950,  Train_accy 63.38, Test_accy 64.76
2023-12-09 14:52:47,606 [foster.py] => SNet: Task 4, Epoch 32/130 => Loss 2.949,  Train_accy 63.39
2023-12-09 14:52:51,760 [foster.py] => SNet: Task 4, Epoch 33/130 => Loss 2.956,  Train_accy 64.09
2023-12-09 14:52:55,888 [foster.py] => SNet: Task 4, Epoch 34/130 => Loss 2.947,  Train_accy 63.67
2023-12-09 14:53:00,502 [foster.py] => SNet: Task 4, Epoch 35/130 => Loss 2.951,  Train_accy 64.62
2023-12-09 14:53:07,248 [foster.py] => SNet: Task 4, Epoch 36/130 => Loss 2.969,  Train_accy 63.11, Test_accy 65.14
2023-12-09 14:53:11,377 [foster.py] => SNet: Task 4, Epoch 37/130 => Loss 2.951,  Train_accy 64.53
2023-12-09 14:53:15,386 [foster.py] => SNet: Task 4, Epoch 38/130 => Loss 2.970,  Train_accy 63.18
2023-12-09 14:53:19,394 [foster.py] => SNet: Task 4, Epoch 39/130 => Loss 2.944,  Train_accy 64.45
2023-12-09 14:53:23,361 [foster.py] => SNet: Task 4, Epoch 40/130 => Loss 2.971,  Train_accy 63.39
2023-12-09 14:53:29,589 [foster.py] => SNet: Task 4, Epoch 41/130 => Loss 2.957,  Train_accy 64.76, Test_accy 64.32
2023-12-09 14:53:33,504 [foster.py] => SNet: Task 4, Epoch 42/130 => Loss 2.950,  Train_accy 64.52
2023-12-09 14:53:37,570 [foster.py] => SNet: Task 4, Epoch 43/130 => Loss 2.938,  Train_accy 64.59
2023-12-09 14:53:41,920 [foster.py] => SNet: Task 4, Epoch 44/130 => Loss 2.972,  Train_accy 64.79
2023-12-09 14:53:46,148 [foster.py] => SNet: Task 4, Epoch 45/130 => Loss 2.955,  Train_accy 63.80
2023-12-09 14:53:52,882 [foster.py] => SNet: Task 4, Epoch 46/130 => Loss 2.963,  Train_accy 65.21, Test_accy 63.80
2023-12-09 14:53:56,941 [foster.py] => SNet: Task 4, Epoch 47/130 => Loss 2.953,  Train_accy 64.71
2023-12-09 14:54:01,019 [foster.py] => SNet: Task 4, Epoch 48/130 => Loss 2.960,  Train_accy 64.53
2023-12-09 14:54:05,154 [foster.py] => SNet: Task 4, Epoch 49/130 => Loss 2.946,  Train_accy 64.50
2023-12-09 14:54:09,202 [foster.py] => SNet: Task 4, Epoch 50/130 => Loss 2.946,  Train_accy 64.42
2023-12-09 14:54:15,511 [foster.py] => SNet: Task 4, Epoch 51/130 => Loss 2.936,  Train_accy 65.36, Test_accy 64.50
2023-12-09 14:54:19,513 [foster.py] => SNet: Task 4, Epoch 52/130 => Loss 2.940,  Train_accy 64.14
2023-12-09 14:54:23,706 [foster.py] => SNet: Task 4, Epoch 53/130 => Loss 2.940,  Train_accy 65.45
2023-12-09 14:54:28,218 [foster.py] => SNet: Task 4, Epoch 54/130 => Loss 2.941,  Train_accy 63.38
2023-12-09 14:54:32,574 [foster.py] => SNet: Task 4, Epoch 55/130 => Loss 2.937,  Train_accy 64.64
2023-12-09 14:54:39,143 [foster.py] => SNet: Task 4, Epoch 56/130 => Loss 2.940,  Train_accy 64.45, Test_accy 65.03
2023-12-09 14:54:43,138 [foster.py] => SNet: Task 4, Epoch 57/130 => Loss 2.944,  Train_accy 65.23
2023-12-09 14:54:47,320 [foster.py] => SNet: Task 4, Epoch 58/130 => Loss 2.931,  Train_accy 65.02
2023-12-09 14:54:51,697 [foster.py] => SNet: Task 4, Epoch 59/130 => Loss 2.951,  Train_accy 65.27
2023-12-09 14:54:55,754 [foster.py] => SNet: Task 4, Epoch 60/130 => Loss 2.964,  Train_accy 64.83
2023-12-09 14:55:01,954 [foster.py] => SNet: Task 4, Epoch 61/130 => Loss 2.933,  Train_accy 65.20, Test_accy 64.92
2023-12-09 14:55:05,999 [foster.py] => SNet: Task 4, Epoch 62/130 => Loss 2.958,  Train_accy 64.85
2023-12-09 14:55:10,149 [foster.py] => SNet: Task 4, Epoch 63/130 => Loss 2.944,  Train_accy 65.18
2023-12-09 14:55:14,230 [foster.py] => SNet: Task 4, Epoch 64/130 => Loss 2.939,  Train_accy 65.39
2023-12-09 14:55:18,366 [foster.py] => SNet: Task 4, Epoch 65/130 => Loss 2.921,  Train_accy 65.44
2023-12-09 14:55:24,650 [foster.py] => SNet: Task 4, Epoch 66/130 => Loss 2.943,  Train_accy 65.62, Test_accy 65.32
2023-12-09 14:55:28,690 [foster.py] => SNet: Task 4, Epoch 67/130 => Loss 2.932,  Train_accy 64.68
2023-12-09 14:55:32,937 [foster.py] => SNet: Task 4, Epoch 68/130 => Loss 2.932,  Train_accy 66.44
2023-12-09 14:55:37,032 [foster.py] => SNet: Task 4, Epoch 69/130 => Loss 2.928,  Train_accy 65.32
2023-12-09 14:55:41,063 [foster.py] => SNet: Task 4, Epoch 70/130 => Loss 2.939,  Train_accy 65.88
2023-12-09 14:55:47,155 [foster.py] => SNet: Task 4, Epoch 71/130 => Loss 2.931,  Train_accy 65.29, Test_accy 65.37
2023-12-09 14:55:51,555 [foster.py] => SNet: Task 4, Epoch 72/130 => Loss 2.943,  Train_accy 64.82
2023-12-09 14:55:55,750 [foster.py] => SNet: Task 4, Epoch 73/130 => Loss 2.931,  Train_accy 65.39
2023-12-09 14:56:00,216 [foster.py] => SNet: Task 4, Epoch 74/130 => Loss 2.946,  Train_accy 65.92
2023-12-09 14:56:04,608 [foster.py] => SNet: Task 4, Epoch 75/130 => Loss 2.939,  Train_accy 64.03
2023-12-09 14:56:11,100 [foster.py] => SNet: Task 4, Epoch 76/130 => Loss 2.928,  Train_accy 65.56, Test_accy 65.21
2023-12-09 14:56:15,175 [foster.py] => SNet: Task 4, Epoch 77/130 => Loss 2.934,  Train_accy 65.80
2023-12-09 14:56:19,332 [foster.py] => SNet: Task 4, Epoch 78/130 => Loss 2.926,  Train_accy 65.94
2023-12-09 14:56:23,365 [foster.py] => SNet: Task 4, Epoch 79/130 => Loss 2.932,  Train_accy 66.41
2023-12-09 14:56:27,441 [foster.py] => SNet: Task 4, Epoch 80/130 => Loss 2.928,  Train_accy 66.48
2023-12-09 14:56:33,577 [foster.py] => SNet: Task 4, Epoch 81/130 => Loss 2.930,  Train_accy 65.98, Test_accy 65.51
2023-12-09 14:56:37,434 [foster.py] => SNet: Task 4, Epoch 82/130 => Loss 2.949,  Train_accy 65.89
2023-12-09 14:56:41,853 [foster.py] => SNet: Task 4, Epoch 83/130 => Loss 2.929,  Train_accy 65.47
2023-12-09 14:56:46,155 [foster.py] => SNet: Task 4, Epoch 84/130 => Loss 2.952,  Train_accy 64.74
2023-12-09 14:56:50,426 [foster.py] => SNet: Task 4, Epoch 85/130 => Loss 2.931,  Train_accy 65.21
2023-12-09 14:56:56,988 [foster.py] => SNet: Task 4, Epoch 86/130 => Loss 2.922,  Train_accy 66.00, Test_accy 65.67
2023-12-09 14:57:01,048 [foster.py] => SNet: Task 4, Epoch 87/130 => Loss 2.916,  Train_accy 66.85
2023-12-09 14:57:05,200 [foster.py] => SNet: Task 4, Epoch 88/130 => Loss 2.943,  Train_accy 65.80
2023-12-09 14:57:09,098 [foster.py] => SNet: Task 4, Epoch 89/130 => Loss 2.930,  Train_accy 65.77
2023-12-09 14:57:12,796 [foster.py] => SNet: Task 4, Epoch 90/130 => Loss 2.931,  Train_accy 66.33
2023-12-09 14:57:18,790 [foster.py] => SNet: Task 4, Epoch 91/130 => Loss 2.926,  Train_accy 66.20, Test_accy 65.76
2023-12-09 14:57:22,888 [foster.py] => SNet: Task 4, Epoch 92/130 => Loss 2.937,  Train_accy 65.95
2023-12-09 14:57:26,900 [foster.py] => SNet: Task 4, Epoch 93/130 => Loss 2.924,  Train_accy 65.94
2023-12-09 14:57:31,082 [foster.py] => SNet: Task 4, Epoch 94/130 => Loss 2.937,  Train_accy 65.36
2023-12-09 14:57:35,463 [foster.py] => SNet: Task 4, Epoch 95/130 => Loss 2.925,  Train_accy 66.05
2023-12-09 14:57:41,645 [foster.py] => SNet: Task 4, Epoch 96/130 => Loss 2.916,  Train_accy 66.45, Test_accy 65.88
2023-12-09 14:57:45,605 [foster.py] => SNet: Task 4, Epoch 97/130 => Loss 2.951,  Train_accy 66.29
2023-12-09 14:57:49,529 [foster.py] => SNet: Task 4, Epoch 98/130 => Loss 2.918,  Train_accy 66.36
2023-12-09 14:57:53,696 [foster.py] => SNet: Task 4, Epoch 99/130 => Loss 2.931,  Train_accy 66.41
2023-12-09 14:57:57,715 [foster.py] => SNet: Task 4, Epoch 100/130 => Loss 2.924,  Train_accy 66.30
2023-12-09 14:58:03,678 [foster.py] => SNet: Task 4, Epoch 101/130 => Loss 2.928,  Train_accy 66.18, Test_accy 65.68
2023-12-09 14:58:07,797 [foster.py] => SNet: Task 4, Epoch 102/130 => Loss 2.916,  Train_accy 66.94
2023-12-09 14:58:12,082 [foster.py] => SNet: Task 4, Epoch 103/130 => Loss 2.923,  Train_accy 65.74
2023-12-09 14:58:16,437 [foster.py] => SNet: Task 4, Epoch 104/130 => Loss 2.931,  Train_accy 66.14
2023-12-09 14:58:20,913 [foster.py] => SNet: Task 4, Epoch 105/130 => Loss 2.933,  Train_accy 66.24
2023-12-09 14:58:27,107 [foster.py] => SNet: Task 4, Epoch 106/130 => Loss 2.928,  Train_accy 66.47, Test_accy 65.86
2023-12-09 14:58:31,197 [foster.py] => SNet: Task 4, Epoch 107/130 => Loss 2.908,  Train_accy 66.76
2023-12-09 14:58:35,249 [foster.py] => SNet: Task 4, Epoch 108/130 => Loss 2.928,  Train_accy 66.65
2023-12-09 14:58:39,470 [foster.py] => SNet: Task 4, Epoch 109/130 => Loss 2.918,  Train_accy 65.76
2023-12-09 14:58:43,514 [foster.py] => SNet: Task 4, Epoch 110/130 => Loss 2.918,  Train_accy 66.44
2023-12-09 14:58:49,688 [foster.py] => SNet: Task 4, Epoch 111/130 => Loss 2.928,  Train_accy 66.83, Test_accy 65.99
2023-12-09 14:58:53,702 [foster.py] => SNet: Task 4, Epoch 112/130 => Loss 2.924,  Train_accy 66.20
2023-12-09 14:58:57,839 [foster.py] => SNet: Task 4, Epoch 113/130 => Loss 2.923,  Train_accy 66.33
2023-12-09 14:59:02,175 [foster.py] => SNet: Task 4, Epoch 114/130 => Loss 2.923,  Train_accy 66.36
2023-12-09 14:59:06,426 [foster.py] => SNet: Task 4, Epoch 115/130 => Loss 2.925,  Train_accy 66.86
2023-12-09 14:59:12,446 [foster.py] => SNet: Task 4, Epoch 116/130 => Loss 2.923,  Train_accy 66.17, Test_accy 66.02
2023-12-09 14:59:16,366 [foster.py] => SNet: Task 4, Epoch 117/130 => Loss 2.915,  Train_accy 66.92
2023-12-09 14:59:20,301 [foster.py] => SNet: Task 4, Epoch 118/130 => Loss 2.946,  Train_accy 66.18
2023-12-09 14:59:24,218 [foster.py] => SNet: Task 4, Epoch 119/130 => Loss 2.932,  Train_accy 66.23
2023-12-09 14:59:28,250 [foster.py] => SNet: Task 4, Epoch 120/130 => Loss 2.917,  Train_accy 66.35
2023-12-09 14:59:34,215 [foster.py] => SNet: Task 4, Epoch 121/130 => Loss 2.927,  Train_accy 66.08, Test_accy 66.00
2023-12-09 14:59:38,473 [foster.py] => SNet: Task 4, Epoch 122/130 => Loss 2.930,  Train_accy 66.15
2023-12-09 14:59:42,650 [foster.py] => SNet: Task 4, Epoch 123/130 => Loss 2.948,  Train_accy 65.97
2023-12-09 14:59:46,972 [foster.py] => SNet: Task 4, Epoch 124/130 => Loss 2.933,  Train_accy 67.08
2023-12-09 14:59:51,424 [foster.py] => SNet: Task 4, Epoch 125/130 => Loss 2.935,  Train_accy 66.67
2023-12-09 14:59:57,591 [foster.py] => SNet: Task 4, Epoch 126/130 => Loss 2.936,  Train_accy 64.98, Test_accy 65.92
2023-12-09 15:00:01,663 [foster.py] => SNet: Task 4, Epoch 127/130 => Loss 2.918,  Train_accy 65.50
2023-12-09 15:00:05,762 [foster.py] => SNet: Task 4, Epoch 128/130 => Loss 2.935,  Train_accy 66.00
2023-12-09 15:00:09,714 [foster.py] => SNet: Task 4, Epoch 129/130 => Loss 2.908,  Train_accy 67.17
2023-12-09 15:00:13,755 [foster.py] => SNet: Task 4, Epoch 130/130 => Loss 2.931,  Train_accy 66.33
2023-12-09 15:00:13,757 [foster.py] => do not weight align student!
2023-12-09 15:00:15,969 [foster.py] => darknet eval: 
2023-12-09 15:00:15,969 [foster.py] => CNN top1 curve: 65.87
2023-12-09 15:00:15,969 [foster.py] => CNN top5 curve: 91.0
2023-12-09 15:00:15,970 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 15:01:37,631 [foster.py] => Exemplar size: 1800
2023-12-09 15:01:37,631 [trainer.py] => CNN: {'total': 67.13, '00-09': 71.9, '10-19': 59.6, '20-29': 73.0, '30-39': 62.8, '40-49': 69.8, '50-59': 58.5, '60-69': 71.3, '70-79': 64.4, '80-89': 72.9, 'old': 66.41, 'new': 72.9}
2023-12-09 15:01:37,631 [trainer.py] => NME: {'total': 58.39, '00-09': 61.0, '10-19': 51.5, '20-29': 64.9, '30-39': 52.2, '40-49': 61.4, '50-59': 49.1, '60-69': 57.1, '70-79': 52.9, '80-89': 75.4, 'old': 56.26, 'new': 75.4}
2023-12-09 15:01:37,631 [trainer.py] => CNN top1 curve: [80.38, 76.75, 74.74, 70.15, 67.13]
2023-12-09 15:01:37,631 [trainer.py] => CNN top5 curve: [96.48, 94.95, 94.13, 92.79, 91.33]
2023-12-09 15:01:37,631 [trainer.py] => NME top1 curve: [78.66, 72.0, 65.59, 60.14, 58.39]
2023-12-09 15:01:37,631 [trainer.py] => NME top5 curve: [96.2, 93.15, 90.0, 87.55, 86.32]

2023-12-09 15:01:37,632 [trainer.py] => All params: 950968
2023-12-09 15:01:37,633 [trainer.py] => Trainable params: 481614
2023-12-09 15:01:37,658 [foster.py] => Learning on 90-100
2023-12-09 15:01:37,659 [foster.py] => All params: 953558
2023-12-09 15:01:37,660 [foster.py] => Trainable params: 483554
2023-12-09 15:01:37,765 [foster.py] => per cls weights : [1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739 1.02987739
 0.73110353 0.73110353 0.73110353 0.73110353 0.73110353 0.73110353
 0.73110353 0.73110353 0.73110353 0.73110353]
2023-12-09 15:01:41,429 [foster.py] => Task 5, Epoch 1/170 => Loss 10.653, Loss_clf 3.777, Loss_fe 3.539, Loss_kd 3.003, Train_accy 42.78
2023-12-09 15:01:47,837 [foster.py] => Task 5, Epoch 2/170 => Loss 7.944, Loss_clf 1.537, Loss_fe 3.108, Loss_kd 2.970, Train_accy 52.56, Test_accy 59.86
2023-12-09 15:01:54,357 [foster.py] => Task 5, Epoch 3/170 => Loss 7.588, Loss_clf 1.330, Loss_fe 2.961, Loss_kd 2.968, Train_accy 55.28, Test_accy 61.38
2023-12-09 15:02:00,975 [foster.py] => Task 5, Epoch 4/170 => Loss 7.488, Loss_clf 1.360, Loss_fe 2.842, Loss_kd 2.957, Train_accy 54.46, Test_accy 60.69
2023-12-09 15:02:07,433 [foster.py] => Task 5, Epoch 5/170 => Loss 7.489, Loss_clf 1.462, Loss_fe 2.753, Loss_kd 2.946, Train_accy 54.29, Test_accy 59.86
2023-12-09 15:02:11,321 [foster.py] => Task 5, Epoch 6/170 => Loss 7.277, Loss_clf 1.347, Loss_fe 2.650, Loss_kd 2.952, Train_accy 57.13
2023-12-09 15:02:18,101 [foster.py] => Task 5, Epoch 7/170 => Loss 7.176, Loss_clf 1.285, Loss_fe 2.596, Loss_kd 2.966, Train_accy 56.78, Test_accy 58.22
2023-12-09 15:02:24,744 [foster.py] => Task 5, Epoch 8/170 => Loss 7.144, Loss_clf 1.313, Loss_fe 2.545, Loss_kd 2.957, Train_accy 56.65, Test_accy 60.86
2023-12-09 15:02:31,170 [foster.py] => Task 5, Epoch 9/170 => Loss 7.134, Loss_clf 1.352, Loss_fe 2.490, Loss_kd 2.963, Train_accy 56.19, Test_accy 61.72
2023-12-09 15:02:37,497 [foster.py] => Task 5, Epoch 10/170 => Loss 7.084, Loss_clf 1.359, Loss_fe 2.437, Loss_kd 2.959, Train_accy 56.21, Test_accy 61.44
2023-12-09 15:02:41,161 [foster.py] => Task 5, Epoch 11/170 => Loss 6.911, Loss_clf 1.271, Loss_fe 2.361, Loss_kd 2.951, Train_accy 56.97
2023-12-09 15:02:47,866 [foster.py] => Task 5, Epoch 12/170 => Loss 6.851, Loss_clf 1.290, Loss_fe 2.269, Loss_kd 2.963, Train_accy 57.96, Test_accy 60.45
2023-12-09 15:02:54,386 [foster.py] => Task 5, Epoch 13/170 => Loss 6.761, Loss_clf 1.215, Loss_fe 2.262, Loss_kd 2.955, Train_accy 58.15, Test_accy 59.39
2023-12-09 15:03:01,101 [foster.py] => Task 5, Epoch 14/170 => Loss 6.838, Loss_clf 1.288, Loss_fe 2.259, Loss_kd 2.962, Train_accy 57.74, Test_accy 62.50
2023-12-09 15:03:07,656 [foster.py] => Task 5, Epoch 15/170 => Loss 6.758, Loss_clf 1.275, Loss_fe 2.178, Loss_kd 2.974, Train_accy 58.66, Test_accy 61.52
2023-12-09 15:03:11,304 [foster.py] => Task 5, Epoch 16/170 => Loss 6.698, Loss_clf 1.224, Loss_fe 2.187, Loss_kd 2.959, Train_accy 59.04
2023-12-09 15:03:17,939 [foster.py] => Task 5, Epoch 17/170 => Loss 6.724, Loss_clf 1.261, Loss_fe 2.171, Loss_kd 2.963, Train_accy 58.43, Test_accy 60.18
2023-12-09 15:03:24,314 [foster.py] => Task 5, Epoch 18/170 => Loss 6.644, Loss_clf 1.220, Loss_fe 2.126, Loss_kd 2.968, Train_accy 60.15, Test_accy 61.26
2023-12-09 15:03:30,737 [foster.py] => Task 5, Epoch 19/170 => Loss 6.586, Loss_clf 1.215, Loss_fe 2.076, Loss_kd 2.966, Train_accy 59.57, Test_accy 60.51
2023-12-09 15:03:37,287 [foster.py] => Task 5, Epoch 20/170 => Loss 6.617, Loss_clf 1.236, Loss_fe 2.099, Loss_kd 2.955, Train_accy 59.66, Test_accy 60.76
2023-12-09 15:03:41,172 [foster.py] => Task 5, Epoch 21/170 => Loss 6.701, Loss_clf 1.314, Loss_fe 2.096, Loss_kd 2.962, Train_accy 59.29
2023-12-09 15:03:47,912 [foster.py] => Task 5, Epoch 22/170 => Loss 6.596, Loss_clf 1.228, Loss_fe 2.075, Loss_kd 2.964, Train_accy 59.00, Test_accy 59.81
2023-12-09 15:03:54,936 [foster.py] => Task 5, Epoch 23/170 => Loss 6.707, Loss_clf 1.350, Loss_fe 2.044, Loss_kd 2.981, Train_accy 59.90, Test_accy 61.10
2023-12-09 15:04:01,572 [foster.py] => Task 5, Epoch 24/170 => Loss 6.624, Loss_clf 1.277, Loss_fe 2.041, Loss_kd 2.976, Train_accy 59.40, Test_accy 60.83
2023-12-09 15:04:08,345 [foster.py] => Task 5, Epoch 25/170 => Loss 6.696, Loss_clf 1.343, Loss_fe 2.055, Loss_kd 2.968, Train_accy 59.59, Test_accy 60.94
2023-12-09 15:04:12,212 [foster.py] => Task 5, Epoch 26/170 => Loss 6.616, Loss_clf 1.261, Loss_fe 2.061, Loss_kd 2.964, Train_accy 59.62
2023-12-09 15:04:18,633 [foster.py] => Task 5, Epoch 27/170 => Loss 6.521, Loss_clf 1.236, Loss_fe 1.979, Loss_kd 2.975, Train_accy 59.06, Test_accy 59.46
2023-12-09 15:04:25,167 [foster.py] => Task 5, Epoch 28/170 => Loss 6.427, Loss_clf 1.187, Loss_fe 1.959, Loss_kd 2.954, Train_accy 61.81, Test_accy 61.76
2023-12-09 15:04:31,843 [foster.py] => Task 5, Epoch 29/170 => Loss 6.452, Loss_clf 1.216, Loss_fe 1.940, Loss_kd 2.967, Train_accy 60.07, Test_accy 57.06
2023-12-09 15:04:38,431 [foster.py] => Task 5, Epoch 30/170 => Loss 6.529, Loss_clf 1.302, Loss_fe 1.931, Loss_kd 2.966, Train_accy 59.90, Test_accy 59.32
2023-12-09 15:04:41,997 [foster.py] => Task 5, Epoch 31/170 => Loss 6.389, Loss_clf 1.188, Loss_fe 1.911, Loss_kd 2.962, Train_accy 59.88
2023-12-09 15:04:48,327 [foster.py] => Task 5, Epoch 32/170 => Loss 6.497, Loss_clf 1.245, Loss_fe 1.951, Loss_kd 2.971, Train_accy 60.09, Test_accy 59.43
2023-12-09 15:04:54,862 [foster.py] => Task 5, Epoch 33/170 => Loss 6.433, Loss_clf 1.222, Loss_fe 1.905, Loss_kd 2.976, Train_accy 61.60, Test_accy 58.73
2023-12-09 15:05:01,207 [foster.py] => Task 5, Epoch 34/170 => Loss 6.489, Loss_clf 1.261, Loss_fe 1.918, Loss_kd 2.978, Train_accy 60.49, Test_accy 60.95
2023-12-09 15:05:07,608 [foster.py] => Task 5, Epoch 35/170 => Loss 6.334, Loss_clf 1.135, Loss_fe 1.913, Loss_kd 2.958, Train_accy 61.15, Test_accy 60.93
2023-12-09 15:05:11,511 [foster.py] => Task 5, Epoch 36/170 => Loss 6.283, Loss_clf 1.138, Loss_fe 1.855, Loss_kd 2.961, Train_accy 62.10
2023-12-09 15:05:17,917 [foster.py] => Task 5, Epoch 37/170 => Loss 6.322, Loss_clf 1.157, Loss_fe 1.886, Loss_kd 2.951, Train_accy 61.00, Test_accy 61.05
2023-12-09 15:05:24,562 [foster.py] => Task 5, Epoch 38/170 => Loss 6.290, Loss_clf 1.150, Loss_fe 1.836, Loss_kd 2.973, Train_accy 61.87, Test_accy 60.91
2023-12-09 15:05:31,078 [foster.py] => Task 5, Epoch 39/170 => Loss 6.225, Loss_clf 1.125, Loss_fe 1.812, Loss_kd 2.958, Train_accy 62.01, Test_accy 60.84
2023-12-09 15:05:37,558 [foster.py] => Task 5, Epoch 40/170 => Loss 6.235, Loss_clf 1.104, Loss_fe 1.842, Loss_kd 2.961, Train_accy 61.04, Test_accy 61.75
2023-12-09 15:05:41,223 [foster.py] => Task 5, Epoch 41/170 => Loss 6.261, Loss_clf 1.130, Loss_fe 1.839, Loss_kd 2.963, Train_accy 61.66
2023-12-09 15:05:47,740 [foster.py] => Task 5, Epoch 42/170 => Loss 6.263, Loss_clf 1.140, Loss_fe 1.822, Loss_kd 2.972, Train_accy 61.87, Test_accy 62.27
2023-12-09 15:05:54,104 [foster.py] => Task 5, Epoch 43/170 => Loss 6.313, Loss_clf 1.181, Loss_fe 1.831, Loss_kd 2.971, Train_accy 61.00, Test_accy 60.78
2023-12-09 15:06:00,683 [foster.py] => Task 5, Epoch 44/170 => Loss 6.259, Loss_clf 1.166, Loss_fe 1.807, Loss_kd 2.958, Train_accy 62.90, Test_accy 60.81
2023-12-09 15:06:07,714 [foster.py] => Task 5, Epoch 45/170 => Loss 6.139, Loss_clf 1.050, Loss_fe 1.787, Loss_kd 2.971, Train_accy 63.84, Test_accy 58.07
2023-12-09 15:06:11,623 [foster.py] => Task 5, Epoch 46/170 => Loss 6.299, Loss_clf 1.236, Loss_fe 1.779, Loss_kd 2.956, Train_accy 61.79
2023-12-09 15:06:18,005 [foster.py] => Task 5, Epoch 47/170 => Loss 6.169, Loss_clf 1.111, Loss_fe 1.777, Loss_kd 2.953, Train_accy 62.90, Test_accy 61.63
2023-12-09 15:06:24,450 [foster.py] => Task 5, Epoch 48/170 => Loss 6.059, Loss_clf 1.069, Loss_fe 1.710, Loss_kd 2.951, Train_accy 63.90, Test_accy 62.28
2023-12-09 15:06:30,880 [foster.py] => Task 5, Epoch 49/170 => Loss 6.181, Loss_clf 1.137, Loss_fe 1.750, Loss_kd 2.964, Train_accy 61.96, Test_accy 60.85
2023-12-09 15:06:37,351 [foster.py] => Task 5, Epoch 50/170 => Loss 6.204, Loss_clf 1.150, Loss_fe 1.756, Loss_kd 2.968, Train_accy 62.16, Test_accy 56.96
2023-12-09 15:06:41,225 [foster.py] => Task 5, Epoch 51/170 => Loss 6.090, Loss_clf 1.080, Loss_fe 1.724, Loss_kd 2.957, Train_accy 63.62
2023-12-09 15:06:48,282 [foster.py] => Task 5, Epoch 52/170 => Loss 6.044, Loss_clf 1.073, Loss_fe 1.684, Loss_kd 2.959, Train_accy 63.94, Test_accy 59.76
2023-12-09 15:06:55,085 [foster.py] => Task 5, Epoch 53/170 => Loss 6.095, Loss_clf 1.111, Loss_fe 1.696, Loss_kd 2.959, Train_accy 63.57, Test_accy 61.60
2023-12-09 15:07:01,538 [foster.py] => Task 5, Epoch 54/170 => Loss 6.024, Loss_clf 1.047, Loss_fe 1.688, Loss_kd 2.960, Train_accy 63.99, Test_accy 62.66
2023-12-09 15:07:08,190 [foster.py] => Task 5, Epoch 55/170 => Loss 6.053, Loss_clf 1.061, Loss_fe 1.704, Loss_kd 2.959, Train_accy 63.19, Test_accy 60.91
2023-12-09 15:07:11,917 [foster.py] => Task 5, Epoch 56/170 => Loss 6.070, Loss_clf 1.080, Loss_fe 1.714, Loss_kd 2.948, Train_accy 63.90
2023-12-09 15:07:18,569 [foster.py] => Task 5, Epoch 57/170 => Loss 6.047, Loss_clf 1.089, Loss_fe 1.673, Loss_kd 2.956, Train_accy 63.35, Test_accy 60.65
2023-12-09 15:07:25,156 [foster.py] => Task 5, Epoch 58/170 => Loss 5.967, Loss_clf 1.042, Loss_fe 1.636, Loss_kd 2.960, Train_accy 65.10, Test_accy 62.69
2023-12-09 15:07:31,911 [foster.py] => Task 5, Epoch 59/170 => Loss 5.984, Loss_clf 1.048, Loss_fe 1.650, Loss_kd 2.958, Train_accy 64.66, Test_accy 62.93
2023-12-09 15:07:38,848 [foster.py] => Task 5, Epoch 60/170 => Loss 5.935, Loss_clf 1.014, Loss_fe 1.648, Loss_kd 2.945, Train_accy 65.87, Test_accy 63.15
2023-12-09 15:07:42,589 [foster.py] => Task 5, Epoch 61/170 => Loss 5.960, Loss_clf 1.049, Loss_fe 1.618, Loss_kd 2.963, Train_accy 64.15
2023-12-09 15:07:49,654 [foster.py] => Task 5, Epoch 62/170 => Loss 5.937, Loss_clf 1.011, Loss_fe 1.639, Loss_kd 2.958, Train_accy 64.75, Test_accy 61.24
2023-12-09 15:07:56,224 [foster.py] => Task 5, Epoch 63/170 => Loss 5.944, Loss_clf 1.024, Loss_fe 1.620, Loss_kd 2.970, Train_accy 64.54, Test_accy 61.27
2023-12-09 15:08:03,561 [foster.py] => Task 5, Epoch 64/170 => Loss 5.805, Loss_clf 0.973, Loss_fe 1.567, Loss_kd 2.939, Train_accy 65.99, Test_accy 62.53
2023-12-09 15:08:10,673 [foster.py] => Task 5, Epoch 65/170 => Loss 5.898, Loss_clf 1.004, Loss_fe 1.606, Loss_kd 2.959, Train_accy 64.29, Test_accy 61.30
2023-12-09 15:08:14,786 [foster.py] => Task 5, Epoch 66/170 => Loss 5.938, Loss_clf 1.002, Loss_fe 1.666, Loss_kd 2.942, Train_accy 65.35
2023-12-09 15:08:22,112 [foster.py] => Task 5, Epoch 67/170 => Loss 5.794, Loss_clf 0.950, Loss_fe 1.591, Loss_kd 2.928, Train_accy 65.76, Test_accy 62.54
2023-12-09 15:08:29,282 [foster.py] => Task 5, Epoch 68/170 => Loss 5.982, Loss_clf 1.076, Loss_fe 1.615, Loss_kd 2.962, Train_accy 64.63, Test_accy 59.75
2023-12-09 15:08:36,438 [foster.py] => Task 5, Epoch 69/170 => Loss 5.964, Loss_clf 1.055, Loss_fe 1.610, Loss_kd 2.969, Train_accy 64.22, Test_accy 61.38
2023-12-09 15:08:43,388 [foster.py] => Task 5, Epoch 70/170 => Loss 5.814, Loss_clf 0.976, Loss_fe 1.564, Loss_kd 2.946, Train_accy 66.81, Test_accy 62.13
2023-12-09 15:08:47,328 [foster.py] => Task 5, Epoch 71/170 => Loss 5.923, Loss_clf 1.047, Loss_fe 1.585, Loss_kd 2.963, Train_accy 65.10
2023-12-09 15:08:54,342 [foster.py] => Task 5, Epoch 72/170 => Loss 5.874, Loss_clf 1.032, Loss_fe 1.563, Loss_kd 2.951, Train_accy 65.34, Test_accy 62.30
2023-12-09 15:09:00,983 [foster.py] => Task 5, Epoch 73/170 => Loss 5.816, Loss_clf 1.000, Loss_fe 1.532, Loss_kd 2.956, Train_accy 65.65, Test_accy 62.10
2023-12-09 15:09:08,303 [foster.py] => Task 5, Epoch 74/170 => Loss 5.735, Loss_clf 0.930, Loss_fe 1.509, Loss_kd 2.967, Train_accy 66.22, Test_accy 62.46
2023-12-09 15:09:15,362 [foster.py] => Task 5, Epoch 75/170 => Loss 5.788, Loss_clf 0.976, Loss_fe 1.526, Loss_kd 2.957, Train_accy 66.69, Test_accy 62.80
2023-12-09 15:09:19,459 [foster.py] => Task 5, Epoch 76/170 => Loss 5.717, Loss_clf 0.924, Loss_fe 1.501, Loss_kd 2.962, Train_accy 67.13
2023-12-09 15:09:26,846 [foster.py] => Task 5, Epoch 77/170 => Loss 5.692, Loss_clf 0.941, Loss_fe 1.471, Loss_kd 2.952, Train_accy 67.16, Test_accy 61.25
2023-12-09 15:09:34,065 [foster.py] => Task 5, Epoch 78/170 => Loss 5.726, Loss_clf 0.948, Loss_fe 1.492, Loss_kd 2.958, Train_accy 67.18, Test_accy 59.08
2023-12-09 15:09:40,710 [foster.py] => Task 5, Epoch 79/170 => Loss 5.895, Loss_clf 1.079, Loss_fe 1.510, Loss_kd 2.976, Train_accy 64.54, Test_accy 62.21
2023-12-09 15:09:47,308 [foster.py] => Task 5, Epoch 80/170 => Loss 5.680, Loss_clf 0.920, Loss_fe 1.479, Loss_kd 2.952, Train_accy 67.71, Test_accy 62.36
2023-12-09 15:09:51,547 [foster.py] => Task 5, Epoch 81/170 => Loss 5.662, Loss_clf 0.921, Loss_fe 1.461, Loss_kd 2.952, Train_accy 67.74
2023-12-09 15:09:58,814 [foster.py] => Task 5, Epoch 82/170 => Loss 5.710, Loss_clf 0.949, Loss_fe 1.477, Loss_kd 2.956, Train_accy 65.96, Test_accy 62.15
2023-12-09 15:10:05,924 [foster.py] => Task 5, Epoch 83/170 => Loss 5.731, Loss_clf 0.949, Loss_fe 1.493, Loss_kd 2.961, Train_accy 67.57, Test_accy 62.14
2023-12-09 15:10:13,034 [foster.py] => Task 5, Epoch 84/170 => Loss 5.724, Loss_clf 0.964, Loss_fe 1.472, Loss_kd 2.960, Train_accy 66.53, Test_accy 61.65
2023-12-09 15:10:20,241 [foster.py] => Task 5, Epoch 85/170 => Loss 5.688, Loss_clf 0.949, Loss_fe 1.448, Loss_kd 2.962, Train_accy 68.24, Test_accy 62.55
2023-12-09 15:10:24,434 [foster.py] => Task 5, Epoch 86/170 => Loss 5.698, Loss_clf 0.944, Loss_fe 1.456, Loss_kd 2.968, Train_accy 66.81
2023-12-09 15:10:31,698 [foster.py] => Task 5, Epoch 87/170 => Loss 5.626, Loss_clf 0.914, Loss_fe 1.424, Loss_kd 2.958, Train_accy 67.93, Test_accy 60.24
2023-12-09 15:10:38,341 [foster.py] => Task 5, Epoch 88/170 => Loss 5.622, Loss_clf 0.921, Loss_fe 1.428, Loss_kd 2.946, Train_accy 68.06, Test_accy 62.79
2023-12-09 15:10:45,346 [foster.py] => Task 5, Epoch 89/170 => Loss 5.656, Loss_clf 0.926, Loss_fe 1.434, Loss_kd 2.966, Train_accy 67.15, Test_accy 62.64
2023-12-09 15:10:52,503 [foster.py] => Task 5, Epoch 90/170 => Loss 5.535, Loss_clf 0.868, Loss_fe 1.392, Loss_kd 2.948, Train_accy 68.82, Test_accy 61.40
2023-12-09 15:10:56,213 [foster.py] => Task 5, Epoch 91/170 => Loss 5.499, Loss_clf 0.863, Loss_fe 1.366, Loss_kd 2.943, Train_accy 69.19
2023-12-09 15:11:02,852 [foster.py] => Task 5, Epoch 92/170 => Loss 5.607, Loss_clf 0.908, Loss_fe 1.417, Loss_kd 2.955, Train_accy 68.16, Test_accy 61.75
2023-12-09 15:11:09,996 [foster.py] => Task 5, Epoch 93/170 => Loss 5.466, Loss_clf 0.860, Loss_fe 1.333, Loss_kd 2.946, Train_accy 69.00, Test_accy 63.17
2023-12-09 15:11:17,360 [foster.py] => Task 5, Epoch 94/170 => Loss 5.477, Loss_clf 0.872, Loss_fe 1.332, Loss_kd 2.946, Train_accy 69.68, Test_accy 63.46
2023-12-09 15:11:24,609 [foster.py] => Task 5, Epoch 95/170 => Loss 5.555, Loss_clf 0.896, Loss_fe 1.378, Loss_kd 2.953, Train_accy 68.54, Test_accy 62.22
2023-12-09 15:11:28,621 [foster.py] => Task 5, Epoch 96/170 => Loss 5.518, Loss_clf 0.883, Loss_fe 1.353, Loss_kd 2.954, Train_accy 69.12
2023-12-09 15:11:35,987 [foster.py] => Task 5, Epoch 97/170 => Loss 5.458, Loss_clf 0.848, Loss_fe 1.328, Loss_kd 2.954, Train_accy 69.31, Test_accy 63.33
2023-12-09 15:11:43,050 [foster.py] => Task 5, Epoch 98/170 => Loss 5.453, Loss_clf 0.828, Loss_fe 1.355, Loss_kd 2.943, Train_accy 69.91, Test_accy 63.26
2023-12-09 15:11:49,898 [foster.py] => Task 5, Epoch 99/170 => Loss 5.405, Loss_clf 0.824, Loss_fe 1.295, Loss_kd 2.957, Train_accy 70.69, Test_accy 63.43
2023-12-09 15:11:56,623 [foster.py] => Task 5, Epoch 100/170 => Loss 5.349, Loss_clf 0.809, Loss_fe 1.263, Loss_kd 2.950, Train_accy 70.19, Test_accy 63.73
2023-12-09 15:12:00,752 [foster.py] => Task 5, Epoch 101/170 => Loss 5.407, Loss_clf 0.814, Loss_fe 1.308, Loss_kd 2.957, Train_accy 70.57
2023-12-09 15:12:08,065 [foster.py] => Task 5, Epoch 102/170 => Loss 5.407, Loss_clf 0.820, Loss_fe 1.296, Loss_kd 2.962, Train_accy 70.82, Test_accy 62.16
2023-12-09 15:12:15,248 [foster.py] => Task 5, Epoch 103/170 => Loss 5.421, Loss_clf 0.844, Loss_fe 1.300, Loss_kd 2.949, Train_accy 70.25, Test_accy 63.27
2023-12-09 15:12:22,583 [foster.py] => Task 5, Epoch 104/170 => Loss 5.349, Loss_clf 0.809, Loss_fe 1.264, Loss_kd 2.949, Train_accy 70.54, Test_accy 63.53
2023-12-09 15:12:29,508 [foster.py] => Task 5, Epoch 105/170 => Loss 5.329, Loss_clf 0.798, Loss_fe 1.254, Loss_kd 2.950, Train_accy 71.31, Test_accy 63.44
2023-12-09 15:12:33,686 [foster.py] => Task 5, Epoch 106/170 => Loss 5.302, Loss_clf 0.801, Loss_fe 1.238, Loss_kd 2.937, Train_accy 70.96
2023-12-09 15:12:40,429 [foster.py] => Task 5, Epoch 107/170 => Loss 5.307, Loss_clf 0.793, Loss_fe 1.231, Loss_kd 2.954, Train_accy 70.93, Test_accy 63.18
2023-12-09 15:12:47,237 [foster.py] => Task 5, Epoch 108/170 => Loss 5.291, Loss_clf 0.792, Loss_fe 1.233, Loss_kd 2.940, Train_accy 71.74, Test_accy 63.81
2023-12-09 15:12:54,344 [foster.py] => Task 5, Epoch 109/170 => Loss 5.344, Loss_clf 0.824, Loss_fe 1.235, Loss_kd 2.956, Train_accy 70.60, Test_accy 63.84
2023-12-09 15:13:01,672 [foster.py] => Task 5, Epoch 110/170 => Loss 5.287, Loss_clf 0.787, Loss_fe 1.215, Loss_kd 2.956, Train_accy 71.96, Test_accy 63.51
2023-12-09 15:13:05,803 [foster.py] => Task 5, Epoch 111/170 => Loss 5.286, Loss_clf 0.793, Loss_fe 1.211, Loss_kd 2.954, Train_accy 72.01
2023-12-09 15:13:13,164 [foster.py] => Task 5, Epoch 112/170 => Loss 5.337, Loss_clf 0.816, Loss_fe 1.240, Loss_kd 2.953, Train_accy 70.51, Test_accy 63.88
2023-12-09 15:13:19,777 [foster.py] => Task 5, Epoch 113/170 => Loss 5.212, Loss_clf 0.763, Loss_fe 1.173, Loss_kd 2.948, Train_accy 72.79, Test_accy 63.24
2023-12-09 15:13:26,657 [foster.py] => Task 5, Epoch 114/170 => Loss 5.177, Loss_clf 0.749, Loss_fe 1.153, Loss_kd 2.948, Train_accy 72.35, Test_accy 63.46
2023-12-09 15:13:33,211 [foster.py] => Task 5, Epoch 115/170 => Loss 5.175, Loss_clf 0.752, Loss_fe 1.146, Loss_kd 2.950, Train_accy 72.62, Test_accy 63.11
2023-12-09 15:13:36,969 [foster.py] => Task 5, Epoch 116/170 => Loss 5.180, Loss_clf 0.753, Loss_fe 1.153, Loss_kd 2.947, Train_accy 72.28
2023-12-09 15:13:43,653 [foster.py] => Task 5, Epoch 117/170 => Loss 5.150, Loss_clf 0.750, Loss_fe 1.132, Loss_kd 2.941, Train_accy 72.87, Test_accy 63.71
2023-12-09 15:13:50,652 [foster.py] => Task 5, Epoch 118/170 => Loss 5.154, Loss_clf 0.747, Loss_fe 1.140, Loss_kd 2.941, Train_accy 73.10, Test_accy 63.11
2023-12-09 15:13:57,385 [foster.py] => Task 5, Epoch 119/170 => Loss 5.157, Loss_clf 0.739, Loss_fe 1.136, Loss_kd 2.954, Train_accy 73.07, Test_accy 64.22
2023-12-09 15:14:03,923 [foster.py] => Task 5, Epoch 120/170 => Loss 5.088, Loss_clf 0.710, Loss_fe 1.103, Loss_kd 2.948, Train_accy 73.76, Test_accy 64.25
2023-12-09 15:14:07,903 [foster.py] => Task 5, Epoch 121/170 => Loss 5.069, Loss_clf 0.700, Loss_fe 1.094, Loss_kd 2.948, Train_accy 74.35
2023-12-09 15:14:14,951 [foster.py] => Task 5, Epoch 122/170 => Loss 4.993, Loss_clf 0.664, Loss_fe 1.058, Loss_kd 2.944, Train_accy 74.81, Test_accy 64.33
2023-12-09 15:14:21,988 [foster.py] => Task 5, Epoch 123/170 => Loss 5.016, Loss_clf 0.695, Loss_fe 1.043, Loss_kd 2.950, Train_accy 74.56, Test_accy 63.07
2023-12-09 15:14:29,408 [foster.py] => Task 5, Epoch 124/170 => Loss 5.053, Loss_clf 0.707, Loss_fe 1.075, Loss_kd 2.944, Train_accy 74.26, Test_accy 64.14
2023-12-09 15:14:36,449 [foster.py] => Task 5, Epoch 125/170 => Loss 5.038, Loss_clf 0.700, Loss_fe 1.066, Loss_kd 2.945, Train_accy 73.99, Test_accy 64.10
2023-12-09 15:14:40,186 [foster.py] => Task 5, Epoch 126/170 => Loss 5.011, Loss_clf 0.688, Loss_fe 1.040, Loss_kd 2.955, Train_accy 75.07
2023-12-09 15:14:47,171 [foster.py] => Task 5, Epoch 127/170 => Loss 5.009, Loss_clf 0.692, Loss_fe 1.053, Loss_kd 2.938, Train_accy 74.13, Test_accy 64.19
2023-12-09 15:14:53,698 [foster.py] => Task 5, Epoch 128/170 => Loss 4.938, Loss_clf 0.664, Loss_fe 1.004, Loss_kd 2.943, Train_accy 75.54, Test_accy 64.16
2023-12-09 15:15:00,243 [foster.py] => Task 5, Epoch 129/170 => Loss 5.006, Loss_clf 0.696, Loss_fe 1.030, Loss_kd 2.952, Train_accy 75.01, Test_accy 63.62
2023-12-09 15:15:07,136 [foster.py] => Task 5, Epoch 130/170 => Loss 4.947, Loss_clf 0.666, Loss_fe 1.009, Loss_kd 2.945, Train_accy 76.25, Test_accy 63.63
2023-12-09 15:15:10,968 [foster.py] => Task 5, Epoch 131/170 => Loss 4.961, Loss_clf 0.666, Loss_fe 1.009, Loss_kd 2.957, Train_accy 75.51
2023-12-09 15:15:18,045 [foster.py] => Task 5, Epoch 132/170 => Loss 4.915, Loss_clf 0.650, Loss_fe 0.982, Loss_kd 2.955, Train_accy 76.29, Test_accy 63.23
2023-12-09 15:15:24,833 [foster.py] => Task 5, Epoch 133/170 => Loss 4.935, Loss_clf 0.662, Loss_fe 0.980, Loss_kd 2.963, Train_accy 76.38, Test_accy 64.24
2023-12-09 15:15:31,512 [foster.py] => Task 5, Epoch 134/170 => Loss 4.880, Loss_clf 0.646, Loss_fe 0.954, Loss_kd 2.952, Train_accy 77.13, Test_accy 64.82
2023-12-09 15:15:38,018 [foster.py] => Task 5, Epoch 135/170 => Loss 4.925, Loss_clf 0.660, Loss_fe 0.983, Loss_kd 2.954, Train_accy 75.31, Test_accy 63.97
2023-12-09 15:15:41,733 [foster.py] => Task 5, Epoch 136/170 => Loss 4.911, Loss_clf 0.657, Loss_fe 0.969, Loss_kd 2.957, Train_accy 76.25
2023-12-09 15:15:48,348 [foster.py] => Task 5, Epoch 137/170 => Loss 4.849, Loss_clf 0.623, Loss_fe 0.937, Loss_kd 2.960, Train_accy 77.49, Test_accy 64.55
2023-12-09 15:15:55,299 [foster.py] => Task 5, Epoch 138/170 => Loss 4.858, Loss_clf 0.639, Loss_fe 0.939, Loss_kd 2.952, Train_accy 76.41, Test_accy 64.42
2023-12-09 15:16:02,549 [foster.py] => Task 5, Epoch 139/170 => Loss 4.829, Loss_clf 0.626, Loss_fe 0.918, Loss_kd 2.957, Train_accy 77.90, Test_accy 64.63
2023-12-09 15:16:09,330 [foster.py] => Task 5, Epoch 140/170 => Loss 4.813, Loss_clf 0.618, Loss_fe 0.920, Loss_kd 2.947, Train_accy 77.78, Test_accy 64.91
2023-12-09 15:16:13,162 [foster.py] => Task 5, Epoch 141/170 => Loss 4.744, Loss_clf 0.594, Loss_fe 0.883, Loss_kd 2.940, Train_accy 77.50
2023-12-09 15:16:20,074 [foster.py] => Task 5, Epoch 142/170 => Loss 4.755, Loss_clf 0.597, Loss_fe 0.883, Loss_kd 2.948, Train_accy 78.37, Test_accy 64.52
2023-12-09 15:16:26,983 [foster.py] => Task 5, Epoch 143/170 => Loss 4.748, Loss_clf 0.597, Loss_fe 0.888, Loss_kd 2.937, Train_accy 78.06, Test_accy 64.90
2023-12-09 15:16:34,232 [foster.py] => Task 5, Epoch 144/170 => Loss 4.719, Loss_clf 0.580, Loss_fe 0.873, Loss_kd 2.939, Train_accy 79.10, Test_accy 64.89
2023-12-09 15:16:41,252 [foster.py] => Task 5, Epoch 145/170 => Loss 4.746, Loss_clf 0.592, Loss_fe 0.875, Loss_kd 2.952, Train_accy 78.63, Test_accy 64.81
2023-12-09 15:16:45,117 [foster.py] => Task 5, Epoch 146/170 => Loss 4.726, Loss_clf 0.575, Loss_fe 0.878, Loss_kd 2.945, Train_accy 79.01
2023-12-09 15:16:52,436 [foster.py] => Task 5, Epoch 147/170 => Loss 4.745, Loss_clf 0.593, Loss_fe 0.871, Loss_kd 2.952, Train_accy 78.40, Test_accy 64.76
2023-12-09 15:16:59,805 [foster.py] => Task 5, Epoch 148/170 => Loss 4.718, Loss_clf 0.584, Loss_fe 0.868, Loss_kd 2.939, Train_accy 79.07, Test_accy 64.84
2023-12-09 15:17:06,933 [foster.py] => Task 5, Epoch 149/170 => Loss 4.675, Loss_clf 0.561, Loss_fe 0.837, Loss_kd 2.950, Train_accy 79.19, Test_accy 64.96
2023-12-09 15:17:13,406 [foster.py] => Task 5, Epoch 150/170 => Loss 4.727, Loss_clf 0.589, Loss_fe 0.864, Loss_kd 2.947, Train_accy 78.96, Test_accy 65.06
2023-12-09 15:17:17,076 [foster.py] => Task 5, Epoch 151/170 => Loss 4.690, Loss_clf 0.572, Loss_fe 0.840, Loss_kd 2.951, Train_accy 78.71
2023-12-09 15:17:23,864 [foster.py] => Task 5, Epoch 152/170 => Loss 4.604, Loss_clf 0.544, Loss_fe 0.800, Loss_kd 2.934, Train_accy 80.24, Test_accy 64.77
2023-12-09 15:17:30,358 [foster.py] => Task 5, Epoch 153/170 => Loss 4.664, Loss_clf 0.570, Loss_fe 0.827, Loss_kd 2.941, Train_accy 79.26, Test_accy 64.72
2023-12-09 15:17:36,997 [foster.py] => Task 5, Epoch 154/170 => Loss 4.631, Loss_clf 0.546, Loss_fe 0.808, Loss_kd 2.949, Train_accy 80.12, Test_accy 64.91
2023-12-09 15:17:43,911 [foster.py] => Task 5, Epoch 155/170 => Loss 4.606, Loss_clf 0.540, Loss_fe 0.800, Loss_kd 2.939, Train_accy 80.25, Test_accy 65.10
2023-12-09 15:17:47,790 [foster.py] => Task 5, Epoch 156/170 => Loss 4.651, Loss_clf 0.565, Loss_fe 0.821, Loss_kd 2.939, Train_accy 79.25
2023-12-09 15:17:54,595 [foster.py] => Task 5, Epoch 157/170 => Loss 4.630, Loss_clf 0.550, Loss_fe 0.807, Loss_kd 2.945, Train_accy 79.75, Test_accy 65.07
2023-12-09 15:18:00,987 [foster.py] => Task 5, Epoch 158/170 => Loss 4.639, Loss_clf 0.557, Loss_fe 0.799, Loss_kd 2.955, Train_accy 80.07, Test_accy 64.90
2023-12-09 15:18:07,398 [foster.py] => Task 5, Epoch 159/170 => Loss 4.616, Loss_clf 0.547, Loss_fe 0.786, Loss_kd 2.955, Train_accy 79.90, Test_accy 65.02
2023-12-09 15:18:14,086 [foster.py] => Task 5, Epoch 160/170 => Loss 4.624, Loss_clf 0.551, Loss_fe 0.794, Loss_kd 2.951, Train_accy 80.69, Test_accy 64.87
2023-12-09 15:18:17,916 [foster.py] => Task 5, Epoch 161/170 => Loss 4.624, Loss_clf 0.545, Loss_fe 0.789, Loss_kd 2.961, Train_accy 80.32
2023-12-09 15:18:24,433 [foster.py] => Task 5, Epoch 162/170 => Loss 4.539, Loss_clf 0.516, Loss_fe 0.759, Loss_kd 2.937, Train_accy 80.97, Test_accy 65.05
2023-12-09 15:18:31,172 [foster.py] => Task 5, Epoch 163/170 => Loss 4.577, Loss_clf 0.527, Loss_fe 0.774, Loss_kd 2.949, Train_accy 80.51, Test_accy 65.03
2023-12-09 15:18:38,055 [foster.py] => Task 5, Epoch 164/170 => Loss 4.650, Loss_clf 0.566, Loss_fe 0.807, Loss_kd 2.950, Train_accy 79.57, Test_accy 65.06
2023-12-09 15:18:44,686 [foster.py] => Task 5, Epoch 165/170 => Loss 4.546, Loss_clf 0.515, Loss_fe 0.766, Loss_kd 2.939, Train_accy 80.82, Test_accy 64.91
2023-12-09 15:18:48,471 [foster.py] => Task 5, Epoch 166/170 => Loss 4.603, Loss_clf 0.546, Loss_fe 0.791, Loss_kd 2.939, Train_accy 80.43
2023-12-09 15:18:55,301 [foster.py] => Task 5, Epoch 167/170 => Loss 4.530, Loss_clf 0.515, Loss_fe 0.755, Loss_kd 2.934, Train_accy 81.49, Test_accy 65.04
2023-12-09 15:19:02,139 [foster.py] => Task 5, Epoch 168/170 => Loss 4.625, Loss_clf 0.541, Loss_fe 0.799, Loss_kd 2.957, Train_accy 80.44, Test_accy 64.91
2023-12-09 15:19:08,961 [foster.py] => Task 5, Epoch 169/170 => Loss 4.637, Loss_clf 0.556, Loss_fe 0.797, Loss_kd 2.955, Train_accy 80.03, Test_accy 64.96
2023-12-09 15:19:15,787 [foster.py] => Task 5, Epoch 170/170 => Loss 4.623, Loss_clf 0.550, Loss_fe 0.788, Loss_kd 2.956, Train_accy 80.22, Test_accy 65.03
2023-12-09 15:19:15,788 [foster.py] => do not weight align teacher!
2023-12-09 15:19:15,789 [foster.py] => per cls weights : [1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 1.0575066  1.0575066  1.0575066  1.0575066  1.0575066  1.0575066
 0.48244061 0.48244061 0.48244061 0.48244061 0.48244061 0.48244061
 0.48244061 0.48244061 0.48244061 0.48244061]
2023-12-09 15:19:22,455 [foster.py] => SNet: Task 5, Epoch 1/130 => Loss 3.403,  Train_accy 38.84, Test_accy 57.71
2023-12-09 15:19:26,638 [foster.py] => SNet: Task 5, Epoch 2/130 => Loss 3.164,  Train_accy 54.91
2023-12-09 15:19:30,812 [foster.py] => SNet: Task 5, Epoch 3/130 => Loss 3.163,  Train_accy 56.74
2023-12-09 15:19:34,855 [foster.py] => SNet: Task 5, Epoch 4/130 => Loss 3.131,  Train_accy 58.69
2023-12-09 15:19:39,107 [foster.py] => SNet: Task 5, Epoch 5/130 => Loss 3.129,  Train_accy 59.60
2023-12-09 15:19:45,627 [foster.py] => SNet: Task 5, Epoch 6/130 => Loss 3.103,  Train_accy 60.60, Test_accy 61.55
2023-12-09 15:19:49,842 [foster.py] => SNet: Task 5, Epoch 7/130 => Loss 3.122,  Train_accy 60.04
2023-12-09 15:19:54,052 [foster.py] => SNet: Task 5, Epoch 8/130 => Loss 3.119,  Train_accy 59.90
2023-12-09 15:19:58,369 [foster.py] => SNet: Task 5, Epoch 9/130 => Loss 3.130,  Train_accy 60.85
2023-12-09 15:20:02,681 [foster.py] => SNet: Task 5, Epoch 10/130 => Loss 3.109,  Train_accy 59.47
2023-12-09 15:20:09,214 [foster.py] => SNet: Task 5, Epoch 11/130 => Loss 3.111,  Train_accy 61.16, Test_accy 61.38
2023-12-09 15:20:13,334 [foster.py] => SNet: Task 5, Epoch 12/130 => Loss 3.111,  Train_accy 62.24
2023-12-09 15:20:17,538 [foster.py] => SNet: Task 5, Epoch 13/130 => Loss 3.085,  Train_accy 61.49
2023-12-09 15:20:21,784 [foster.py] => SNet: Task 5, Epoch 14/130 => Loss 3.109,  Train_accy 62.46
2023-12-09 15:20:25,919 [foster.py] => SNet: Task 5, Epoch 15/130 => Loss 3.093,  Train_accy 62.79
2023-12-09 15:20:32,159 [foster.py] => SNet: Task 5, Epoch 16/130 => Loss 3.086,  Train_accy 62.79, Test_accy 62.28
2023-12-09 15:20:36,370 [foster.py] => SNet: Task 5, Epoch 17/130 => Loss 3.092,  Train_accy 62.25
2023-12-09 15:20:40,559 [foster.py] => SNet: Task 5, Epoch 18/130 => Loss 3.070,  Train_accy 63.00
2023-12-09 15:20:44,925 [foster.py] => SNet: Task 5, Epoch 19/130 => Loss 3.088,  Train_accy 62.32
2023-12-09 15:20:49,511 [foster.py] => SNet: Task 5, Epoch 20/130 => Loss 3.087,  Train_accy 62.85
2023-12-09 15:20:55,880 [foster.py] => SNet: Task 5, Epoch 21/130 => Loss 3.105,  Train_accy 62.54, Test_accy 62.11
2023-12-09 15:21:00,029 [foster.py] => SNet: Task 5, Epoch 22/130 => Loss 3.086,  Train_accy 63.41
2023-12-09 15:21:04,115 [foster.py] => SNet: Task 5, Epoch 23/130 => Loss 3.062,  Train_accy 63.46
2023-12-09 15:21:08,232 [foster.py] => SNet: Task 5, Epoch 24/130 => Loss 3.097,  Train_accy 62.07
2023-12-09 15:21:12,441 [foster.py] => SNet: Task 5, Epoch 25/130 => Loss 3.087,  Train_accy 62.94
2023-12-09 15:21:18,926 [foster.py] => SNet: Task 5, Epoch 26/130 => Loss 3.068,  Train_accy 63.21, Test_accy 61.29
2023-12-09 15:21:23,205 [foster.py] => SNet: Task 5, Epoch 27/130 => Loss 3.080,  Train_accy 63.82
2023-12-09 15:21:27,343 [foster.py] => SNet: Task 5, Epoch 28/130 => Loss 3.093,  Train_accy 62.60
2023-12-09 15:21:31,664 [foster.py] => SNet: Task 5, Epoch 29/130 => Loss 3.087,  Train_accy 64.18
2023-12-09 15:21:35,973 [foster.py] => SNet: Task 5, Epoch 30/130 => Loss 3.088,  Train_accy 62.85
2023-12-09 15:21:42,190 [foster.py] => SNet: Task 5, Epoch 31/130 => Loss 3.076,  Train_accy 63.41, Test_accy 62.88
2023-12-09 15:21:46,287 [foster.py] => SNet: Task 5, Epoch 32/130 => Loss 3.092,  Train_accy 63.75
2023-12-09 15:21:50,759 [foster.py] => SNet: Task 5, Epoch 33/130 => Loss 3.090,  Train_accy 63.15
2023-12-09 15:21:54,840 [foster.py] => SNet: Task 5, Epoch 34/130 => Loss 3.077,  Train_accy 63.79
2023-12-09 15:21:59,098 [foster.py] => SNet: Task 5, Epoch 35/130 => Loss 3.072,  Train_accy 64.00
2023-12-09 15:22:05,560 [foster.py] => SNet: Task 5, Epoch 36/130 => Loss 3.090,  Train_accy 64.06, Test_accy 62.85
2023-12-09 15:22:09,779 [foster.py] => SNet: Task 5, Epoch 37/130 => Loss 3.089,  Train_accy 63.59
2023-12-09 15:22:14,315 [foster.py] => SNet: Task 5, Epoch 38/130 => Loss 3.069,  Train_accy 64.60
2023-12-09 15:22:18,777 [foster.py] => SNet: Task 5, Epoch 39/130 => Loss 3.096,  Train_accy 63.07
2023-12-09 15:22:23,054 [foster.py] => SNet: Task 5, Epoch 40/130 => Loss 3.075,  Train_accy 64.40
2023-12-09 15:22:29,540 [foster.py] => SNet: Task 5, Epoch 41/130 => Loss 3.090,  Train_accy 64.37, Test_accy 62.36
2023-12-09 15:22:33,666 [foster.py] => SNet: Task 5, Epoch 42/130 => Loss 3.057,  Train_accy 64.25
2023-12-09 15:22:37,733 [foster.py] => SNet: Task 5, Epoch 43/130 => Loss 3.067,  Train_accy 65.15
2023-12-09 15:22:42,012 [foster.py] => SNet: Task 5, Epoch 44/130 => Loss 3.073,  Train_accy 63.97
2023-12-09 15:22:46,365 [foster.py] => SNet: Task 5, Epoch 45/130 => Loss 3.064,  Train_accy 65.43
2023-12-09 15:22:52,758 [foster.py] => SNet: Task 5, Epoch 46/130 => Loss 3.076,  Train_accy 65.74, Test_accy 63.36
2023-12-09 15:22:57,338 [foster.py] => SNet: Task 5, Epoch 47/130 => Loss 3.065,  Train_accy 64.16
2023-12-09 15:23:01,882 [foster.py] => SNet: Task 5, Epoch 48/130 => Loss 3.079,  Train_accy 64.34
2023-12-09 15:23:06,158 [foster.py] => SNet: Task 5, Epoch 49/130 => Loss 3.072,  Train_accy 65.21
2023-12-09 15:23:10,326 [foster.py] => SNet: Task 5, Epoch 50/130 => Loss 3.061,  Train_accy 64.74
2023-12-09 15:23:16,593 [foster.py] => SNet: Task 5, Epoch 51/130 => Loss 3.085,  Train_accy 64.49, Test_accy 63.05
2023-12-09 15:23:20,918 [foster.py] => SNet: Task 5, Epoch 52/130 => Loss 3.056,  Train_accy 65.50
2023-12-09 15:23:25,032 [foster.py] => SNet: Task 5, Epoch 53/130 => Loss 3.055,  Train_accy 64.38
2023-12-09 15:23:29,199 [foster.py] => SNet: Task 5, Epoch 54/130 => Loss 3.066,  Train_accy 64.24
2023-12-09 15:23:33,262 [foster.py] => SNet: Task 5, Epoch 55/130 => Loss 3.073,  Train_accy 64.51
2023-12-09 15:23:39,807 [foster.py] => SNet: Task 5, Epoch 56/130 => Loss 3.080,  Train_accy 64.79, Test_accy 62.90
2023-12-09 15:23:44,089 [foster.py] => SNet: Task 5, Epoch 57/130 => Loss 3.065,  Train_accy 64.43
2023-12-09 15:23:48,423 [foster.py] => SNet: Task 5, Epoch 58/130 => Loss 3.077,  Train_accy 64.25
2023-12-09 15:23:52,788 [foster.py] => SNet: Task 5, Epoch 59/130 => Loss 3.066,  Train_accy 63.78
2023-12-09 15:23:57,022 [foster.py] => SNet: Task 5, Epoch 60/130 => Loss 3.054,  Train_accy 65.71
2023-12-09 15:24:03,382 [foster.py] => SNet: Task 5, Epoch 61/130 => Loss 3.076,  Train_accy 65.47, Test_accy 63.49
2023-12-09 15:24:07,528 [foster.py] => SNet: Task 5, Epoch 62/130 => Loss 3.077,  Train_accy 64.65
2023-12-09 15:24:11,692 [foster.py] => SNet: Task 5, Epoch 63/130 => Loss 3.047,  Train_accy 65.60
2023-12-09 15:24:15,892 [foster.py] => SNet: Task 5, Epoch 64/130 => Loss 3.071,  Train_accy 64.49
2023-12-09 15:24:20,005 [foster.py] => SNet: Task 5, Epoch 65/130 => Loss 3.067,  Train_accy 64.68
2023-12-09 15:24:26,441 [foster.py] => SNet: Task 5, Epoch 66/130 => Loss 3.069,  Train_accy 65.44, Test_accy 63.09
2023-12-09 15:24:30,723 [foster.py] => SNet: Task 5, Epoch 67/130 => Loss 3.078,  Train_accy 64.96
2023-12-09 15:24:35,048 [foster.py] => SNet: Task 5, Epoch 68/130 => Loss 3.076,  Train_accy 65.07
2023-12-09 15:24:39,219 [foster.py] => SNet: Task 5, Epoch 69/130 => Loss 3.068,  Train_accy 65.60
2023-12-09 15:24:43,255 [foster.py] => SNet: Task 5, Epoch 70/130 => Loss 3.070,  Train_accy 65.84
2023-12-09 15:24:49,672 [foster.py] => SNet: Task 5, Epoch 71/130 => Loss 3.069,  Train_accy 65.24, Test_accy 63.79
2023-12-09 15:24:53,771 [foster.py] => SNet: Task 5, Epoch 72/130 => Loss 3.074,  Train_accy 65.34
2023-12-09 15:24:57,872 [foster.py] => SNet: Task 5, Epoch 73/130 => Loss 3.072,  Train_accy 65.35
2023-12-09 15:25:02,027 [foster.py] => SNet: Task 5, Epoch 74/130 => Loss 3.060,  Train_accy 65.59
2023-12-09 15:25:06,185 [foster.py] => SNet: Task 5, Epoch 75/130 => Loss 3.067,  Train_accy 65.97
2023-12-09 15:25:12,592 [foster.py] => SNet: Task 5, Epoch 76/130 => Loss 3.074,  Train_accy 64.78, Test_accy 63.88
2023-12-09 15:25:16,702 [foster.py] => SNet: Task 5, Epoch 77/130 => Loss 3.076,  Train_accy 64.69
2023-12-09 15:25:20,826 [foster.py] => SNet: Task 5, Epoch 78/130 => Loss 3.054,  Train_accy 65.53
2023-12-09 15:25:25,187 [foster.py] => SNet: Task 5, Epoch 79/130 => Loss 3.066,  Train_accy 65.15
2023-12-09 15:25:29,483 [foster.py] => SNet: Task 5, Epoch 80/130 => Loss 3.051,  Train_accy 66.01
2023-12-09 15:25:35,876 [foster.py] => SNet: Task 5, Epoch 81/130 => Loss 3.052,  Train_accy 65.10, Test_accy 63.64
2023-12-09 15:25:40,022 [foster.py] => SNet: Task 5, Epoch 82/130 => Loss 3.069,  Train_accy 65.56
2023-12-09 15:25:44,235 [foster.py] => SNet: Task 5, Epoch 83/130 => Loss 3.064,  Train_accy 65.81
2023-12-09 15:25:48,430 [foster.py] => SNet: Task 5, Epoch 84/130 => Loss 3.060,  Train_accy 65.00
2023-12-09 15:25:52,504 [foster.py] => SNet: Task 5, Epoch 85/130 => Loss 3.045,  Train_accy 66.37
2023-12-09 15:25:58,907 [foster.py] => SNet: Task 5, Epoch 86/130 => Loss 3.068,  Train_accy 65.91, Test_accy 63.83
2023-12-09 15:26:03,052 [foster.py] => SNet: Task 5, Epoch 87/130 => Loss 3.058,  Train_accy 66.60
2023-12-09 15:26:07,178 [foster.py] => SNet: Task 5, Epoch 88/130 => Loss 3.046,  Train_accy 65.74
2023-12-09 15:26:11,640 [foster.py] => SNet: Task 5, Epoch 89/130 => Loss 3.071,  Train_accy 65.66
2023-12-09 15:26:15,849 [foster.py] => SNet: Task 5, Epoch 90/130 => Loss 3.057,  Train_accy 66.21
2023-12-09 15:26:22,424 [foster.py] => SNet: Task 5, Epoch 91/130 => Loss 3.040,  Train_accy 65.59, Test_accy 64.10
2023-12-09 15:26:26,485 [foster.py] => SNet: Task 5, Epoch 92/130 => Loss 3.066,  Train_accy 65.82
2023-12-09 15:26:30,578 [foster.py] => SNet: Task 5, Epoch 93/130 => Loss 3.048,  Train_accy 65.49
2023-12-09 15:26:34,666 [foster.py] => SNet: Task 5, Epoch 94/130 => Loss 3.066,  Train_accy 65.43
2023-12-09 15:26:38,836 [foster.py] => SNet: Task 5, Epoch 95/130 => Loss 3.052,  Train_accy 66.31
2023-12-09 15:26:45,229 [foster.py] => SNet: Task 5, Epoch 96/130 => Loss 3.056,  Train_accy 65.15, Test_accy 64.20
2023-12-09 15:26:49,320 [foster.py] => SNet: Task 5, Epoch 97/130 => Loss 3.081,  Train_accy 65.25
2023-12-09 15:26:53,484 [foster.py] => SNet: Task 5, Epoch 98/130 => Loss 3.059,  Train_accy 65.91
2023-12-09 15:26:57,750 [foster.py] => SNet: Task 5, Epoch 99/130 => Loss 3.056,  Train_accy 66.34
2023-12-09 15:27:01,978 [foster.py] => SNet: Task 5, Epoch 100/130 => Loss 3.065,  Train_accy 66.21
2023-12-09 15:27:08,447 [foster.py] => SNet: Task 5, Epoch 101/130 => Loss 3.044,  Train_accy 66.76, Test_accy 64.15
2023-12-09 15:27:12,606 [foster.py] => SNet: Task 5, Epoch 102/130 => Loss 3.063,  Train_accy 65.84
2023-12-09 15:27:17,008 [foster.py] => SNet: Task 5, Epoch 103/130 => Loss 3.053,  Train_accy 66.01
2023-12-09 15:27:21,180 [foster.py] => SNet: Task 5, Epoch 104/130 => Loss 3.058,  Train_accy 66.25
2023-12-09 15:27:25,271 [foster.py] => SNet: Task 5, Epoch 105/130 => Loss 3.046,  Train_accy 66.49
2023-12-09 15:27:31,554 [foster.py] => SNet: Task 5, Epoch 106/130 => Loss 3.045,  Train_accy 65.51, Test_accy 64.21
2023-12-09 15:27:35,760 [foster.py] => SNet: Task 5, Epoch 107/130 => Loss 3.061,  Train_accy 65.88
2023-12-09 15:27:40,121 [foster.py] => SNet: Task 5, Epoch 108/130 => Loss 3.044,  Train_accy 66.46
2023-12-09 15:27:44,308 [foster.py] => SNet: Task 5, Epoch 109/130 => Loss 3.067,  Train_accy 65.47
2023-12-09 15:27:48,649 [foster.py] => SNet: Task 5, Epoch 110/130 => Loss 3.059,  Train_accy 65.47
2023-12-09 15:27:55,212 [foster.py] => SNet: Task 5, Epoch 111/130 => Loss 3.049,  Train_accy 66.62, Test_accy 64.25
2023-12-09 15:27:59,331 [foster.py] => SNet: Task 5, Epoch 112/130 => Loss 3.058,  Train_accy 66.01
2023-12-09 15:28:03,677 [foster.py] => SNet: Task 5, Epoch 113/130 => Loss 3.053,  Train_accy 66.32
2023-12-09 15:28:07,910 [foster.py] => SNet: Task 5, Epoch 114/130 => Loss 3.037,  Train_accy 65.40
2023-12-09 15:28:12,002 [foster.py] => SNet: Task 5, Epoch 115/130 => Loss 3.043,  Train_accy 66.41
2023-12-09 15:28:18,247 [foster.py] => SNet: Task 5, Epoch 116/130 => Loss 3.055,  Train_accy 66.13, Test_accy 64.09
2023-12-09 15:28:22,506 [foster.py] => SNet: Task 5, Epoch 117/130 => Loss 3.073,  Train_accy 65.50
2023-12-09 15:28:26,716 [foster.py] => SNet: Task 5, Epoch 118/130 => Loss 3.044,  Train_accy 65.24
2023-12-09 15:28:30,859 [foster.py] => SNet: Task 5, Epoch 119/130 => Loss 3.041,  Train_accy 66.01
2023-12-09 15:28:35,092 [foster.py] => SNet: Task 5, Epoch 120/130 => Loss 3.057,  Train_accy 65.88
2023-12-09 15:28:41,371 [foster.py] => SNet: Task 5, Epoch 121/130 => Loss 3.056,  Train_accy 66.32, Test_accy 64.23
2023-12-09 15:28:45,644 [foster.py] => SNet: Task 5, Epoch 122/130 => Loss 3.040,  Train_accy 65.24
2023-12-09 15:28:49,997 [foster.py] => SNet: Task 5, Epoch 123/130 => Loss 3.051,  Train_accy 66.06
2023-12-09 15:28:54,107 [foster.py] => SNet: Task 5, Epoch 124/130 => Loss 3.056,  Train_accy 66.41
2023-12-09 15:28:58,298 [foster.py] => SNet: Task 5, Epoch 125/130 => Loss 3.050,  Train_accy 66.60
2023-12-09 15:29:04,668 [foster.py] => SNet: Task 5, Epoch 126/130 => Loss 3.054,  Train_accy 65.50, Test_accy 64.32
2023-12-09 15:29:08,791 [foster.py] => SNet: Task 5, Epoch 127/130 => Loss 3.055,  Train_accy 66.38
2023-12-09 15:29:13,015 [foster.py] => SNet: Task 5, Epoch 128/130 => Loss 3.055,  Train_accy 66.18
2023-12-09 15:29:17,307 [foster.py] => SNet: Task 5, Epoch 129/130 => Loss 3.053,  Train_accy 65.50
2023-12-09 15:29:21,636 [foster.py] => SNet: Task 5, Epoch 130/130 => Loss 3.051,  Train_accy 66.32
2023-12-09 15:29:21,637 [foster.py] => do not weight align student!
2023-12-09 15:29:23,754 [foster.py] => darknet eval: 
2023-12-09 15:29:23,755 [foster.py] => CNN top1 curve: 64.22
2023-12-09 15:29:23,755 [foster.py] => CNN top5 curve: 89.79
2023-12-09 15:29:23,756 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-12-09 15:30:52,184 [foster.py] => Exemplar size: 2000
2023-12-09 15:30:52,184 [trainer.py] => CNN: {'total': 65.03, '00-09': 70.6, '10-19': 57.5, '20-29': 69.2, '30-39': 59.3, '40-49': 67.6, '50-59': 55.1, '60-69': 67.4, '70-79': 59.7, '80-89': 68.0, '90-99': 75.9, 'old': 63.82, 'new': 75.9}
2023-12-09 15:30:52,184 [trainer.py] => NME: {'total': 55.56, '00-09': 56.9, '10-19': 47.1, '20-29': 60.9, '30-39': 51.2, '40-49': 61.6, '50-59': 45.1, '60-69': 57.8, '70-79': 51.1, '80-89': 53.1, '90-99': 70.8, 'old': 53.87, 'new': 70.8}
2023-12-09 15:30:52,184 [trainer.py] => CNN top1 curve: [80.38, 76.75, 74.74, 70.15, 67.13, 65.03]
2023-12-09 15:30:52,184 [trainer.py] => CNN top5 curve: [96.48, 94.95, 94.13, 92.79, 91.33, 90.01]
2023-12-09 15:30:52,184 [trainer.py] => NME top1 curve: [78.66, 72.0, 65.59, 60.14, 58.39, 55.56]
2023-12-09 15:30:52,184 [trainer.py] => NME top5 curve: [96.2, 93.15, 90.0, 87.55, 86.32, 83.84]

